// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#ifndef __STDC_FORMAT_MACROS
#define __STDC_FORMAT_MACROS
#endif

#include <ATen/MkldnnCPUType.h>

// @generated by aten/src/ATen/gen.py from TypeDerived.cpp

#include <c10/core/TensorImpl.h>
#include <ATen/CPUGeneratorImpl.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NativeFunctions.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/Half.h>
#include <c10/core/TensorImpl.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>
#include <ATen/core/op_registration/hacky_wrapper_for_legacy_signatures.h>
#include <torch/library.h>



namespace {
static const char* named_tensors_unsupported_error =
  " is not yet supported with named tensors. Please drop names via "
  "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
  "and set names on the result of the operation.";
}

namespace at {

/* example
Tensor * MkldnnCPUType::add(Tensor & a, Tensor & b) {
  std::cout << "add Tensor with backend MkldnnCPU\n";
  return &a;
}
*/

namespace MkldnnCPUType {

Tensor add_Tensor(const Tensor & self, const Tensor & other, Scalar alpha) {
    return at::native::mkldnn_add(self, other, alpha);
}
Tensor & add__Tensor(Tensor & self, const Tensor & other, Scalar alpha) {
    return at::native::mkldnn_add_(self, other, alpha);
}
Tensor & add_out_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) {
    return at::native::mkldnn_add_out(out, self, other, alpha);
}
Tensor empty_memory_format(IntArrayRef size, const TensorOptions & options, c10::optional<MemoryFormat> memory_format) {
    return at::native::empty_mkldnn(size, options, memory_format);
}
Tensor mkldnn_linear(const Tensor & input, const Tensor & weight, const Tensor & bias) {
    return at::native::mkldnn_linear(input, weight, bias);
}
Tensor mkldnn_max_pool2d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) {
    return at::native::mkldnn_max_pool2d(self, kernel_size, stride, padding, dilation, ceil_mode);
}
Tensor mul_Tensor(const Tensor & self, const Tensor & other) {
    return at::native::mkldnn_mul(self, other);
}
Tensor & mul__Tensor(Tensor & self, const Tensor & other) {
    return at::native::mkldnn_mul_(self, other);
}
Tensor & mul_out_out(Tensor & out, const Tensor & self, const Tensor & other) {
    return at::native::mkldnn_mul_out(out, self, other);
}
std::tuple<Tensor,Tensor,Tensor> native_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps) {
    return at::native::mkldnn_batch_norm(input, weight, bias, running_mean, running_var, training, momentum, eps);
}
Tensor _mkldnn_reshape(const Tensor & self, IntArrayRef shape) {
    return at::native::mkldnn_reshape(self, shape);
}
Tensor relu(const Tensor & self) {
    return at::native::mkldnn_relu(self);
}
Tensor & relu_(Tensor & self) {
    return at::native::mkldnn_relu_(self);
}
Tensor sigmoid(const Tensor & self) {
    return at::native::mkldnn_sigmoid(self);
}
Tensor & sigmoid_(Tensor & self) {
    return at::native::mkldnn_sigmoid_(self);
}
Tensor _softmax(const Tensor & self, int64_t dim, bool half_to_float) {
    return at::native::mkldnn_softmax(self, dim, half_to_float);
}
Tensor _mkldnn_transpose(const Tensor & self, int64_t dim0, int64_t dim1) {
    return at::native::mkldnn_transpose(self, dim0, dim1);
}
Tensor & _mkldnn_transpose_(Tensor & self, int64_t dim0, int64_t dim1) {
    return at::native::mkldnn_transpose_(self, dim0, dim1);
}
Tensor clone(const Tensor & self, c10::optional<MemoryFormat> memory_format) {
    return at::native::mkldnn_clone(self, memory_format);
}
Tensor & zero_(Tensor & self) {
    return at::native::mkldnn_zero_(self);
}
Tensor to_dense(const Tensor & self) {
    return at::native::mkldnn_to_dense(self);
}
Tensor mkldnn_reorder_conv2d_weight(const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups) {
    return at::native::mkldnn_reorder_conv2d_weight(self, padding, stride, dilation, groups);
}
Tensor view(const Tensor & self, IntArrayRef size) {
    return at::native::mkldnn_view(self, size);
}
Tensor & adaptive_avg_pool2d_out_out(Tensor & out, const Tensor & self, IntArrayRef output_size) {
    return at::native::mkldnn_adaptive_avg_pool2d_out(out, self, output_size);
}
Tensor mkldnn_adaptive_avg_pool2d(const Tensor & self, IntArrayRef output_size) {
    return at::native::mkldnn_adaptive_avg_pool2d(self, output_size);
}
Tensor & avg_pool2d_out_out(Tensor & out, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
    return at::native::mkldnn_avg_pool2d_out(out, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
Tensor avg_pool2d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
    return at::native::mkldnn_avg_pool2d(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}

}  // namespace MkldnnCPUType

TORCH_LIBRARY_IMPL(aten, MkldnnCPU, m) {
  m.impl("add.Tensor",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(MkldnnCPUType::add_Tensor)))
  );
  m.impl("add_.Tensor",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         torch::CppFunction::makeUnboxedOnly(&MkldnnCPUType::add__Tensor))
  );
  m.impl("add.out",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         torch::CppFunction::makeUnboxedOnly(&MkldnnCPUType::add_out_out))
  );
  m.impl("empty.memory_format",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(MkldnnCPUType::empty_memory_format)))
  );
  m.impl("mkldnn_linear",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         torch::CppFunction::makeUnboxedOnly(&MkldnnCPUType::mkldnn_linear))
  );
  m.impl("mkldnn_max_pool2d",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(MkldnnCPUType::mkldnn_max_pool2d)))
  );
  m.impl("mul.Tensor",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(MkldnnCPUType::mul_Tensor)))
  );
  m.impl("mul_.Tensor",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         torch::CppFunction::makeUnboxedOnly(&MkldnnCPUType::mul__Tensor))
  );
  m.impl("mul.out",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         torch::CppFunction::makeUnboxedOnly(&MkldnnCPUType::mul_out_out))
  );
  m.impl("native_batch_norm",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         torch::CppFunction::makeUnboxedOnly(&MkldnnCPUType::native_batch_norm))
  );
  m.impl("_mkldnn_reshape",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(MkldnnCPUType::_mkldnn_reshape)))
  );
  m.impl("relu",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(MkldnnCPUType::relu)))
  );
  m.impl("relu_",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         torch::CppFunction::makeUnboxedOnly(&MkldnnCPUType::relu_))
  );
  m.impl("sigmoid",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(MkldnnCPUType::sigmoid)))
  );
  m.impl("sigmoid_",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         torch::CppFunction::makeUnboxedOnly(&MkldnnCPUType::sigmoid_))
  );
  m.impl("_softmax",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(MkldnnCPUType::_softmax)))
  );
  m.impl("_mkldnn_transpose",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(MkldnnCPUType::_mkldnn_transpose)))
  );
  m.impl("_mkldnn_transpose_",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         torch::CppFunction::makeUnboxedOnly(&MkldnnCPUType::_mkldnn_transpose_))
  );
  m.impl("clone",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(MkldnnCPUType::clone)))
  );
  m.impl("zero_",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         torch::CppFunction::makeUnboxedOnly(&MkldnnCPUType::zero_))
  );
  m.impl("to_dense",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(MkldnnCPUType::to_dense)))
  );
  m.impl("mkldnn_reorder_conv2d_weight",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(MkldnnCPUType::mkldnn_reorder_conv2d_weight)))
  );
  m.impl("view",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(MkldnnCPUType::view)))
  );
  m.impl("adaptive_avg_pool2d.out",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         torch::CppFunction::makeUnboxedOnly(&MkldnnCPUType::adaptive_avg_pool2d_out_out))
  );
  m.impl("mkldnn_adaptive_avg_pool2d",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(MkldnnCPUType::mkldnn_adaptive_avg_pool2d)))
  );
  m.impl("avg_pool2d.out",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         torch::CppFunction::makeUnboxedOnly(&MkldnnCPUType::avg_pool2d_out_out))
  );
  m.impl("avg_pool2d",
         torch::dispatch(DispatchKey::MkldnnCPU,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(MkldnnCPUType::avg_pool2d)))
  );
}

} // namespace at
