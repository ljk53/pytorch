// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#ifndef __STDC_FORMAT_MACROS
#define __STDC_FORMAT_MACROS
#endif

#include <ATen/QuantizedCUDAType.h>

// @generated by aten/src/ATen/gen.py from TypeDerived.cpp

#include <c10/core/TensorImpl.h>
#include <ATen/CUDAGeneratorImpl.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NativeFunctions.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/Half.h>
#include <c10/core/TensorImpl.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>
#include <ATen/core/op_registration/hacky_wrapper_for_legacy_signatures.h>
#include <torch/library.h>
#include <ATen/DeviceGuard.h>
#include <ATen/cuda/ATenCUDAGeneral.h>
#include <ATen/cuda/CUDADevice.h>
#include <ATen/cuda/CUDAContext.h>


namespace {
static const char* named_tensors_unsupported_error =
  " is not yet supported with named tensors. Please drop names via "
  "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
  "and set names on the result of the operation.";
}

namespace at {

/* example
Tensor * QuantizedCUDAType::add(Tensor & a, Tensor & b) {
  std::cout << "add Tensor with backend QuantizedCUDA\n";
  return &a;
}
*/

namespace QuantizedCUDAType {

Tensor as_strided(const Tensor & self, IntArrayRef size, IntArrayRef stride, c10::optional<int64_t> storage_offset) {

    // DeviceGuard omitted
    return at::native::as_strided_qtensorimpl(self, size, stride, storage_offset);
}
Tensor _empty_affine_quantized(IntArrayRef size, const TensorOptions & options, double scale, int64_t zero_point, c10::optional<MemoryFormat> memory_format) {
    globalContext().lazyInitCUDA();
    const DeviceGuard device_guard(options.device());
    return at::native::empty_affine_quantized(size, options, scale, zero_point, memory_format);
}
Tensor empty_quantized(IntArrayRef size, const Tensor & qtensor) {

    const OptionalDeviceGuard device_guard(device_of(qtensor));
    return at::native::empty_quantized(size, qtensor);
}
Tensor clone(const Tensor & self, c10::optional<MemoryFormat> memory_format) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::quantized_clone(self, memory_format);
}
Tensor dequantize_self(const Tensor & self) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::dequantize_quant(self);
}
double q_scale(const Tensor & self) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::q_scale_quant(self);
}
int64_t q_zero_point(const Tensor & self) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::q_zero_point_quant(self);
}
Tensor int_repr(const Tensor & self) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::int_repr_quant_cuda(self);
}
QScheme qscheme(const Tensor & self) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::qscheme_quant(self);
}
Tensor & set__source_Storage_storage_offset(Tensor & self, Storage source, int64_t storage_offset, IntArrayRef size, IntArrayRef stride) {

    // DeviceGuard omitted
    return at::native::set_storage_quantized_(self, source, storage_offset, size, stride);
}
Tensor & set_quantizer_(Tensor & self, ConstQuantizerPtr quantizer) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::set_quantizer_(self, quantizer);
}
Tensor view(const Tensor & self, IntArrayRef size) {

    // DeviceGuard omitted
    return at::native::view(self, size);
}
Tensor unfold(const Tensor & self, int64_t dimension, int64_t size, int64_t step) {

    // DeviceGuard omitted
    return at::native::unfold(self, dimension, size, step);
}

}  // namespace QuantizedCUDAType

TORCH_LIBRARY_IMPL(aten, QuantizedCUDA, m) {
  m.impl("as_strided",
         torch::dispatch(DispatchKey::QuantizedCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(QuantizedCUDAType::as_strided)))
  );
  m.impl("_empty_affine_quantized",
         torch::dispatch(DispatchKey::QuantizedCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(QuantizedCUDAType::_empty_affine_quantized)))
  );
  m.impl("empty_quantized",
         torch::dispatch(DispatchKey::QuantizedCUDA,
                         torch::CppFunction::makeUnboxedOnly(&QuantizedCUDAType::empty_quantized))
  );
  m.impl("clone",
         torch::dispatch(DispatchKey::QuantizedCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(QuantizedCUDAType::clone)))
  );
  m.impl("dequantize.self",
         torch::dispatch(DispatchKey::QuantizedCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(QuantizedCUDAType::dequantize_self)))
  );
  m.impl("q_scale",
         torch::dispatch(DispatchKey::QuantizedCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(QuantizedCUDAType::q_scale)))
  );
  m.impl("q_zero_point",
         torch::dispatch(DispatchKey::QuantizedCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(QuantizedCUDAType::q_zero_point)))
  );
  m.impl("int_repr",
         torch::dispatch(DispatchKey::QuantizedCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(QuantizedCUDAType::int_repr)))
  );
  m.impl("qscheme",
         torch::dispatch(DispatchKey::QuantizedCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(QuantizedCUDAType::qscheme)))
  );
  m.impl("set_.source_Storage_storage_offset",
         torch::dispatch(DispatchKey::QuantizedCUDA,
                         torch::CppFunction::makeUnboxedOnly(&QuantizedCUDAType::set__source_Storage_storage_offset))
  );
  m.impl("set_quantizer_",
         torch::dispatch(DispatchKey::QuantizedCUDA,
                         torch::CppFunction::makeUnboxedOnly(&QuantizedCUDAType::set_quantizer_))
  );
  m.impl("view",
         torch::dispatch(DispatchKey::QuantizedCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(QuantizedCUDAType::view)))
  );
  m.impl("unfold",
         torch::dispatch(DispatchKey::QuantizedCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(QuantizedCUDAType::unfold)))
  );
}

} // namespace at
