// We register ops with a higher priority dispatch key (BackendSelect) than the usual backend-specific keys (e.g. CPU)
// which makes calls to the factory functions dispatch to here.
// We then 'manually' compute a lower-priority to re-dispatch to (e.g. CPU) to get to the eventually correct backend.

// @generated by aten/src/ATen/gen.py from BackendSelectRegister.cpp

#include <ATen/ATen.h>
#include <ATen/Dispatch.h>
#include <torch/library.h>
#include <ATen/core/op_registration/hacky_wrapper_for_legacy_signatures.h>
#include <c10/core/TensorOptions.h>

namespace at {

namespace {

// aten::_cudnn_init_dropout_state(float dropout, bool train, int dropout_seed, *, ScalarType dtype, Layout layout, Device device, bool pin_memory=False) -> Tensor
Tensor _cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions & options) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::_cudnn_init_dropout_state", "")
    .typed<Tensor (double, bool, int64_t, const TensorOptions &)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, dropout, train, dropout_seed, options);
}
// aten::arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor arange(Scalar end, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::arange", "")
    .typed<Tensor (Scalar, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, end, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor arange_start(Scalar start, Scalar end, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::arange", "start")
    .typed<Tensor (Scalar, Scalar, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, start, end, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::arange.start_step(Scalar start, Scalar end, Scalar step, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor arange_start_step(Scalar start, Scalar end, Scalar step, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::arange", "start_step")
    .typed<Tensor (Scalar, Scalar, Scalar, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, start, end, step, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::bartlett_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor bartlett_window(int64_t window_length, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::bartlett_window", "")
    .typed<Tensor (int64_t, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, window_length, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::bartlett_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor bartlett_window_periodic(int64_t window_length, bool periodic, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::bartlett_window", "periodic")
    .typed<Tensor (int64_t, bool, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, window_length, periodic, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::blackman_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor blackman_window(int64_t window_length, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::blackman_window", "")
    .typed<Tensor (int64_t, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, window_length, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::blackman_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor blackman_window_periodic(int64_t window_length, bool periodic, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::blackman_window", "periodic")
    .typed<Tensor (int64_t, bool, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, window_length, periodic, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::empty_meta(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
Tensor empty_meta(IntArrayRef size, const TensorOptions & options={}, c10::optional<MemoryFormat> memory_format=c10::nullopt) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::empty_meta", "")
    .typed<Tensor (IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>, c10::optional<MemoryFormat>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::process_memory_format(options, memory_format));
}
// aten::empty.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
Tensor empty_names(IntArrayRef size, c10::optional<DimnameList> names, const TensorOptions & options={}, c10::optional<MemoryFormat> memory_format=c10::nullopt) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::empty", "names")
    .typed<Tensor (IntArrayRef, c10::optional<DimnameList>, const TensorOptions &, c10::optional<MemoryFormat>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, size, names, options, memory_format);
}
// aten::empty.memory_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
Tensor empty_memory_format(IntArrayRef size, const TensorOptions & options={}, c10::optional<MemoryFormat> memory_format=c10::nullopt) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::empty", "memory_format")
    .typed<Tensor (IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>, c10::optional<MemoryFormat>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::process_memory_format(options, memory_format));
}
// aten::_empty_affine_quantized(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format) -> Tensor
Tensor _empty_affine_quantized(IntArrayRef size, const TensorOptions & options={}, double scale=1, int64_t zero_point=0, c10::optional<MemoryFormat> memory_format=MemoryFormat::Contiguous) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::_empty_affine_quantized", "")
    .typed<Tensor (IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>, double, int64_t, c10::optional<MemoryFormat>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), scale, zero_point, c10::impl::process_memory_format(options, memory_format));
}
// aten::_empty_per_channel_affine_quantized(int[] size, *, Tensor scales, Tensor zero_points, int axis, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=contiguous_format) -> Tensor
Tensor _empty_per_channel_affine_quantized(IntArrayRef size, const Tensor & scales, const Tensor & zero_points, int64_t axis, const TensorOptions & options={}, c10::optional<MemoryFormat> memory_format=MemoryFormat::Contiguous) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::_empty_per_channel_affine_quantized", "")
    .typed<Tensor (IntArrayRef, const Tensor &, const Tensor &, int64_t, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>, c10::optional<MemoryFormat>)>();
  DispatchKeySet _dk_set = DispatchKeySet(options.computeDispatchKey()) | c10::detail::multi_dispatch_key_set(scales, zero_points);
  DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
  DispatchKey _dk = c10::impl::dispatchTypeId(_dk_set, _dk_mask);
  return op.callWithDispatchKey(_dk, size, scales, zero_points, axis, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), c10::impl::process_memory_format(options, memory_format));
}
// aten::empty_strided(int[] size, int[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor empty_strided(IntArrayRef size, IntArrayRef stride, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::empty_strided", "")
    .typed<Tensor (IntArrayRef, IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, size, stride, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::eye(int n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor eye(int64_t n, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::eye", "")
    .typed<Tensor (int64_t, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, n, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::eye.m(int n, int m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor eye_m(int64_t n, int64_t m, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::eye", "m")
    .typed<Tensor (int64_t, int64_t, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, n, m, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::full.names(int[] size, Scalar fill_value, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor full_names(IntArrayRef size, Scalar fill_value, c10::optional<DimnameList> names, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::full", "names")
    .typed<Tensor (IntArrayRef, Scalar, c10::optional<DimnameList>, const TensorOptions &)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, size, fill_value, names, options);
}
// aten::full(int[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor full(IntArrayRef size, Scalar fill_value, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::full", "")
    .typed<Tensor (IntArrayRef, Scalar, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, size, fill_value, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::from_file(str filename, bool? shared=None, int? size=0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor from_file(std::string filename, c10::optional<bool> shared=c10::nullopt, c10::optional<int64_t> size=0, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::from_file", "")
    .typed<Tensor (std::string, c10::optional<bool>, c10::optional<int64_t>, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, filename, shared, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::hann_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor hann_window(int64_t window_length, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::hann_window", "")
    .typed<Tensor (int64_t, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, window_length, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::hann_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor hann_window_periodic(int64_t window_length, bool periodic, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::hann_window", "periodic")
    .typed<Tensor (int64_t, bool, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, window_length, periodic, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::hamming_window(int window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor hamming_window(int64_t window_length, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::hamming_window", "")
    .typed<Tensor (int64_t, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, window_length, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::hamming_window.periodic(int window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor hamming_window_periodic(int64_t window_length, bool periodic, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::hamming_window", "periodic")
    .typed<Tensor (int64_t, bool, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, window_length, periodic, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::hamming_window.periodic_alpha(int window_length, bool periodic, float alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor hamming_window_periodic_alpha(int64_t window_length, bool periodic, double alpha, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::hamming_window", "periodic_alpha")
    .typed<Tensor (int64_t, bool, double, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, window_length, periodic, alpha, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::hamming_window.periodic_alpha_beta(int window_length, bool periodic, float alpha, float beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor hamming_window_periodic_alpha_beta(int64_t window_length, bool periodic, double alpha, double beta, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::hamming_window", "periodic_alpha_beta")
    .typed<Tensor (int64_t, bool, double, double, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, window_length, periodic, alpha, beta, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::linspace(Scalar start, Scalar end, int steps=100, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor linspace(Scalar start, Scalar end, int64_t steps=100, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::linspace", "")
    .typed<Tensor (Scalar, Scalar, int64_t, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, start, end, steps, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::logspace(Scalar start, Scalar end, int steps=100, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor logspace(Scalar start, Scalar end, int64_t steps=100, double base=10.0, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::logspace", "")
    .typed<Tensor (Scalar, Scalar, int64_t, double, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, start, end, steps, base, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::ones.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor ones_names(IntArrayRef size, c10::optional<DimnameList> names, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::ones", "names")
    .typed<Tensor (IntArrayRef, c10::optional<DimnameList>, const TensorOptions &)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, size, names, options);
}
// aten::ones(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor ones(IntArrayRef size, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::ones", "")
    .typed<Tensor (IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::scalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor scalar_tensor(Scalar s, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::scalar_tensor", "")
    .typed<Tensor (Scalar, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, s, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::rand.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor rand_names(IntArrayRef size, c10::optional<DimnameList> names, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::rand", "names")
    .typed<Tensor (IntArrayRef, c10::optional<DimnameList>, const TensorOptions &)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, size, names, options);
}
// aten::rand.generator_with_names(int[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor rand_generator_with_names(IntArrayRef size, c10::optional<Generator> generator, c10::optional<DimnameList> names, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::rand", "generator_with_names")
    .typed<Tensor (IntArrayRef, c10::optional<Generator>, c10::optional<DimnameList>, const TensorOptions &)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, size, generator, names, options);
}
// aten::rand(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor rand(IntArrayRef size, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::rand", "")
    .typed<Tensor (IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::rand.generator(int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor rand_generator(IntArrayRef size, c10::optional<Generator> generator, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::rand", "generator")
    .typed<Tensor (IntArrayRef, c10::optional<Generator>, const TensorOptions &)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, size, generator, options);
}
// aten::randint(int high, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor randint(int64_t high, IntArrayRef size, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::randint", "")
    .typed<Tensor (int64_t, IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, high, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::randint.generator(int high, int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor randint_generator(int64_t high, IntArrayRef size, c10::optional<Generator> generator, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::randint", "generator")
    .typed<Tensor (int64_t, IntArrayRef, c10::optional<Generator>, const TensorOptions &)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, high, size, generator, options);
}
// aten::randint.low(int low, int high, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor randint_low(int64_t low, int64_t high, IntArrayRef size, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::randint", "low")
    .typed<Tensor (int64_t, int64_t, IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, low, high, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::randint.low_generator(int low, int high, int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor randint_low_generator(int64_t low, int64_t high, IntArrayRef size, c10::optional<Generator> generator, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::randint", "low_generator")
    .typed<Tensor (int64_t, int64_t, IntArrayRef, c10::optional<Generator>, const TensorOptions &)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, low, high, size, generator, options);
}
// aten::randn(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor randn(IntArrayRef size, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::randn", "")
    .typed<Tensor (IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::randn.generator(int[] size, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor randn_generator(IntArrayRef size, c10::optional<Generator> generator, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::randn", "generator")
    .typed<Tensor (IntArrayRef, c10::optional<Generator>, const TensorOptions &)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, size, generator, options);
}
// aten::randn.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor randn_names(IntArrayRef size, c10::optional<DimnameList> names, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::randn", "names")
    .typed<Tensor (IntArrayRef, c10::optional<DimnameList>, const TensorOptions &)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, size, names, options);
}
// aten::randn.generator_with_names(int[] size, *, Generator? generator, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor randn_generator_with_names(IntArrayRef size, c10::optional<Generator> generator, c10::optional<DimnameList> names, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::randn", "generator_with_names")
    .typed<Tensor (IntArrayRef, c10::optional<Generator>, c10::optional<DimnameList>, const TensorOptions &)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, size, generator, names, options);
}
// aten::randperm(int n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor randperm(int64_t n, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::randperm", "")
    .typed<Tensor (int64_t, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, n, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::randperm.generator(int n, *, Generator? generator, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor randperm_generator(int64_t n, c10::optional<Generator> generator, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::randperm", "generator")
    .typed<Tensor (int64_t, c10::optional<Generator>, const TensorOptions &)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, n, generator, options);
}
// aten::range.step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor range_step(Scalar start, Scalar end, Scalar step=1, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::range", "step")
    .typed<Tensor (Scalar, Scalar, Scalar, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, start, end, step, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::range(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor range(Scalar start, Scalar end, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::range", "")
    .typed<Tensor (Scalar, Scalar, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, start, end, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::zeros.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor zeros_names(IntArrayRef size, c10::optional<DimnameList> names, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::zeros", "names")
    .typed<Tensor (IntArrayRef, c10::optional<DimnameList>, const TensorOptions &)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, size, names, options);
}
// aten::zeros(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor zeros(IntArrayRef size, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::zeros", "")
    .typed<Tensor (IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::sparse_coo_tensor.size(int[] size, *, ScalarType dtype, Layout layout, Device device, bool pin_memory=False) -> Tensor
Tensor sparse_coo_tensor_size(IntArrayRef size, const TensorOptions & options) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::sparse_coo_tensor", "size")
    .typed<Tensor (IntArrayRef, const TensorOptions &)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, size, options);
}
// aten::sparse_coo_tensor.indices(Tensor indices, Tensor values, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor sparse_coo_tensor_indices(const Tensor & indices, const Tensor & values, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::sparse_coo_tensor", "indices")
    .typed<Tensor (const Tensor &, const Tensor &, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKeySet _dk_set = DispatchKeySet(options.computeDispatchKey()) | c10::detail::multi_dispatch_key_set(indices, values);
  DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
  DispatchKey _dk = c10::impl::dispatchTypeId(_dk_set, _dk_mask);
  return op.callWithDispatchKey(_dk, indices, values, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::sparse_coo_tensor.indices_size(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor sparse_coo_tensor_indices_size(const Tensor & indices, const Tensor & values, IntArrayRef size, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::sparse_coo_tensor", "indices_size")
    .typed<Tensor (const Tensor &, const Tensor &, IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKeySet _dk_set = DispatchKeySet(options.computeDispatchKey()) | c10::detail::multi_dispatch_key_set(indices, values);
  DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
  DispatchKey _dk = c10::impl::dispatchTypeId(_dk_set, _dk_mask);
  return op.callWithDispatchKey(_dk, indices, values, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::_sparse_coo_tensor_unsafe(Tensor indices, Tensor values, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor _sparse_coo_tensor_unsafe(const Tensor & indices, const Tensor & values, IntArrayRef size, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::_sparse_coo_tensor_unsafe", "")
    .typed<Tensor (const Tensor &, const Tensor &, IntArrayRef, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKeySet _dk_set = DispatchKeySet(options.computeDispatchKey()) | c10::detail::multi_dispatch_key_set(indices, values);
  DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
  DispatchKey _dk = c10::impl::dispatchTypeId(_dk_set, _dk_mask);
  return op.callWithDispatchKey(_dk, indices, values, size, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::_sparse_coo_tensor_with_dims(int sparse_dim, int dense_dim, int[] size, *, ScalarType dtype, Layout layout, Device device, bool pin_memory=False) -> Tensor
Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::_sparse_coo_tensor_with_dims", "")
    .typed<Tensor (int64_t, int64_t, IntArrayRef, const TensorOptions &)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, sparse_dim, dense_dim, size, options);
}
// aten::_sparse_coo_tensor_with_dims_and_tensors(int sparse_dim, int dense_dim, int[] size, Tensor indices, Tensor values, *, ScalarType dtype, Layout layout, Device device, bool pin_memory=False) -> Tensor
Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::_sparse_coo_tensor_with_dims_and_tensors", "")
    .typed<Tensor (int64_t, int64_t, IntArrayRef, const Tensor &, const Tensor &, const TensorOptions &)>();
  DispatchKeySet _dk_set = DispatchKeySet(options.computeDispatchKey()) | c10::detail::multi_dispatch_key_set(indices, values);
  DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
  DispatchKey _dk = c10::impl::dispatchTypeId(_dk_set, _dk_mask);
  return op.callWithDispatchKey(_dk, sparse_dim, dense_dim, size, indices, values, options);
}
// aten::to.dtype_layout(Tensor self, *, ScalarType dtype, Layout layout, Device device, bool pin_memory=False, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor
Tensor to_dtype_layout(const Tensor & self, const TensorOptions & options, bool non_blocking=false, bool copy=false, c10::optional<MemoryFormat> memory_format=c10::nullopt) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::to", "dtype_layout")
    .typed<Tensor (const Tensor &, const TensorOptions &, bool, bool, c10::optional<MemoryFormat>)>();
  DispatchKeySet _dk_set = DispatchKeySet(options.computeDispatchKey()) | c10::detail::multi_dispatch_key_set(self);
  DispatchKeySet _dk_mask = c10::DispatchKeySet(DispatchKeySet::FULL_AFTER, DispatchKey::BackendSelect);
  DispatchKey _dk = c10::impl::dispatchTypeId(_dk_set, _dk_mask);
  return op.callWithDispatchKey(_dk, self, options, non_blocking, copy, memory_format);
}
// aten::tril_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor tril_indices(int64_t row, int64_t col, int64_t offset=0, const TensorOptions & options=at::kLong) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::tril_indices", "")
    .typed<Tensor (int64_t, int64_t, int64_t, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, row, col, offset, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::triu_indices(int row, int col, int offset=0, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor triu_indices(int64_t row, int64_t col, int64_t offset=0, const TensorOptions & options=at::kLong) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::triu_indices", "")
    .typed<Tensor (int64_t, int64_t, int64_t, c10::optional<ScalarType>, c10::optional<Layout>, c10::optional<Device>, c10::optional<bool>)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, row, col, offset, optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt());
}
// aten::normal.float_float(float mean, float std, int[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
Tensor normal_float_float(double mean, double std, IntArrayRef size, c10::optional<Generator> generator=c10::nullopt, const TensorOptions & options={}) {
  static auto op = c10::Dispatcher::singleton()
    .findSchemaOrThrow("aten::normal", "float_float")
    .typed<Tensor (double, double, IntArrayRef, c10::optional<Generator>, const TensorOptions &)>();
  DispatchKey _dk = options.computeDispatchKey();
  return op.callWithDispatchKey(_dk, mean, std, size, generator, options);
}

TORCH_LIBRARY_IMPL(aten, BackendSelect, m) {
    m.impl_UNBOXED("aten::_cudnn_init_dropout_state", _cudnn_init_dropout_state);
    m.impl("aten::arange", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(arange)));
    m.impl("aten::arange.start", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(arange_start)));
    m.impl("aten::arange.start_step", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(arange_start_step)));
    m.impl("aten::bartlett_window", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(bartlett_window)));
    m.impl("aten::bartlett_window.periodic", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(bartlett_window_periodic)));
    m.impl("aten::blackman_window", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(blackman_window)));
    m.impl("aten::blackman_window.periodic", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(blackman_window_periodic)));
    m.impl("aten::empty_meta", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(empty_meta)));
    m.impl_UNBOXED("aten::empty.names", empty_names);
    m.impl("aten::empty.memory_format", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(empty_memory_format)));
    m.impl("aten::_empty_affine_quantized", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(_empty_affine_quantized)));
    m.impl("aten::_empty_per_channel_affine_quantized", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(_empty_per_channel_affine_quantized)));
    m.impl("aten::empty_strided", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(empty_strided)));
    m.impl("aten::eye", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(eye)));
    m.impl("aten::eye.m", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(eye_m)));
    m.impl_UNBOXED("aten::full.names", full_names);
    m.impl("aten::full", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(full)));
    m.impl("aten::from_file", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(from_file)));
    m.impl("aten::hann_window", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(hann_window)));
    m.impl("aten::hann_window.periodic", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(hann_window_periodic)));
    m.impl("aten::hamming_window", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(hamming_window)));
    m.impl("aten::hamming_window.periodic", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(hamming_window_periodic)));
    m.impl("aten::hamming_window.periodic_alpha", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(hamming_window_periodic_alpha)));
    m.impl("aten::hamming_window.periodic_alpha_beta", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(hamming_window_periodic_alpha_beta)));
    m.impl("aten::linspace", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(linspace)));
    m.impl("aten::logspace", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(logspace)));
    m.impl_UNBOXED("aten::ones.names", ones_names);
    m.impl("aten::ones", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(ones)));
    m.impl("aten::scalar_tensor", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(scalar_tensor)));
    m.impl_UNBOXED("aten::rand.names", rand_names);
    m.impl_UNBOXED("aten::rand.generator_with_names", rand_generator_with_names);
    m.impl("aten::rand", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(rand)));
    m.impl_UNBOXED("aten::rand.generator", rand_generator);
    m.impl("aten::randint", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(randint)));
    m.impl_UNBOXED("aten::randint.generator", randint_generator);
    m.impl("aten::randint.low", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(randint_low)));
    m.impl_UNBOXED("aten::randint.low_generator", randint_low_generator);
    m.impl("aten::randn", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(randn)));
    m.impl_UNBOXED("aten::randn.generator", randn_generator);
    m.impl_UNBOXED("aten::randn.names", randn_names);
    m.impl_UNBOXED("aten::randn.generator_with_names", randn_generator_with_names);
    m.impl("aten::randperm", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(randperm)));
    m.impl_UNBOXED("aten::randperm.generator", randperm_generator);
    m.impl("aten::range.step", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(range_step)));
    m.impl("aten::range", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(range)));
    m.impl_UNBOXED("aten::zeros.names", zeros_names);
    m.impl("aten::zeros", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(zeros)));
    m.impl_UNBOXED("aten::sparse_coo_tensor.size", sparse_coo_tensor_size);
    m.impl("aten::sparse_coo_tensor.indices", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(sparse_coo_tensor_indices)));
    m.impl("aten::sparse_coo_tensor.indices_size", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(sparse_coo_tensor_indices_size)));
    m.impl("aten::_sparse_coo_tensor_unsafe", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(_sparse_coo_tensor_unsafe)));
    m.impl_UNBOXED("aten::_sparse_coo_tensor_with_dims", _sparse_coo_tensor_with_dims);
    m.impl_UNBOXED("aten::_sparse_coo_tensor_with_dims_and_tensors", _sparse_coo_tensor_with_dims_and_tensors);
    m.impl_UNBOXED("aten::to.dtype_layout", to_dtype_layout);
    m.impl("aten::tril_indices", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(tril_indices)));
    m.impl("aten::triu_indices", c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(triu_indices)));
    m.impl_UNBOXED("aten::normal.float_float", normal_float_float);;
}

} // namespace
} // at
