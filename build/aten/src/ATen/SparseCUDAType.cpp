// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#define __STDC_FORMAT_MACROS

#include <ATen/SparseCUDAType.h>

// @generated by aten/src/ATen/gen.py from SparseTypeDerived.cpp

#include <ATen/CUDAGeneratorImpl.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NativeFunctions.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/Half.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>
#include <ATen/core/op_registration/hacky_wrapper_for_legacy_signatures.h>
#include <torch/library.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>
#include <ATen/DeviceGuard.h>
#include <ATen/cuda/ATenCUDAGeneral.h>
#include <ATen/cuda/CUDADevice.h>
#include <ATen/cuda/CUDAContext.h>

namespace {
static const char* named_tensors_unsupported_error =
  " is not yet supported with named tensors. Please drop names via "
  "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
  "and set names on the result of the operation.";
}

namespace at {

namespace SparseCUDAType {

Tensor add_Tensor(const Tensor & self, const Tensor & other, Scalar alpha) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::add_sparse(self, other, alpha);
}
Tensor & add__Tensor(Tensor & self, const Tensor & other, Scalar alpha) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::add_sparse_(self, other, alpha);
}
Tensor & add_out_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::add_out_sparse_cuda(out, self, other, alpha);
}
Tensor bmm(const Tensor & self, const Tensor & mat2) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::bmm_sparse_cuda(self, mat2);
}
Tensor _bmm(const Tensor & self, const Tensor & mat2, bool deterministic) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_bmm_sparse_cuda(self, mat2, deterministic);
}
Tensor & bmm_out_out(Tensor & out, const Tensor & self, const Tensor & mat2) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::bmm_out_sparse_cuda(out, self, mat2);
}
Tensor & _bmm_out_out(Tensor & out, const Tensor & self, const Tensor & mat2, bool deterministic) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_bmm_out_sparse_cuda(out, self, mat2, deterministic);
}
Tensor div_Tensor(const Tensor & self, const Tensor & other) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::div_sparse(self, other);
}
Tensor & div__Tensor(Tensor & self, const Tensor & other) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::div_sparse_(self, other);
}
Tensor & div_out_out(Tensor & out, const Tensor & self, const Tensor & other) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::div_out_sparse_zerodim(out, self, other);
}
Tensor empty_memory_format(IntArrayRef size, const TensorOptions & options, c10::optional<MemoryFormat> memory_format) {
    globalContext().lazyInitCUDA();
    const DeviceGuard device_guard(options.device());
    return at::native::empty_sparse(size, options, memory_format);
}
Tensor floor_divide(const Tensor & self, const Tensor & other) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::floor_divide_sparse(self, other);
}
Tensor & floor_divide__Tensor(Tensor & self, const Tensor & other) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::floor_divide_sparse_(self, other);
}
Tensor & floor_divide_out_out(Tensor & out, const Tensor & self, const Tensor & other) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::floor_divide_out_sparse_zerodim(out, self, other);
}
Tensor isnan(const Tensor & self) {

    // DeviceGuard omitted
    return at::native::isnan_sparse(self);
}
Tensor & log1p_(Tensor & self) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::log1p_sparse_(self);
}
Tensor & log1p_out_out(Tensor & out, const Tensor & self) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::log1p_out_sparse(out, self);
}
Tensor mm(const Tensor & self, const Tensor & mat2) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sparse_mm(self, mat2);
}
Tensor & mm_out_out(Tensor & out, const Tensor & self, const Tensor & mat2) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sparse_mm_out(out, self, mat2);
}
Tensor mul_Tensor(const Tensor & self, const Tensor & other) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mul_sparse(self, other);
}
Tensor & mul__Tensor(Tensor & self, const Tensor & other) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mul_sparse_(self, other);
}
Tensor & mul_out_out(Tensor & out, const Tensor & self, const Tensor & other) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mul_out_sparse_cuda(out, self, other);
}
Tensor mv(const Tensor & self, const Tensor & vec) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mv_sparse(self, vec);
}
Tensor narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::narrow_copy_sparse(self, dim, start, length);
}
Tensor & sspaddmm_out_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sspaddmm_out_cuda(out, self, mat1, mat2, beta, alpha);
}
Tensor true_divide_Tensor(const Tensor & self, const Tensor & other) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::true_divide_sparse(self, other);
}
Tensor & true_divide__Tensor(Tensor & self, const Tensor & other) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::true_divide_sparse_(self, other);
}
Tensor & true_divide_out_out(Tensor & out, const Tensor & self, const Tensor & other) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::true_divide_out_sparse_zerodim(out, self, other);
}
Tensor native_norm(const Tensor & self, Scalar p) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::norm_sparse(self, p);
}
Tensor _sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sparse_sum_backward_cuda(grad, self, dim);
}
Tensor clone(const Tensor & self, c10::optional<MemoryFormat> memory_format) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::clone_sparse(self, memory_format);
}
Tensor & pow_out_Tensor_Scalar_out(Tensor & out, const Tensor & self, Scalar exponent) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::pow_out_sparse_scalar(out, self, exponent);
}
Tensor pow_Tensor_Scalar(const Tensor & self, Scalar exponent) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::pow_sparse_scalar(self, exponent);
}
Tensor & zero_(Tensor & self) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::zero_sparse_(self);
}
Tensor & sub_out_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sub_out_sparse(out, self, other, alpha);
}
Tensor sub_Tensor(const Tensor & self, const Tensor & other, Scalar alpha) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sub_sparse(self, other, alpha);
}
Tensor & sub__Tensor(Tensor & self, const Tensor & other, Scalar alpha) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sub_sparse_(self, other, alpha);
}
Tensor & addmm_out_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addmm_out_sparse_dense_cuda(out, self, mat1, mat2, beta, alpha);
}
Tensor addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addmm_sparse_dense_cuda(self, mat1, mat2, beta, alpha);
}
Tensor & addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::s_addmm_sparse_dense_cuda_(self, mat1, mat2, beta, alpha);
}
Tensor _sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) {
    globalContext().lazyInitCUDA();
    const DeviceGuard device_guard(options.device());
    return at::native::new_with_dims_sparse(sparse_dim, dense_dim, size, options);
}
Tensor _sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) {
    globalContext().lazyInitCUDA();
    const DeviceGuard device_guard(options.device());
    return at::native::new_with_dims_and_tensor_sparse(sparse_dim, dense_dim, size, indices, values, options);
}
Tensor & sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sparse_resize_(self, size, sparse_dim, dense_dim);
}
Tensor & sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sparse_resize_and_clear_(self, size, sparse_dim, dense_dim);
}
Tensor sparse_mask(const Tensor & self, const Tensor & mask) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sparse_mask_cuda(self, mask);
}
Tensor to_dense(const Tensor & self) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sparse_to_dense(self);
}
int64_t sparse_dim(const Tensor & self) {

    // DeviceGuard omitted
    return at::native::sparse_dim_sparse(self);
}
int64_t _dimI(const Tensor & self) {

    // DeviceGuard omitted
    return at::native::sparse_dim_sparse(self);
}
int64_t dense_dim(const Tensor & self) {

    // DeviceGuard omitted
    return at::native::dense_dim_sparse(self);
}
int64_t _dimV(const Tensor & self) {

    // DeviceGuard omitted
    return at::native::dense_dim_sparse(self);
}
int64_t _nnz(const Tensor & self) {

    // DeviceGuard omitted
    return at::native::_nnz_sparse(self);
}
Tensor coalesce(const Tensor & self) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::coalesce_sparse_cuda(self);
}
bool is_coalesced(const Tensor & self) {

    // DeviceGuard omitted
    return at::native::is_coalesced_sparse(self);
}
Tensor _indices(const Tensor & self) {

    // DeviceGuard omitted
    return at::native::_indices_sparse(self);
}
Tensor _values(const Tensor & self) {

    // DeviceGuard omitted
    return at::native::_values_sparse(self);
}
Tensor & _coalesced_(Tensor & self, bool coalesced) {

    // DeviceGuard omitted
    return at::native::_coalesced_sparse_(self, coalesced);
}
Tensor indices(const Tensor & self) {

    // DeviceGuard omitted
    return at::native::indices_sparse(self);
}
Tensor values(const Tensor & self) {

    // DeviceGuard omitted
    return at::native::values_sparse(self);
}
Tensor & hspmm_out_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) {

    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::hspmm_out_sparse_cuda(out, mat1, mat2);
}
Tensor hspmm(const Tensor & mat1, const Tensor & mat2) {

    const OptionalDeviceGuard device_guard(device_of(mat1));
    return at::native::hspmm_sparse_cuda(mat1, mat2);
}
Tensor & copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::copy_sparse_(self, src, non_blocking);
}
Tensor index_select(const Tensor & self, int64_t dim, const Tensor & index) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::index_select_sparse(self, dim, index);
}
Tensor any(const Tensor & self) {

    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::any_sparse(self);
}

}  // namespace SparseCUDAType

TORCH_LIBRARY_IMPL(aten, SparseCUDA, m) {
  m.impl("add.Tensor",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::add_Tensor)))
  );
  m.impl("add_.Tensor",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::add__Tensor))
  );
  m.impl("add.out",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::add_out_out))
  );
  m.impl("bmm",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::bmm)))
  );
  m.impl("_bmm",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::_bmm)))
  );
  m.impl("bmm.out",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::bmm_out_out))
  );
  m.impl("_bmm.out",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::_bmm_out_out))
  );
  m.impl("div.Tensor",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::div_Tensor)))
  );
  m.impl("div_.Tensor",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::div__Tensor))
  );
  m.impl("div.out",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::div_out_out))
  );
  m.impl("empty.memory_format",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::empty_memory_format)))
  );
  m.impl("floor_divide",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::floor_divide)))
  );
  m.impl("floor_divide_.Tensor",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::floor_divide__Tensor))
  );
  m.impl("floor_divide.out",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::floor_divide_out_out))
  );
  m.impl("isnan",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::isnan)))
  );
  m.impl("log1p_",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::log1p_))
  );
  m.impl("log1p.out",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::log1p_out_out))
  );
  m.impl("mm",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::mm)))
  );
  m.impl("mm.out",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::mm_out_out))
  );
  m.impl("mul.Tensor",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::mul_Tensor)))
  );
  m.impl("mul_.Tensor",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::mul__Tensor))
  );
  m.impl("mul.out",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::mul_out_out))
  );
  m.impl("mv",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::mv)))
  );
  m.impl("narrow_copy",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::narrow_copy)))
  );
  m.impl("sspaddmm.out",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::sspaddmm_out_out))
  );
  m.impl("true_divide.Tensor",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::true_divide_Tensor)))
  );
  m.impl("true_divide_.Tensor",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::true_divide__Tensor))
  );
  m.impl("true_divide.out",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::true_divide_out_out))
  );
  m.impl("native_norm",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::native_norm)))
  );
  m.impl("_sparse_sum_backward",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::_sparse_sum_backward)))
  );
  m.impl("clone",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::clone)))
  );
  m.impl("pow.Tensor_Scalar_out",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::pow_out_Tensor_Scalar_out))
  );
  m.impl("pow.Tensor_Scalar",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::pow_Tensor_Scalar)))
  );
  m.impl("zero_",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::zero_))
  );
  m.impl("sub.out",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::sub_out_out))
  );
  m.impl("sub.Tensor",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::sub_Tensor)))
  );
  m.impl("sub_.Tensor",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::sub__Tensor))
  );
  m.impl("addmm.out",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::addmm_out_out))
  );
  m.impl("addmm",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::addmm)))
  );
  m.impl("addmm_",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::addmm_))
  );
  m.impl("_sparse_coo_tensor_with_dims",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::_sparse_coo_tensor_with_dims))
  );
  m.impl("_sparse_coo_tensor_with_dims_and_tensors",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::_sparse_coo_tensor_with_dims_and_tensors))
  );
  m.impl("sparse_resize_",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::sparse_resize_))
  );
  m.impl("sparse_resize_and_clear_",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::sparse_resize_and_clear_))
  );
  m.impl("sparse_mask",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::sparse_mask)))
  );
  m.impl("to_dense",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::to_dense)))
  );
  m.impl("sparse_dim",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::sparse_dim)))
  );
  m.impl("_dimI",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::_dimI)))
  );
  m.impl("dense_dim",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::dense_dim)))
  );
  m.impl("_dimV",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::_dimV)))
  );
  m.impl("_nnz",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::_nnz)))
  );
  m.impl("coalesce",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::coalesce)))
  );
  m.impl("is_coalesced",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::is_coalesced)))
  );
  m.impl("_indices",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::_indices)))
  );
  m.impl("_values",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::_values)))
  );
  m.impl("_coalesced_",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::_coalesced_))
  );
  m.impl("indices",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::indices)))
  );
  m.impl("values",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::values)))
  );
  m.impl("hspmm.out",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::hspmm_out_out))
  );
  m.impl("hspmm",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::hspmm)))
  );
  m.impl("copy_sparse_to_sparse_",
         torch::dispatch(DispatchKey::SparseCUDA,
                         torch::CppFunction::makeUnboxedOnly(&SparseCUDAType::copy_sparse_to_sparse_))
  );
  m.impl("index_select",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::index_select)))
  );
  m.impl("any",
         torch::dispatch(DispatchKey::SparseCUDA,
                         c10::impl::hacky_wrapper_for_legacy_signatures(TORCH_FN(SparseCUDAType::any)))
  );;
}

} // namespace at
