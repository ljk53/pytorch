// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#define __STDC_FORMAT_MACROS

#include <ATen/Type.h>

// @generated by aten/src/ATen/gen.py

#include <TH/TH.h>
#include <TH/THTensor.hpp>
#include <THNN/THNN.h>
#undef THNN_
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NativeFunctions.h>
#include <ATen/Utils.h>
#include <ATen/ExpandUtils.h>
#include <ATen/CPUGenerator.h>
#include <ATen/Context.h>
#include <ATen/CheckGenerator.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/Half.h>
#include <c10/core/TensorImpl.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>

namespace at {

static inline ScalarType infer_scalar_type(const Tensor & t) {
   return t.scalar_type();
 }

static inline ScalarType infer_scalar_type(const TensorList & tl) {
   AT_CHECK(tl.size() > 0, "expected a non-empty list of Tensors");
   return tl[0].scalar_type();
 }

Tensor & Type::_th_masked_fill_(Tensor & self, const Tensor & mask, Scalar value) const {
    // DeviceGuard omitted
    Tensor b_mask;
    std::tie(b_mask) = expand_inplace(self, mask, "_th_masked_fill_");
    return s__th_masked_fill_(self, b_mask, value);
}
Tensor & Type::_th_masked_fill_(Tensor & self, const Tensor & mask, const Tensor & value) const {
    // DeviceGuard omitted
    Tensor b_mask;
    std::tie(b_mask) = expand_inplace(self, mask, "_th_masked_fill_");
    return s__th_masked_fill_(self, b_mask, value);
}
Tensor & Type::_th_masked_scatter_(Tensor & self, const Tensor & mask, const Tensor & source) const {
    // DeviceGuard omitted
    Tensor b_mask;
    std::tie(b_mask) = expand_inplace(self, mask, "_th_masked_scatter_");
    return s__th_masked_scatter_(self, b_mask, source);
}
Tensor & Type::_th_masked_select_out(Tensor & result, const Tensor & self, const Tensor & mask) const {
    // DeviceGuard omitted
    Tensor b_self, b_mask;
    std::tie(b_self, b_mask) = expand_outplace(self, mask, "_th_masked_select_out");
    return s__th_masked_select_out(result, b_self, b_mask);
}
Tensor Type::_th_masked_select(const Tensor & self, const Tensor & mask) const {
    // DeviceGuard omitted
    Tensor b_self, b_mask;
    std::tie(b_self, b_mask) = expand_outplace(self, mask, "_th_masked_select");
    return s__th_masked_select(b_self, b_mask);
}
Tensor & Type::_th_and_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_and_out");
    return s__th_and_out(result, b_self, b_other);
}
Tensor Type::_th_and(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_and");
    return s__th_and(b_self, b_other);
}
Tensor & Type::_th_iand_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_iand_");
    return s__th_iand_(self, b_other);
}
Tensor & Type::_th_or_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_or_out");
    return s__th_or_out(result, b_self, b_other);
}
Tensor Type::_th_or(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_or");
    return s__th_or(b_self, b_other);
}
Tensor & Type::_th_ior_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_ior_");
    return s__th_ior_(self, b_other);
}
Tensor & Type::_th_xor_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_xor_out");
    return s__th_xor_out(result, b_self, b_other);
}
Tensor Type::_th_xor(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_xor");
    return s__th_xor(b_self, b_other);
}
Tensor & Type::_th_ixor_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_ixor_");
    return s__th_ixor_(self, b_other);
}
Tensor & Type::_th_lshift_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_lshift_out");
    return s__th_lshift_out(result, b_self, b_other);
}
Tensor Type::_th_lshift(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_lshift");
    return s__th_lshift(b_self, b_other);
}
Tensor & Type::_th_ilshift_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_ilshift_");
    return s__th_ilshift_(self, b_other);
}
Tensor & Type::_th_rshift_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_rshift_out");
    return s__th_rshift_out(result, b_self, b_other);
}
Tensor Type::_th_rshift(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_rshift");
    return s__th_rshift(b_self, b_other);
}
Tensor & Type::_th_irshift_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_irshift_");
    return s__th_irshift_(self, b_other);
}
Tensor & Type::_th_lt_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_lt_out");
    return s__th_lt_out(result, b_self, b_other);
}
Tensor Type::_th_lt(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_lt");
    return s__th_lt(b_self, b_other);
}
Tensor & Type::_th_lt_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_lt_");
    return s__th_lt_(self, b_other);
}
Tensor & Type::_th_gt_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_gt_out");
    return s__th_gt_out(result, b_self, b_other);
}
Tensor Type::_th_gt(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_gt");
    return s__th_gt(b_self, b_other);
}
Tensor & Type::_th_gt_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_gt_");
    return s__th_gt_(self, b_other);
}
Tensor & Type::_th_le_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_le_out");
    return s__th_le_out(result, b_self, b_other);
}
Tensor Type::_th_le(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_le");
    return s__th_le(b_self, b_other);
}
Tensor & Type::_th_le_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_le_");
    return s__th_le_(self, b_other);
}
Tensor & Type::_th_ge_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_ge_out");
    return s__th_ge_out(result, b_self, b_other);
}
Tensor Type::_th_ge(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_ge");
    return s__th_ge(b_self, b_other);
}
Tensor & Type::_th_ge_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_ge_");
    return s__th_ge_(self, b_other);
}
Tensor & Type::_th_eq_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_eq_out");
    return s__th_eq_out(result, b_self, b_other);
}
Tensor Type::_th_eq(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_eq");
    return s__th_eq(b_self, b_other);
}
Tensor & Type::_th_eq_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_eq_");
    return s__th_eq_(self, b_other);
}
Tensor & Type::_th_ne_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_ne_out");
    return s__th_ne_out(result, b_self, b_other);
}
Tensor Type::_th_ne(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_ne");
    return s__th_ne(b_self, b_other);
}
Tensor & Type::_th_ne_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_ne_");
    return s__th_ne_(self, b_other);
}
Tensor & Type::_th_min_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_min_out");
    return s__th_min_out(result, b_self, b_other);
}
Tensor Type::_th_min(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_min");
    return s__th_min(b_self, b_other);
}
Tensor & Type::_th_max_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_max_out");
    return s__th_max_out(result, b_self, b_other);
}
Tensor Type::_th_max(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_max");
    return s__th_max(b_self, b_other);
}
Tensor Type::_th_dist(const Tensor & self, const Tensor & other, Scalar p) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_dist");
    return s__th_dist(b_self, b_other, p);
}
Tensor & Type::_th_atan2_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_atan2_out");
    return s__th_atan2_out(result, b_self, b_other);
}
Tensor Type::_th_atan2(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_atan2");
    return s__th_atan2(b_self, b_other);
}
Tensor & Type::_th_atan2_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_atan2_");
    return s__th_atan2_(self, b_other);
}
Tensor & Type::_th_pow_out(Tensor & result, const Tensor & self, const Tensor & exponent) const {
    // DeviceGuard omitted
    Tensor b_self, b_exponent;
    std::tie(b_self, b_exponent) = expand_outplace(self, exponent, "_th_pow_out");
    return s__th_pow_out(result, b_self, b_exponent);
}
Tensor Type::_th_pow(const Tensor & self, const Tensor & exponent) const {
    // DeviceGuard omitted
    Tensor b_self, b_exponent;
    std::tie(b_self, b_exponent) = expand_outplace(self, exponent, "_th_pow");
    return s__th_pow(b_self, b_exponent);
}
Tensor & Type::_th_pow_(Tensor & self, const Tensor & exponent) const {
    // DeviceGuard omitted
    Tensor b_exponent;
    std::tie(b_exponent) = expand_inplace(self, exponent, "_th_pow_");
    return s__th_pow_(self, b_exponent);
}
Tensor & Type::_th_fmod_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_fmod_out");
    return s__th_fmod_out(result, b_self, b_other);
}
Tensor Type::_th_fmod(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_fmod");
    return s__th_fmod(b_self, b_other);
}
Tensor & Type::_th_fmod_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_fmod_");
    return s__th_fmod_(self, b_other);
}
Tensor & Type::_th_remainder_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_remainder_out");
    return s__th_remainder_out(result, b_self, b_other);
}
Tensor Type::_th_remainder(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_remainder");
    return s__th_remainder(b_self, b_other);
}
Tensor & Type::_th_remainder_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_remainder_");
    return s__th_remainder_(self, b_other);
}
Tensor & Type::_th_addmm_out(Tensor & result, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {mat1.size(0),mat2.size(1)}, "_th_addmm_out");
    return s__th_addmm_out(result, b_self, mat1, mat2, beta, alpha);
}
Tensor Type::_th_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {mat1.size(0),mat2.size(1)}, "_th_addmm");
    return s__th_addmm(b_self, mat1, mat2, beta, alpha);
}
Tensor & Type::_th_addmv_out(Tensor & result, const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {mat.size(0)}, "_th_addmv_out");
    return s__th_addmv_out(result, b_self, mat, vec, beta, alpha);
}
Tensor Type::_th_addmv(const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {mat.size(0)}, "_th_addmv");
    return s__th_addmv(b_self, mat, vec, beta, alpha);
}
Tensor & Type::_th_addr_out(Tensor & result, const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {vec1.size(0),vec2.size(0)}, "_th_addr_out");
    return s__th_addr_out(result, b_self, vec1, vec2, beta, alpha);
}
Tensor Type::_th_addr(const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {vec1.size(0),vec2.size(0)}, "_th_addr");
    return s__th_addr(b_self, vec1, vec2, beta, alpha);
}
Tensor & Type::_th_addbmm_out(Tensor & result, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {batch1.size(1),batch2.size(2)}, "_th_addbmm_out");
    return s__th_addbmm_out(result, b_self, batch1, batch2, beta, alpha);
}
Tensor Type::_th_addbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {batch1.size(1),batch2.size(2)}, "_th_addbmm");
    return s__th_addbmm(b_self, batch1, batch2, beta, alpha);
}
Tensor & Type::_th_baddbmm_out(Tensor & result, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {batch1.size(0),batch1.size(1),batch2.size(2)}, "_th_baddbmm_out");
    return s__th_baddbmm_out(result, b_self, batch1, batch2, beta, alpha);
}
Tensor Type::_th_baddbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {batch1.size(0),batch1.size(1),batch2.size(2)}, "_th_baddbmm");
    return s__th_baddbmm(b_self, batch1, batch2, beta, alpha);
}
Tensor & Type::_th_addcmul_out(Tensor & result, const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    Tensor b_self, b_tensor1, b_tensor2;
    std::tie(b_self, b_tensor1, b_tensor2) = expand_outplace(self, tensor1, tensor2, "_th_addcmul_out");
    return s__th_addcmul_out(result, b_self, b_tensor1, b_tensor2, value);
}
Tensor Type::_th_addcmul(const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    Tensor b_self, b_tensor1, b_tensor2;
    std::tie(b_self, b_tensor1, b_tensor2) = expand_outplace(self, tensor1, tensor2, "_th_addcmul");
    return s__th_addcmul(b_self, b_tensor1, b_tensor2, value);
}
Tensor & Type::_th_addcmul_(Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    Tensor b_tensor1, b_tensor2;
    std::tie(b_tensor1, b_tensor2) = expand_inplace(self, tensor1, tensor2, "_th_addcmul_");
    return s__th_addcmul_(self, b_tensor1, b_tensor2, value);
}
Tensor & Type::_th_addcdiv_out(Tensor & result, const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    Tensor b_self, b_tensor1, b_tensor2;
    std::tie(b_self, b_tensor1, b_tensor2) = expand_outplace(self, tensor1, tensor2, "_th_addcdiv_out");
    return s__th_addcdiv_out(result, b_self, b_tensor1, b_tensor2, value);
}
Tensor Type::_th_addcdiv(const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    Tensor b_self, b_tensor1, b_tensor2;
    std::tie(b_self, b_tensor1, b_tensor2) = expand_outplace(self, tensor1, tensor2, "_th_addcdiv");
    return s__th_addcdiv(b_self, b_tensor1, b_tensor2, value);
}
Tensor & Type::_th_addcdiv_(Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    Tensor b_tensor1, b_tensor2;
    std::tie(b_tensor1, b_tensor2) = expand_inplace(self, tensor1, tensor2, "_th_addcdiv_");
    return s__th_addcdiv_(self, b_tensor1, b_tensor2, value);
}
Tensor & Type::_thnn_elu_(Tensor & self, Scalar alpha, Scalar scale, Scalar input_scale) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return _thnn_elu_forward_(self, alpha, scale, input_scale);
}
Tensor & Type::_thnn_hardtanh_(Tensor & self, Scalar min_val, Scalar max_val) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return _thnn_hardtanh_forward_(self, min_val, max_val);
}
Tensor & Type::_thnn_leaky_relu_(Tensor & self, Scalar negative_slope) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return _thnn_leaky_relu_forward_(self, negative_slope);
}
Tensor & Type::_thnn_rrelu_with_noise_(Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return _thnn_rrelu_with_noise_forward_(self, noise, lower, upper, training, generator);
}
Tensor Type::_cast_Byte(const Tensor & self, bool non_blocking) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_cast_Byte(/* native_actuals */ self, non_blocking);
}
Tensor Type::_cast_Char(const Tensor & self, bool non_blocking) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_cast_Char(/* native_actuals */ self, non_blocking);
}
Tensor Type::_cast_Double(const Tensor & self, bool non_blocking) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_cast_Double(/* native_actuals */ self, non_blocking);
}
Tensor Type::_cast_Float(const Tensor & self, bool non_blocking) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_cast_Float(/* native_actuals */ self, non_blocking);
}
Tensor Type::_cast_Int(const Tensor & self, bool non_blocking) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_cast_Int(/* native_actuals */ self, non_blocking);
}
Tensor Type::_cast_Long(const Tensor & self, bool non_blocking) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_cast_Long(/* native_actuals */ self, non_blocking);
}
Tensor Type::_cast_Short(const Tensor & self, bool non_blocking) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_cast_Short(/* native_actuals */ self, non_blocking);
}
Tensor Type::_cast_Half(const Tensor & self, bool non_blocking) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_cast_Half(/* native_actuals */ self, non_blocking);
}
int64_t Type::_debug_has_internal_overlap(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_debug_has_internal_overlap(/* native_actuals */ self);
}
std::tuple<Tensor,Tensor> Type::_sobol_engine_draw(const Tensor & quasi, int64_t n, const Tensor & sobolstate, int64_t dimension, int64_t num_generated, c10::optional<ScalarType> dtype) const {
    const OptionalDeviceGuard device_guard(device_of(quasi));
    return at::native::_sobol_engine_draw(/* native_actuals */ quasi, n, sobolstate, dimension, num_generated, dtype);
}
Tensor & Type::_sobol_engine_ff_(Tensor & self, int64_t n, const Tensor & sobolstate, int64_t dimension, int64_t num_generated) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sobol_engine_ff_(/* native_actuals */ self, n, sobolstate, dimension, num_generated);
}
Tensor & Type::_sobol_engine_scramble_(Tensor & self, const Tensor & ltm, int64_t dimension) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sobol_engine_scramble_(/* native_actuals */ self, ltm, dimension);
}
Tensor & Type::_sobol_engine_initialize_state_(Tensor & self, int64_t dimension) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sobol_engine_initialize_state_(/* native_actuals */ self, dimension);
}
Tensor Type::_reshape_from_tensor(const Tensor & self, const Tensor & shape) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_reshape_from_tensor(/* native_actuals */ self, shape);
}
Tensor Type::_shape_as_tensor(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_shape_as_tensor(/* native_actuals */ self);
}
Tensor Type::dropout(const Tensor & input, double p, bool train) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::dropout(/* native_actuals */ input, p, train);
}
Tensor & Type::dropout_(Tensor & self, double p, bool train) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::dropout_(/* native_actuals */ self, p, train);
}
Tensor Type::feature_dropout(const Tensor & input, double p, bool train) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::feature_dropout(/* native_actuals */ input, p, train);
}
Tensor & Type::feature_dropout_(Tensor & self, double p, bool train) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::feature_dropout_(/* native_actuals */ self, p, train);
}
Tensor Type::alpha_dropout(const Tensor & input, double p, bool train) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::alpha_dropout(/* native_actuals */ input, p, train);
}
Tensor & Type::alpha_dropout_(Tensor & self, double p, bool train) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::alpha_dropout_(/* native_actuals */ self, p, train);
}
Tensor Type::feature_alpha_dropout(const Tensor & input, double p, bool train) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::feature_alpha_dropout(/* native_actuals */ input, p, train);
}
Tensor & Type::feature_alpha_dropout_(Tensor & self, double p, bool train) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::feature_alpha_dropout_(/* native_actuals */ self, p, train);
}
Tensor Type::abs(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::abs(/* native_actuals */ self);
}
Tensor Type::acos(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::acos(/* native_actuals */ self);
}
Tensor Type::avg_pool1d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::avg_pool1d(/* native_actuals */ self, kernel_size, stride, padding, ceil_mode, count_include_pad);
}
Tensor Type::adaptive_avg_pool1d(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::adaptive_avg_pool1d(/* native_actuals */ self, output_size);
}
std::tuple<Tensor,Tensor> Type::adaptive_max_pool1d(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::adaptive_max_pool1d(/* native_actuals */ self, output_size);
}
Tensor Type::add(const Tensor & self, Scalar other, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::add(/* native_actuals */ self, other, alpha);
}
Tensor & Type::add_(Tensor & self, Scalar other, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::add_(/* native_actuals */ self, other, alpha);
}
Tensor Type::addmv(const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addmv(/* native_actuals */ self, mat, vec, beta, alpha);
}
Tensor & Type::addmv_(Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addmv_(/* native_actuals */ self, mat, vec, beta, alpha);
}
Tensor & Type::addmv_out(Tensor & out, const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addmv_out(/* native_actuals */ out, self, mat, vec, beta, alpha);
}
Tensor Type::addr(const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addr(/* native_actuals */ self, vec1, vec2, beta, alpha);
}
Tensor & Type::addr_(Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addr_(/* native_actuals */ self, vec1, vec2, beta, alpha);
}
Tensor & Type::addr_out(Tensor & out, const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addr_out(/* native_actuals */ out, self, vec1, vec2, beta, alpha);
}
Tensor Type::affine_grid_generator(const Tensor & theta, IntArrayRef size) const {
    const OptionalDeviceGuard device_guard(device_of(theta));
    return at::native::affine_grid_generator(/* native_actuals */ theta, size);
}
Tensor Type::affine_grid_generator_backward(const Tensor & grad, IntArrayRef size) const {
    const OptionalDeviceGuard device_guard(device_of(grad));
    return at::native::affine_grid_generator_backward(/* native_actuals */ grad, size);
}
Tensor Type::all(const Tensor & self, int64_t dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::all(/* native_actuals */ self, dim, keepdim);
}
Tensor & Type::all_out(Tensor & out, const Tensor & self, int64_t dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::all_out(/* native_actuals */ out, self, dim, keepdim);
}
bool Type::allclose(const Tensor & self, const Tensor & other, double rtol, double atol, bool equal_nan) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::allclose(/* native_actuals */ self, other, rtol, atol, equal_nan);
}
Tensor Type::any(const Tensor & self, int64_t dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::any(/* native_actuals */ self, dim, keepdim);
}
Tensor & Type::any_out(Tensor & out, const Tensor & self, int64_t dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::any_out(/* native_actuals */ out, self, dim, keepdim);
}
Tensor Type::arange(Scalar end, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::arange(/* native_actuals */ end, options);
}
Tensor Type::arange(Scalar start, Scalar end, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::arange(/* native_actuals */ start, end, options);
}
Tensor Type::arange(Scalar start, Scalar end, Scalar step, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::arange(/* native_actuals */ start, end, step, options);
}
Tensor & Type::arange_out(Tensor & out, Scalar end) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::arange_out(/* native_actuals */ out, end);
}
Tensor Type::_dim_arange(const Tensor & like, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(like));
    return at::native::_dim_arange(/* native_actuals */ like, dim);
}
Tensor Type::argmax(const Tensor & self, c10::optional<int64_t> dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::argmax(/* native_actuals */ self, dim, keepdim);
}
Tensor Type::argmin(const Tensor & self, c10::optional<int64_t> dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::argmin(/* native_actuals */ self, dim, keepdim);
}
Tensor & Type::as_strided_(Tensor & self, IntArrayRef size, IntArrayRef stride, c10::optional<int64_t> storage_offset) const {
    // DeviceGuard omitted
    return at::native::as_strided_(/* native_actuals */ self, size, stride, storage_offset);
}
Tensor Type::asin(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::asin(/* native_actuals */ self);
}
Tensor Type::atan(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::atan(/* native_actuals */ self);
}
Tensor & Type::_baddbmm_mkl_(Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_baddbmm_mkl_(/* native_actuals */ self, batch1, batch2, beta, alpha);
}
Tensor Type::bartlett_window(int64_t window_length, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::bartlett_window(/* native_actuals */ window_length, options);
}
Tensor Type::bartlett_window(int64_t window_length, bool periodic, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::bartlett_window(/* native_actuals */ window_length, periodic, options);
}
Tensor Type::batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps, bool cudnn_enabled) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::batch_norm(/* native_actuals */ input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled);
}
std::tuple<Tensor,Tensor,Tensor,int64_t> Type::_batch_norm_impl_index(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps, bool cudnn_enabled) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::_batch_norm_impl_index(/* native_actuals */ input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled);
}
std::tuple<Tensor,Tensor,Tensor> Type::_batch_norm_impl_index_backward(int64_t impl_index, const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var_transform, bool train, double eps, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::_batch_norm_impl_index_backward(/* native_actuals */ impl_index, input, grad_output, weight, running_mean, running_var, save_mean, save_var_transform, train, eps, output_mask);
}
Tensor Type::bernoulli(const Tensor & self, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::bernoulli(/* native_actuals */ self, generator);
}
Tensor & Type::bernoulli_out(Tensor & out, const Tensor & self, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::bernoulli_out(/* native_actuals */ out, self, generator);
}
Tensor Type::bernoulli(const Tensor & self, double p, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::bernoulli(/* native_actuals */ self, p, generator);
}
Tensor Type::bilinear(const Tensor & input1, const Tensor & input2, const Tensor & weight, const Tensor & bias) const {
    const OptionalDeviceGuard device_guard(device_of(input1));
    return at::native::bilinear(/* native_actuals */ input1, input2, weight, bias);
}
Tensor Type::binary_cross_entropy_with_logits(const Tensor & self, const Tensor & target, const Tensor & weight, const Tensor & pos_weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::binary_cross_entropy_with_logits(/* native_actuals */ self, target, weight, pos_weight, reduction);
}
Tensor Type::binary_cross_entropy_with_logits_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, const Tensor & pos_weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::binary_cross_entropy_with_logits_backward(/* native_actuals */ grad_output, self, target, weight, pos_weight, reduction);
}
Tensor Type::blackman_window(int64_t window_length, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::blackman_window(/* native_actuals */ window_length, options);
}
Tensor Type::blackman_window(int64_t window_length, bool periodic, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::blackman_window(/* native_actuals */ window_length, periodic, options);
}
std::vector<Tensor> Type::broadcast_tensors(TensorList tensors) const {
    // DeviceGuard omitted
    return at::native::broadcast_tensors(/* native_actuals */ tensors);
}
Tensor Type::cat(TensorList tensors, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(tensors));
    return at::native::cat(/* native_actuals */ tensors, dim);
}
Tensor & Type::cat_out(Tensor & out, TensorList tensors, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::cat_out(/* native_actuals */ out, tensors, dim);
}
Tensor Type::ceil(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::ceil(/* native_actuals */ self);
}
Tensor Type::chain_matmul(TensorList matrices) const {
    const OptionalDeviceGuard device_guard(device_of(matrices));
    return at::native::chain_matmul(/* native_actuals */ matrices);
}
std::vector<Tensor> Type::chunk(const Tensor & self, int64_t chunks, int64_t dim) const {
    // DeviceGuard omitted
    return at::native::chunk(/* native_actuals */ self, chunks, dim);
}
Tensor Type::clamp(const Tensor & self, c10::optional<Scalar> min, c10::optional<Scalar> max) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::clamp(/* native_actuals */ self, min, max);
}
Tensor Type::clamp_max(const Tensor & self, Scalar max) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::clamp_max(/* native_actuals */ self, max);
}
Tensor Type::clamp_min(const Tensor & self, Scalar min) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::clamp_min(/* native_actuals */ self, min);
}
bool Type::cudnn_is_acceptable(const Tensor & self) const {
    // DeviceGuard omitted
    return at::native::cudnn_is_acceptable(/* native_actuals */ self);
}
Tensor Type::constant_pad_nd(const Tensor & self, IntArrayRef pad, Scalar value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::constant_pad_nd(/* native_actuals */ self, pad, value);
}
Tensor Type::contiguous(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::contiguous(/* native_actuals */ self);
}
Tensor Type::convolution(const Tensor & input, const Tensor & weight, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool transposed, IntArrayRef output_padding, int64_t groups) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::convolution(/* native_actuals */ input, weight, bias, stride, padding, dilation, transposed, output_padding, groups);
}
Tensor Type::_convolution(const Tensor & input, const Tensor & weight, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool transposed, IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::_convolution(/* native_actuals */ input, weight, bias, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled);
}
Tensor Type::_convolution_nogroup(const Tensor & input, const Tensor & weight, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool transposed, IntArrayRef output_padding) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::_convolution_nogroup(/* native_actuals */ input, weight, bias, stride, padding, dilation, transposed, output_padding);
}
std::tuple<Tensor,Tensor,Tensor> Type::_convolution_double_backward(const Tensor & ggI, const Tensor & ggW, const Tensor & ggb, const Tensor & gO, const Tensor & weight, const Tensor & self, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool transposed, IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_convolution_double_backward(/* native_actuals */ ggI, ggW, ggb, gO, weight, self, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled, output_mask);
}
Tensor Type::conv1d(const Tensor & input, const Tensor & weight, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, int64_t groups) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::conv1d(/* native_actuals */ input, weight, bias, stride, padding, dilation, groups);
}
Tensor Type::conv2d(const Tensor & input, const Tensor & weight, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, int64_t groups) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::conv2d(/* native_actuals */ input, weight, bias, stride, padding, dilation, groups);
}
Tensor Type::conv3d(const Tensor & input, const Tensor & weight, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, int64_t groups) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::conv3d(/* native_actuals */ input, weight, bias, stride, padding, dilation, groups);
}
Tensor Type::conv_tbc(const Tensor & self, const Tensor & weight, const Tensor & bias, int64_t pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::conv_tbc(/* native_actuals */ self, weight, bias, pad);
}
std::tuple<Tensor,Tensor,Tensor> Type::conv_tbc_backward(const Tensor & self, const Tensor & input, const Tensor & weight, const Tensor & bias, int64_t pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::conv_tbc_backward(/* native_actuals */ self, input, weight, bias, pad);
}
Tensor Type::conv_transpose1d(const Tensor & input, const Tensor & weight, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, int64_t groups, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::conv_transpose1d(/* native_actuals */ input, weight, bias, stride, padding, output_padding, groups, dilation);
}
Tensor Type::conv_transpose2d(const Tensor & input, const Tensor & weight, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, int64_t groups, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::conv_transpose2d(/* native_actuals */ input, weight, bias, stride, padding, output_padding, groups, dilation);
}
Tensor Type::conv_transpose3d(const Tensor & input, const Tensor & weight, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, int64_t groups, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::conv_transpose3d(/* native_actuals */ input, weight, bias, stride, padding, output_padding, groups, dilation);
}
Tensor & Type::copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
    // DeviceGuard omitted
    return at::native::copy_(/* native_actuals */ self, src, non_blocking);
}
Tensor Type::cos(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::cos(/* native_actuals */ self);
}
Tensor Type::cosh(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::cosh(/* native_actuals */ self);
}
Tensor Type::cosine_embedding_loss(const Tensor & input1, const Tensor & input2, const Tensor & target, double margin, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(input1));
    return at::native::cosine_embedding_loss(/* native_actuals */ input1, input2, target, margin, reduction);
}
Tensor Type::cumsum(const Tensor & self, int64_t dim, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::cumsum(/* native_actuals */ self, dim, dtype);
}
Tensor Type::cumsum(const Tensor & self, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::cumsum(/* native_actuals */ self, dim);
}
Tensor & Type::cumsum_out(Tensor & out, const Tensor & self, int64_t dim, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::cumsum_out(/* native_actuals */ out, self, dim, dtype);
}
Tensor & Type::cumsum_out(Tensor & out, const Tensor & self, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::cumsum_out(/* native_actuals */ out, self, dim);
}
Tensor Type::cumprod(const Tensor & self, int64_t dim, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::cumprod(/* native_actuals */ self, dim, dtype);
}
Tensor Type::cumprod(const Tensor & self, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::cumprod(/* native_actuals */ self, dim);
}
Tensor & Type::cumprod_out(Tensor & out, const Tensor & self, int64_t dim, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::cumprod_out(/* native_actuals */ out, self, dim, dtype);
}
Tensor & Type::cumprod_out(Tensor & out, const Tensor & self, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::cumprod_out(/* native_actuals */ out, self, dim);
}
Tensor Type::ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, int64_t reduction, bool zero_infinity) const {
    const OptionalDeviceGuard device_guard(device_of(log_probs));
    return at::native::ctc_loss(/* native_actuals */ log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity);
}
Tensor Type::ctc_loss(const Tensor & log_probs, const Tensor & targets, const Tensor & input_lengths, const Tensor & target_lengths, int64_t blank, int64_t reduction, bool zero_infinity) const {
    const OptionalDeviceGuard device_guard(device_of(log_probs));
    return at::native::ctc_loss(/* native_actuals */ log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity);
}
Tensor Type::det(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::det(/* native_actuals */ self);
}
Tensor Type::diag_embed(const Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::diag_embed(/* native_actuals */ self, offset, dim1, dim2);
}
Tensor Type::diagflat(const Tensor & self, int64_t offset) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::diagflat(/* native_actuals */ self, offset);
}
Tensor Type::diagonal(const Tensor & self, int64_t offset, int64_t dim1, int64_t dim2) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::diagonal(/* native_actuals */ self, offset, dim1, dim2);
}
Tensor Type::div(const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::div(/* native_actuals */ self, other);
}
Tensor & Type::div_(Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::div_(/* native_actuals */ self, other);
}
Tensor & Type::div_out(Tensor & out, const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::div_out(/* native_actuals */ out, self, other);
}
Tensor Type::div(const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::div(/* native_actuals */ self, other);
}
Tensor & Type::div_(Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::div_(/* native_actuals */ self, other);
}
Tensor Type::dot(const Tensor & self, const Tensor & tensor) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::dot(/* native_actuals */ self, tensor);
}
Tensor & Type::dot_out(Tensor & out, const Tensor & self, const Tensor & tensor) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::dot_out(/* native_actuals */ out, self, tensor);
}
Tensor Type::einsum(std::string equation, TensorList tensors) const {
    const OptionalDeviceGuard device_guard(device_of(tensors));
    return at::native::einsum(/* native_actuals */ equation, tensors);
}
Tensor Type::embedding(const Tensor & weight, const Tensor & indices, int64_t padding_idx, bool scale_grad_by_freq, bool sparse) const {
    const OptionalDeviceGuard device_guard(device_of(weight));
    return at::native::embedding(/* native_actuals */ weight, indices, padding_idx, scale_grad_by_freq, sparse);
}
Tensor Type::embedding_backward(const Tensor & grad, const Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq, bool sparse) const {
    const OptionalDeviceGuard device_guard(device_of(grad));
    return at::native::embedding_backward(/* native_actuals */ grad, indices, num_weights, padding_idx, scale_grad_by_freq, sparse);
}
Tensor Type::embedding_sparse_backward(const Tensor & grad, const Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) const {
    const OptionalDeviceGuard device_guard(device_of(grad));
    return at::native::embedding_sparse_backward(/* native_actuals */ grad, indices, num_weights, padding_idx, scale_grad_by_freq);
}
std::tuple<Tensor,Tensor,Tensor,Tensor> Type::embedding_bag(const Tensor & weight, const Tensor & indices, const Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const Tensor & per_sample_weights) const {
    const OptionalDeviceGuard device_guard(device_of(weight));
    return at::native::embedding_bag(/* native_actuals */ weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights);
}
Tensor Type::_embedding_bag_backward(const Tensor & grad, const Tensor & indices, const Tensor & offsets, const Tensor & offset2bag, const Tensor & bag_size, const Tensor & maximum_indices, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, bool sparse, const Tensor & per_sample_weights) const {
    const OptionalDeviceGuard device_guard(device_of(grad));
    return at::native::_embedding_bag_backward(/* native_actuals */ grad, indices, offsets, offset2bag, bag_size, maximum_indices, num_weights, scale_grad_by_freq, mode, sparse, per_sample_weights);
}
Tensor Type::_embedding_bag_sparse_backward(const Tensor & grad, const Tensor & indices, const Tensor & offsets, const Tensor & offset2bag, const Tensor & bag_size, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, const Tensor & per_sample_weights) const {
    const OptionalDeviceGuard device_guard(device_of(grad));
    return at::native::_embedding_bag_sparse_backward(/* native_actuals */ grad, indices, offsets, offset2bag, bag_size, num_weights, scale_grad_by_freq, mode, per_sample_weights);
}
Tensor & Type::empty_out(Tensor & out, IntArrayRef size) const {
    // DeviceGuard omitted
    return at::native::empty_out(/* native_actuals */ out, size);
}
Tensor Type::empty_like(const Tensor & self) const {
    // DeviceGuard omitted
    return at::native::empty_like(/* native_actuals */ self);
}
Tensor Type::empty_like(const Tensor & self, const TensorOptions & options) const {
    // DeviceGuard omitted
    return at::native::empty_like(/* native_actuals */ self, options);
}
Tensor Type::erf(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::erf(/* native_actuals */ self);
}
Tensor Type::erfc(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::erfc(/* native_actuals */ self);
}
Tensor Type::exp(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::exp(/* native_actuals */ self);
}
Tensor Type::expm1(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::expm1(/* native_actuals */ self);
}
Tensor Type::expand(const Tensor & self, IntArrayRef size, bool implicit) const {
    // DeviceGuard omitted
    return at::native::expand(/* native_actuals */ self, size, implicit);
}
Tensor Type::expand_as(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    return at::native::expand_as(/* native_actuals */ self, other);
}
Tensor Type::eye(int64_t n, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::eye(/* native_actuals */ n, options);
}
Tensor Type::eye(int64_t n, int64_t m, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::eye(/* native_actuals */ n, m, options);
}
Tensor Type::flatten(const Tensor & self, int64_t start_dim, int64_t end_dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::flatten(/* native_actuals */ self, start_dim, end_dim);
}
Tensor & Type::fill_(Tensor & self, Scalar value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::fill_(/* native_actuals */ self, value);
}
Tensor & Type::fill_(Tensor & self, const Tensor & value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::fill_(/* native_actuals */ self, value);
}
Tensor Type::floor(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::floor(/* native_actuals */ self);
}
Tensor Type::frac(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::frac(/* native_actuals */ self);
}
Tensor Type::full(IntArrayRef size, Scalar fill_value, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::full(/* native_actuals */ size, fill_value, options);
}
Tensor & Type::full_out(Tensor & out, IntArrayRef size, Scalar fill_value) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::full_out(/* native_actuals */ out, size, fill_value);
}
Tensor Type::full_like(const Tensor & self, Scalar fill_value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::full_like(/* native_actuals */ self, fill_value);
}
Tensor Type::full_like(const Tensor & self, Scalar fill_value, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::full_like(/* native_actuals */ self, fill_value, options);
}
Tensor Type::grid_sampler(const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::grid_sampler(/* native_actuals */ input, grid, interpolation_mode, padding_mode);
}
Tensor Type::hann_window(int64_t window_length, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::hann_window(/* native_actuals */ window_length, options);
}
Tensor Type::hann_window(int64_t window_length, bool periodic, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::hann_window(/* native_actuals */ window_length, periodic, options);
}
Tensor Type::hamming_window(int64_t window_length, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::hamming_window(/* native_actuals */ window_length, options);
}
Tensor Type::hamming_window(int64_t window_length, bool periodic, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::hamming_window(/* native_actuals */ window_length, periodic, options);
}
Tensor Type::hamming_window(int64_t window_length, bool periodic, double alpha, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::hamming_window(/* native_actuals */ window_length, periodic, alpha, options);
}
Tensor Type::hamming_window(int64_t window_length, bool periodic, double alpha, double beta, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::hamming_window(/* native_actuals */ window_length, periodic, alpha, beta, options);
}
Tensor Type::hinge_embedding_loss(const Tensor & self, const Tensor & target, double margin, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::hinge_embedding_loss(/* native_actuals */ self, target, margin, reduction);
}
Tensor Type::ger(const Tensor & self, const Tensor & vec2) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::ger(/* native_actuals */ self, vec2);
}
Tensor & Type::ger_out(Tensor & out, const Tensor & self, const Tensor & vec2) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::ger_out(/* native_actuals */ out, self, vec2);
}
Tensor Type::group_norm(const Tensor & input, int64_t num_groups, const Tensor & weight, const Tensor & bias, double eps, bool cudnn_enabled) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::group_norm(/* native_actuals */ input, num_groups, weight, bias, eps, cudnn_enabled);
}
Tensor Type::fft(const Tensor & self, int64_t signal_ndim, bool normalized) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::fft(/* native_actuals */ self, signal_ndim, normalized);
}
Tensor Type::ifft(const Tensor & self, int64_t signal_ndim, bool normalized) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::ifft(/* native_actuals */ self, signal_ndim, normalized);
}
Tensor Type::rfft(const Tensor & self, int64_t signal_ndim, bool normalized, bool onesided) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::rfft(/* native_actuals */ self, signal_ndim, normalized, onesided);
}
Tensor Type::irfft(const Tensor & self, int64_t signal_ndim, bool normalized, bool onesided, IntArrayRef signal_sizes) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::irfft(/* native_actuals */ self, signal_ndim, normalized, onesided, signal_sizes);
}
int64_t Type::_cufft_get_plan_cache_size(int64_t device_index) const {
    // DeviceGuard omitted
    return at::native::_cufft_get_plan_cache_size(/* native_actuals */ device_index);
}
int64_t Type::_cufft_get_plan_cache_max_size(int64_t device_index) const {
    // DeviceGuard omitted
    return at::native::_cufft_get_plan_cache_max_size(/* native_actuals */ device_index);
}
void Type::_cufft_set_plan_cache_max_size(int64_t device_index, int64_t max_size) const {
    // DeviceGuard omitted
     at::native::_cufft_set_plan_cache_max_size(/* native_actuals */ device_index, max_size);
}
void Type::_cufft_clear_plan_cache(int64_t device_index) const {
    // DeviceGuard omitted
     at::native::_cufft_clear_plan_cache(/* native_actuals */ device_index);
}
Tensor Type::index(const Tensor & self, TensorList indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::index(/* native_actuals */ self, indices);
}
Tensor & Type::index_copy_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::index_copy_(/* native_actuals */ self, dim, index, source);
}
Tensor Type::index_copy(const Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::index_copy(/* native_actuals */ self, dim, index, source);
}
Tensor & Type::index_put_(Tensor & self, TensorList indices, const Tensor & values, bool accumulate) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::index_put_(/* native_actuals */ self, indices, values, accumulate);
}
Tensor Type::index_put(const Tensor & self, TensorList indices, const Tensor & values, bool accumulate) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::index_put(/* native_actuals */ self, indices, values, accumulate);
}
Tensor Type::instance_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool use_input_stats, double momentum, double eps, bool cudnn_enabled) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::instance_norm(/* native_actuals */ input, weight, bias, running_mean, running_var, use_input_stats, momentum, eps, cudnn_enabled);
}
Tensor Type::inverse(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::inverse(/* native_actuals */ self);
}
Tensor & Type::inverse_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::inverse_out(/* native_actuals */ out, self);
}
Tensor Type::isclose(const Tensor & self, const Tensor & other, double rtol, double atol, bool equal_nan) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::isclose(/* native_actuals */ self, other, rtol, atol, equal_nan);
}
Tensor Type::isnan(const Tensor & self) const {
    // DeviceGuard omitted
    return at::native::isnan(/* native_actuals */ self);
}
bool Type::is_distributed(const Tensor & self) const {
    // DeviceGuard omitted
    return at::native::is_distributed(/* native_actuals */ self);
}
bool Type::is_floating_point(const Tensor & self) const {
    // DeviceGuard omitted
    return at::native::is_floating_point(/* native_actuals */ self);
}
bool Type::is_complex(const Tensor & self) const {
    // DeviceGuard omitted
    return at::native::is_complex(/* native_actuals */ self);
}
bool Type::is_nonzero(const Tensor & self) const {
    // DeviceGuard omitted
    return at::native::is_nonzero(/* native_actuals */ self);
}
bool Type::is_same_size(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    return at::native::is_same_size(/* native_actuals */ self, other);
}
bool Type::is_signed(const Tensor & self) const {
    // DeviceGuard omitted
    return at::native::is_signed(/* native_actuals */ self);
}
Tensor Type::kl_div(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::kl_div(/* native_actuals */ self, target, reduction);
}
std::tuple<Tensor,Tensor> Type::kthvalue(const Tensor & self, int64_t k, int64_t dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::kthvalue(/* native_actuals */ self, k, dim, keepdim);
}
Tensor Type::layer_norm(const Tensor & input, IntArrayRef normalized_shape, const Tensor & weight, const Tensor & bias, double eps, bool cudnn_enable) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::layer_norm(/* native_actuals */ input, normalized_shape, weight, bias, eps, cudnn_enable);
}
Tensor Type::linear(const Tensor & input, const Tensor & weight, const Tensor & bias) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::linear(/* native_actuals */ input, weight, bias);
}
Tensor Type::fbgemm_linear_int8_weight(const Tensor & input, const Tensor & weight, const Tensor & packed, const Tensor & col_offsets, Scalar weight_scale, Scalar weight_zero_point, const Tensor & bias) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::fbgemm_linear_int8_weight(/* native_actuals */ input, weight, packed, col_offsets, weight_scale, weight_zero_point, bias);
}
std::tuple<Tensor,Tensor,double,int64_t> Type::fbgemm_linear_quantize_weight(const Tensor & input) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::fbgemm_linear_quantize_weight(/* native_actuals */ input);
}
Tensor Type::fbgemm_pack_quantized_matrix(const Tensor & input, int64_t K, int64_t N) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::fbgemm_pack_quantized_matrix(/* native_actuals */ input, K, N);
}
bool Type::fbgemm_is_cpu_supported() const {
    // DeviceGuard omitted
    return at::native::fbgemm_is_cpu_supported(/* native_actuals */ );
}
Tensor Type::linspace(Scalar start, Scalar end, int64_t steps, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::linspace(/* native_actuals */ start, end, steps, options);
}
Tensor Type::log(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::log(/* native_actuals */ self);
}
Tensor Type::log10(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::log10(/* native_actuals */ self);
}
Tensor Type::log1p(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::log1p(/* native_actuals */ self);
}
Tensor Type::log2(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::log2(/* native_actuals */ self);
}
Tensor Type::logdet(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::logdet(/* native_actuals */ self);
}
Tensor Type::logspace(Scalar start, Scalar end, int64_t steps, double base, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::logspace(/* native_actuals */ start, end, steps, base, options);
}
Tensor Type::log_softmax(const Tensor & self, int64_t dim, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::log_softmax(/* native_actuals */ self, dim, dtype);
}
Tensor Type::log_softmax(const Tensor & self, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::log_softmax(/* native_actuals */ self, dim);
}
Tensor Type::logsumexp(const Tensor & self, IntArrayRef dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::logsumexp(/* native_actuals */ self, dim, keepdim);
}
Tensor & Type::logsumexp_out(Tensor & out, const Tensor & self, IntArrayRef dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::logsumexp_out(/* native_actuals */ out, self, dim, keepdim);
}
Tensor Type::margin_ranking_loss(const Tensor & input1, const Tensor & input2, const Tensor & target, double margin, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(input1));
    return at::native::margin_ranking_loss(/* native_actuals */ input1, input2, target, margin, reduction);
}
Tensor Type::matmul(const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::matmul(/* native_actuals */ self, other);
}
Tensor & Type::matmul_out(Tensor & out, const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::matmul_out(/* native_actuals */ out, self, other);
}
Tensor Type::matrix_rank(const Tensor & self, double tol, bool symmetric) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::matrix_rank(/* native_actuals */ self, tol, symmetric);
}
Tensor Type::matrix_rank(const Tensor & self, bool symmetric) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::matrix_rank(/* native_actuals */ self, symmetric);
}
Tensor Type::matrix_power(const Tensor & self, int64_t n) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::matrix_power(/* native_actuals */ self, n);
}
std::tuple<Tensor,Tensor> Type::max(const Tensor & self, int64_t dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max(/* native_actuals */ self, dim, keepdim);
}
std::tuple<Tensor &,Tensor &> Type::max_out(Tensor & max, Tensor & max_values, const Tensor & self, int64_t dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_out(/* native_actuals */ max, max_values, self, dim, keepdim);
}
Tensor Type::max_values(const Tensor & self, IntArrayRef dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_values(/* native_actuals */ self, dim, keepdim);
}
std::tuple<Tensor,Tensor> Type::max_pool1d_with_indices(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_pool1d_with_indices(/* native_actuals */ self, kernel_size, stride, padding, dilation, ceil_mode);
}
Tensor Type::max_pool1d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_pool1d(/* native_actuals */ self, kernel_size, stride, padding, dilation, ceil_mode);
}
Tensor Type::max_pool2d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_pool2d(/* native_actuals */ self, kernel_size, stride, padding, dilation, ceil_mode);
}
Tensor Type::max_pool3d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_pool3d(/* native_actuals */ self, kernel_size, stride, padding, dilation, ceil_mode);
}
Tensor Type::mean(const Tensor & self, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mean(/* native_actuals */ self, dtype);
}
Tensor Type::mean(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mean(/* native_actuals */ self);
}
Tensor Type::mean(const Tensor & self, IntArrayRef dim, bool keepdim, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mean(/* native_actuals */ self, dim, keepdim, dtype);
}
Tensor Type::mean(const Tensor & self, IntArrayRef dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mean(/* native_actuals */ self, dim, keepdim);
}
Tensor Type::mean(const Tensor & self, IntArrayRef dim, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mean(/* native_actuals */ self, dim, dtype);
}
Tensor & Type::mean_out(Tensor & out, const Tensor & self, IntArrayRef dim, bool keepdim, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mean_out(/* native_actuals */ out, self, dim, keepdim, dtype);
}
Tensor & Type::mean_out(Tensor & out, const Tensor & self, IntArrayRef dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mean_out(/* native_actuals */ out, self, dim, keepdim);
}
Tensor & Type::mean_out(Tensor & out, const Tensor & self, IntArrayRef dim, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mean_out(/* native_actuals */ out, self, dim, dtype);
}
std::tuple<Tensor,Tensor> Type::median(const Tensor & self, int64_t dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::median(/* native_actuals */ self, dim, keepdim);
}
std::tuple<Tensor &,Tensor &> Type::median_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::median_out(/* native_actuals */ values, indices, self, dim, keepdim);
}
std::tuple<Tensor,Tensor> Type::min(const Tensor & self, int64_t dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::min(/* native_actuals */ self, dim, keepdim);
}
std::tuple<Tensor &,Tensor &> Type::min_out(Tensor & min, Tensor & min_indices, const Tensor & self, int64_t dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::min_out(/* native_actuals */ min, min_indices, self, dim, keepdim);
}
Tensor Type::min_values(const Tensor & self, IntArrayRef dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::min_values(/* native_actuals */ self, dim, keepdim);
}
Tensor Type::mkldnn_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_convolution(/* native_actuals */ self, weight, bias, padding, stride, dilation, groups);
}
Tensor Type::mkldnn_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool bias_defined) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    return at::native::mkldnn_convolution_backward_input(/* native_actuals */ self_size, grad_output, weight, padding, stride, dilation, groups, bias_defined);
}
std::tuple<Tensor,Tensor> Type::mkldnn_convolution_backward_weights(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool bias_defined) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_convolution_backward_weights(/* native_actuals */ weight_size, grad_output, self, padding, stride, dilation, groups, bias_defined);
}
std::tuple<Tensor,Tensor,Tensor> Type::mkldnn_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_convolution_backward(/* native_actuals */ self, grad_output, weight, padding, stride, dilation, groups, output_mask);
}
Tensor Type::mm(const Tensor & self, const Tensor & mat2) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mm(/* native_actuals */ self, mat2);
}
Tensor & Type::mm_out(Tensor & out, const Tensor & self, const Tensor & mat2) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mm_out(/* native_actuals */ out, self, mat2);
}
Tensor Type::_sparse_mm(const Tensor & sparse, const Tensor & dense) const {
    const OptionalDeviceGuard device_guard(device_of(sparse));
    return at::native::_sparse_mm(/* native_actuals */ sparse, dense);
}
std::tuple<Tensor,Tensor> Type::mode(const Tensor & self, int64_t dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mode(/* native_actuals */ self, dim, keepdim);
}
std::tuple<Tensor &,Tensor &> Type::mode_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mode_out(/* native_actuals */ values, indices, self, dim, keepdim);
}
Tensor Type::mul(const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mul(/* native_actuals */ self, other);
}
Tensor & Type::mul_(Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mul_(/* native_actuals */ self, other);
}
Tensor & Type::mul_out(Tensor & out, const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mul_out(/* native_actuals */ out, self, other);
}
Tensor Type::mul(const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mul(/* native_actuals */ self, other);
}
Tensor & Type::mul_(Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mul_(/* native_actuals */ self, other);
}
Tensor Type::mv(const Tensor & self, const Tensor & vec) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mv(/* native_actuals */ self, vec);
}
Tensor & Type::mv_out(Tensor & out, const Tensor & self, const Tensor & vec) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mv_out(/* native_actuals */ out, self, vec);
}
Tensor Type::mvlgamma(const Tensor & self, int64_t p) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mvlgamma(/* native_actuals */ self, p);
}
Tensor & Type::mvlgamma_(Tensor & self, int64_t p) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mvlgamma_(/* native_actuals */ self, p);
}
Tensor Type::narrow(const Tensor & self, int64_t dim, int64_t start, int64_t length) const {
    // DeviceGuard omitted
    return at::native::narrow(/* native_actuals */ self, dim, start, length);
}
bool Type::_nnpack_available() const {
    // DeviceGuard omitted
    return at::native::_nnpack_available(/* native_actuals */ );
}
Tensor Type::_nnpack_spatial_convolution(const Tensor & input, const Tensor & weight, const Tensor & bias, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::_nnpack_spatial_convolution(/* native_actuals */ input, weight, bias, padding);
}
std::tuple<Tensor,Tensor,Tensor> Type::_nnpack_spatial_convolution_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::_nnpack_spatial_convolution_backward(/* native_actuals */ input, grad_output, weight, padding, output_mask);
}
Tensor Type::_nnpack_spatial_convolution_backward_input(const Tensor & input, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::_nnpack_spatial_convolution_backward_input(/* native_actuals */ input, grad_output, weight, padding);
}
Tensor Type::_nnpack_spatial_convolution_backward_weight(const Tensor & input, IntArrayRef weightsize, const Tensor & grad_output, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::_nnpack_spatial_convolution_backward_weight(/* native_actuals */ input, weightsize, grad_output, padding);
}
Tensor Type::ones(IntArrayRef size, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::ones(/* native_actuals */ size, options);
}
Tensor & Type::ones_out(Tensor & out, IntArrayRef size) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::ones_out(/* native_actuals */ out, size);
}
Tensor Type::ones_like(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::ones_like(/* native_actuals */ self);
}
Tensor Type::ones_like(const Tensor & self, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::ones_like(/* native_actuals */ self, options);
}
Tensor Type::pairwise_distance(const Tensor & x1, const Tensor & x2, double p, double eps, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(x1));
    return at::native::pairwise_distance(/* native_actuals */ x1, x2, p, eps, keepdim);
}
Tensor Type::cdist(const Tensor & x1, const Tensor & x2, double p) const {
    const OptionalDeviceGuard device_guard(device_of(x1));
    return at::native::cdist(/* native_actuals */ x1, x2, p);
}
Tensor Type::_cdist_backward(const Tensor & grad, const Tensor & x1, const Tensor & x2, double p, const Tensor & cdist) const {
    const OptionalDeviceGuard device_guard(device_of(grad));
    return at::native::_cdist_backward(/* native_actuals */ grad, x1, x2, p, cdist);
}
Tensor Type::pdist(const Tensor & self, double p) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::pdist(/* native_actuals */ self, p);
}
Tensor Type::_pdist_forward(const Tensor & self, double p) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_pdist_forward(/* native_actuals */ self, p);
}
Tensor Type::_pdist_backward(const Tensor & grad, const Tensor & self, double p, const Tensor & pdist) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_pdist_backward(/* native_actuals */ grad, self, p, pdist);
}
Tensor Type::cosine_similarity(const Tensor & x1, const Tensor & x2, int64_t dim, double eps) const {
    const OptionalDeviceGuard device_guard(device_of(x1));
    return at::native::cosine_similarity(/* native_actuals */ x1, x2, dim, eps);
}
Tensor Type::permute(const Tensor & self, IntArrayRef dims) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::permute(/* native_actuals */ self, dims);
}
Tensor Type::pixel_shuffle(const Tensor & self, int64_t upscale_factor) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::pixel_shuffle(/* native_actuals */ self, upscale_factor);
}
Tensor Type::pin_memory(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::pin_memory(/* native_actuals */ self);
}
Tensor Type::pinverse(const Tensor & self, double rcond) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::pinverse(/* native_actuals */ self, rcond);
}
Tensor Type::scalar_tensor(Scalar s, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::scalar_tensor(/* native_actuals */ s, options);
}
Tensor Type::rand(IntArrayRef size, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::rand(/* native_actuals */ size, options);
}
Tensor Type::rand(IntArrayRef size, Generator * generator, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::rand(/* native_actuals */ size, generator, options);
}
Tensor & Type::rand_out(Tensor & out, IntArrayRef size) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::rand_out(/* native_actuals */ out, size);
}
Tensor & Type::rand_out(Tensor & out, IntArrayRef size, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::rand_out(/* native_actuals */ out, size, generator);
}
Tensor Type::rand_like(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::rand_like(/* native_actuals */ self);
}
Tensor Type::rand_like(const Tensor & self, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::rand_like(/* native_actuals */ self, options);
}
Tensor Type::randint(int64_t high, IntArrayRef size, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::randint(/* native_actuals */ high, size, options);
}
Tensor Type::randint(int64_t high, IntArrayRef size, Generator * generator, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::randint(/* native_actuals */ high, size, generator, options);
}
Tensor Type::randint(int64_t low, int64_t high, IntArrayRef size, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::randint(/* native_actuals */ low, high, size, options);
}
Tensor Type::randint(int64_t low, int64_t high, IntArrayRef size, Generator * generator, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::randint(/* native_actuals */ low, high, size, generator, options);
}
Tensor & Type::randint_out(Tensor & out, int64_t high, IntArrayRef size) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::randint_out(/* native_actuals */ out, high, size);
}
Tensor & Type::randint_out(Tensor & out, int64_t high, IntArrayRef size, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::randint_out(/* native_actuals */ out, high, size, generator);
}
Tensor & Type::randint_out(Tensor & out, int64_t low, int64_t high, IntArrayRef size) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::randint_out(/* native_actuals */ out, low, high, size);
}
Tensor & Type::randint_out(Tensor & out, int64_t low, int64_t high, IntArrayRef size, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::randint_out(/* native_actuals */ out, low, high, size, generator);
}
Tensor Type::randint_like(const Tensor & self, int64_t high) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::randint_like(/* native_actuals */ self, high);
}
Tensor Type::randint_like(const Tensor & self, int64_t low, int64_t high) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::randint_like(/* native_actuals */ self, low, high);
}
Tensor Type::randint_like(const Tensor & self, int64_t high, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::randint_like(/* native_actuals */ self, high, options);
}
Tensor Type::randint_like(const Tensor & self, int64_t low, int64_t high, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::randint_like(/* native_actuals */ self, low, high, options);
}
Tensor Type::randn(IntArrayRef size, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::randn(/* native_actuals */ size, options);
}
Tensor Type::randn(IntArrayRef size, Generator * generator, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::randn(/* native_actuals */ size, generator, options);
}
Tensor & Type::randn_out(Tensor & out, IntArrayRef size) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::randn_out(/* native_actuals */ out, size);
}
Tensor & Type::randn_out(Tensor & out, IntArrayRef size, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::randn_out(/* native_actuals */ out, size, generator);
}
Tensor Type::randn_like(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::randn_like(/* native_actuals */ self);
}
Tensor Type::randn_like(const Tensor & self, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::randn_like(/* native_actuals */ self, options);
}
Tensor Type::randperm(int64_t n, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::randperm(/* native_actuals */ n, options);
}
Tensor Type::randperm(int64_t n, Generator * generator, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::randperm(/* native_actuals */ n, generator, options);
}
Tensor & Type::randperm_out(Tensor & out, int64_t n) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::randperm_out(/* native_actuals */ out, n);
}
Tensor Type::range(Scalar start, Scalar end, Scalar step, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::range(/* native_actuals */ start, end, step, options);
}
Tensor Type::range(Scalar start, Scalar end, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::range(/* native_actuals */ start, end, options);
}
Tensor Type::reciprocal(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::reciprocal(/* native_actuals */ self);
}
Tensor Type::neg(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::neg(/* native_actuals */ self);
}
Tensor Type::repeat(const Tensor & self, IntArrayRef repeats) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::repeat(/* native_actuals */ self, repeats);
}
Tensor Type::repeat_interleave(const Tensor & self, const Tensor & repeats, c10::optional<int64_t> dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::repeat_interleave(/* native_actuals */ self, repeats, dim);
}
Tensor Type::repeat_interleave(const Tensor & self, int64_t repeats, c10::optional<int64_t> dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::repeat_interleave(/* native_actuals */ self, repeats, dim);
}
Tensor Type::reshape(const Tensor & self, IntArrayRef shape) const {
    // DeviceGuard omitted
    return at::native::reshape(/* native_actuals */ self, shape);
}
Tensor Type::reshape_as(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    return at::native::reshape_as(/* native_actuals */ self, other);
}
Tensor Type::round(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::round(/* native_actuals */ self);
}
Tensor Type::rrelu(const Tensor & self, Scalar lower, Scalar upper, bool training, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::rrelu(/* native_actuals */ self, lower, upper, training, generator);
}
Tensor & Type::rrelu_(Tensor & self, Scalar lower, Scalar upper, bool training, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::rrelu_(/* native_actuals */ self, lower, upper, training, generator);
}
Tensor Type::rsqrt(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::rsqrt(/* native_actuals */ self);
}
Tensor Type::select(const Tensor & self, int64_t dim, int64_t index) const {
    // DeviceGuard omitted
    return at::native::select(/* native_actuals */ self, dim, index);
}
Tensor Type::selu(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::selu(/* native_actuals */ self);
}
Tensor & Type::selu_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::selu_(/* native_actuals */ self);
}
Tensor Type::celu(const Tensor & self, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::celu(/* native_actuals */ self, alpha);
}
Tensor & Type::celu_(Tensor & self, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::celu_(/* native_actuals */ self, alpha);
}
Tensor Type::sigmoid(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sigmoid(/* native_actuals */ self);
}
Tensor Type::sin(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sin(/* native_actuals */ self);
}
Tensor Type::sinh(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sinh(/* native_actuals */ self);
}
Tensor Type::detach(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::detach(/* native_actuals */ self);
}
Tensor & Type::detach_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::detach_(/* native_actuals */ self);
}
int64_t Type::size(const Tensor & self, int64_t dim) const {
    // DeviceGuard omitted
    return at::native::size(/* native_actuals */ self, dim);
}
Tensor Type::slice(const Tensor & self, int64_t dim, int64_t start, int64_t end, int64_t step) const {
    // DeviceGuard omitted
    return at::native::slice(/* native_actuals */ self, dim, start, end, step);
}
std::tuple<Tensor,Tensor> Type::slogdet(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::slogdet(/* native_actuals */ self);
}
Tensor Type::smm(const Tensor & self, const Tensor & mat2) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::smm(/* native_actuals */ self, mat2);
}
Tensor Type::softmax(const Tensor & self, int64_t dim, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::softmax(/* native_actuals */ self, dim, dtype);
}
Tensor Type::softmax(const Tensor & self, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::softmax(/* native_actuals */ self, dim);
}
std::vector<Tensor> Type::split(const Tensor & self, int64_t split_size, int64_t dim) const {
    // DeviceGuard omitted
    return at::native::split(/* native_actuals */ self, split_size, dim);
}
std::vector<Tensor> Type::split_with_sizes(const Tensor & self, IntArrayRef split_sizes, int64_t dim) const {
    // DeviceGuard omitted
    return at::native::split_with_sizes(/* native_actuals */ self, split_sizes, dim);
}
Tensor Type::squeeze(const Tensor & self) const {
    // DeviceGuard omitted
    return at::native::squeeze(/* native_actuals */ self);
}
Tensor Type::squeeze(const Tensor & self, int64_t dim) const {
    // DeviceGuard omitted
    return at::native::squeeze(/* native_actuals */ self, dim);
}
Tensor & Type::squeeze_(Tensor & self) const {
    // DeviceGuard omitted
    return at::native::squeeze_(/* native_actuals */ self);
}
Tensor & Type::squeeze_(Tensor & self, int64_t dim) const {
    // DeviceGuard omitted
    return at::native::squeeze_(/* native_actuals */ self, dim);
}
Tensor Type::sspaddmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sspaddmm(/* native_actuals */ self, mat1, mat2, beta, alpha);
}
Tensor Type::stack(TensorList tensors, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(tensors));
    return at::native::stack(/* native_actuals */ tensors, dim);
}
Tensor & Type::stack_out(Tensor & out, TensorList tensors, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::stack_out(/* native_actuals */ out, tensors, dim);
}
Tensor Type::stft(const Tensor & self, int64_t n_fft, c10::optional<int64_t> hop_length, c10::optional<int64_t> win_length, const Tensor & window, bool normalized, bool onesided) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::stft(/* native_actuals */ self, n_fft, hop_length, win_length, window, normalized, onesided);
}
int64_t Type::stride(const Tensor & self, int64_t dim) const {
    // DeviceGuard omitted
    return at::native::stride(/* native_actuals */ self, dim);
}
Tensor Type::sum(const Tensor & self, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sum(/* native_actuals */ self, dtype);
}
Tensor Type::sum(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sum(/* native_actuals */ self);
}
Tensor Type::sum(const Tensor & self, IntArrayRef dim, bool keepdim, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sum(/* native_actuals */ self, dim, keepdim, dtype);
}
Tensor Type::sum(const Tensor & self, IntArrayRef dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sum(/* native_actuals */ self, dim, keepdim);
}
Tensor Type::sum(const Tensor & self, IntArrayRef dim, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sum(/* native_actuals */ self, dim, dtype);
}
Tensor & Type::sum_out(Tensor & out, const Tensor & self, IntArrayRef dim, bool keepdim, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sum_out(/* native_actuals */ out, self, dim, keepdim, dtype);
}
Tensor & Type::sum_out(Tensor & out, const Tensor & self, IntArrayRef dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sum_out(/* native_actuals */ out, self, dim, keepdim);
}
Tensor & Type::sum_out(Tensor & out, const Tensor & self, IntArrayRef dim, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sum_out(/* native_actuals */ out, self, dim, dtype);
}
Tensor Type::sum_to_size(const Tensor & self, IntArrayRef size) const {
    // DeviceGuard omitted
    return at::native::sum_to_size(/* native_actuals */ self, size);
}
Tensor Type::sqrt(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sqrt(/* native_actuals */ self);
}
Tensor Type::std(const Tensor & self, bool unbiased) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::std(/* native_actuals */ self, unbiased);
}
Tensor Type::std(const Tensor & self, IntArrayRef dim, bool unbiased, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::std(/* native_actuals */ self, dim, unbiased, keepdim);
}
Tensor & Type::std_out(Tensor & out, const Tensor & self, IntArrayRef dim, bool unbiased, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::std_out(/* native_actuals */ out, self, dim, unbiased, keepdim);
}
Tensor Type::prod(const Tensor & self, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::prod(/* native_actuals */ self, dtype);
}
Tensor Type::prod(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::prod(/* native_actuals */ self);
}
Tensor Type::prod(const Tensor & self, int64_t dim, bool keepdim, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::prod(/* native_actuals */ self, dim, keepdim, dtype);
}
Tensor Type::prod(const Tensor & self, int64_t dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::prod(/* native_actuals */ self, dim, keepdim);
}
Tensor Type::prod(const Tensor & self, int64_t dim, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::prod(/* native_actuals */ self, dim, dtype);
}
Tensor & Type::prod_out(Tensor & out, const Tensor & self, int64_t dim, bool keepdim, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::prod_out(/* native_actuals */ out, self, dim, keepdim, dtype);
}
Tensor & Type::prod_out(Tensor & out, const Tensor & self, int64_t dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::prod_out(/* native_actuals */ out, self, dim, keepdim);
}
Tensor & Type::prod_out(Tensor & out, const Tensor & self, int64_t dim, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::prod_out(/* native_actuals */ out, self, dim, dtype);
}
Tensor Type::t(const Tensor & self) const {
    // DeviceGuard omitted
    return at::native::t(/* native_actuals */ self);
}
Tensor & Type::t_(Tensor & self) const {
    // DeviceGuard omitted
    return at::native::t_(/* native_actuals */ self);
}
Tensor Type::tan(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::tan(/* native_actuals */ self);
}
Tensor Type::tanh(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::tanh(/* native_actuals */ self);
}
Tensor Type::tensordot(const Tensor & self, const Tensor & other, IntArrayRef dims_self, IntArrayRef dims_other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::tensordot(/* native_actuals */ self, other, dims_self, dims_other);
}
Tensor Type::threshold(const Tensor & self, Scalar threshold, Scalar value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::threshold(/* native_actuals */ self, threshold, value);
}
Tensor & Type::threshold_(Tensor & self, Scalar threshold, Scalar value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::threshold_(/* native_actuals */ self, threshold, value);
}
Tensor & Type::threshold_out(Tensor & out, const Tensor & self, Scalar threshold, Scalar value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::threshold_out(/* native_actuals */ out, self, threshold, value);
}
Tensor Type::threshold_backward(const Tensor & grad_output, const Tensor & self, Scalar threshold) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::threshold_backward(/* native_actuals */ grad_output, self, threshold);
}
Tensor Type::transpose(const Tensor & self, int64_t dim0, int64_t dim1) const {
    // DeviceGuard omitted
    return at::native::transpose(/* native_actuals */ self, dim0, dim1);
}
Tensor & Type::transpose_(Tensor & self, int64_t dim0, int64_t dim1) const {
    // DeviceGuard omitted
    return at::native::transpose_(/* native_actuals */ self, dim0, dim1);
}
Tensor Type::one_hot(const Tensor & self, int64_t num_classes) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::one_hot(/* native_actuals */ self, num_classes);
}
Tensor Type::rot90(const Tensor & self, int64_t k, IntArrayRef dims) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::rot90(/* native_actuals */ self, k, dims);
}
Tensor Type::_trilinear(const Tensor & i1, const Tensor & i2, const Tensor & i3, IntArrayRef expand1, IntArrayRef expand2, IntArrayRef expand3, IntArrayRef sumdim, int64_t unroll_dim) const {
    const OptionalDeviceGuard device_guard(device_of(i1));
    return at::native::_trilinear(/* native_actuals */ i1, i2, i3, expand1, expand2, expand3, sumdim, unroll_dim);
}
Tensor Type::triplet_margin_loss(const Tensor & anchor, const Tensor & positive, const Tensor & negative, double margin, double p, double eps, bool swap, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(anchor));
    return at::native::triplet_margin_loss(/* native_actuals */ anchor, positive, negative, margin, p, eps, swap, reduction);
}
Tensor Type::trunc(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::trunc(/* native_actuals */ self);
}
Tensor Type::type_as(const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::type_as(/* native_actuals */ self, other);
}
Tensor Type::_unsafe_view(const Tensor & self, IntArrayRef size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_unsafe_view(/* native_actuals */ self, size);
}
Tensor Type::unsqueeze(const Tensor & self, int64_t dim) const {
    // DeviceGuard omitted
    return at::native::unsqueeze(/* native_actuals */ self, dim);
}
Tensor & Type::unsqueeze_(Tensor & self, int64_t dim) const {
    // DeviceGuard omitted
    return at::native::unsqueeze_(/* native_actuals */ self, dim);
}
Tensor Type::var(const Tensor & self, bool unbiased) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::var(/* native_actuals */ self, unbiased);
}
Tensor Type::var(const Tensor & self, IntArrayRef dim, bool unbiased, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::var(/* native_actuals */ self, dim, unbiased, keepdim);
}
Tensor & Type::var_out(Tensor & out, const Tensor & self, IntArrayRef dim, bool unbiased, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::var_out(/* native_actuals */ out, self, dim, unbiased, keepdim);
}
Tensor Type::view_as(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    return at::native::view_as(/* native_actuals */ self, other);
}
Tensor Type::where(const Tensor & condition, const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::where(/* native_actuals */ condition, self, other);
}
Tensor Type::norm_except_dim(const Tensor & v, int64_t pow, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(v));
    return at::native::norm_except_dim(/* native_actuals */ v, pow, dim);
}
Tensor Type::_weight_norm(const Tensor & v, const Tensor & g, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(v));
    return at::native::_weight_norm(/* native_actuals */ v, g, dim);
}
std::tuple<Tensor,Tensor> Type::_weight_norm_differentiable_backward(const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(grad_w));
    return at::native::_weight_norm_differentiable_backward(/* native_actuals */ grad_w, saved_v, saved_g, saved_norms, dim);
}
Tensor Type::zeros(IntArrayRef size, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::zeros(/* native_actuals */ size, options);
}
Tensor & Type::zeros_out(Tensor & out, IntArrayRef size) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::zeros_out(/* native_actuals */ out, size);
}
Tensor Type::zeros_like(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::zeros_like(/* native_actuals */ self);
}
Tensor Type::zeros_like(const Tensor & self, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::zeros_like(/* native_actuals */ self, options);
}
Tensor Type::_sparse_sum(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sparse_sum(/* native_actuals */ self);
}
Tensor Type::_sparse_sum(const Tensor & self, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sparse_sum(/* native_actuals */ self, dtype);
}
Tensor Type::_sparse_sum(const Tensor & self, IntArrayRef dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sparse_sum(/* native_actuals */ self, dim);
}
Tensor Type::_sparse_sum(const Tensor & self, IntArrayRef dim, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sparse_sum(/* native_actuals */ self, dim, dtype);
}
Tensor Type::norm(const Tensor & self, c10::optional<Scalar> p, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::norm(/* native_actuals */ self, p, dtype);
}
Tensor Type::norm(const Tensor & self, Scalar p) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::norm(/* native_actuals */ self, p);
}
Tensor Type::norm(const Tensor & self, c10::optional<Scalar> p, IntArrayRef dim, bool keepdim, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::norm(/* native_actuals */ self, p, dim, keepdim, dtype);
}
Tensor Type::norm(const Tensor & self, c10::optional<Scalar> p, IntArrayRef dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::norm(/* native_actuals */ self, p, dim, keepdim);
}
Tensor & Type::norm_out(Tensor & out, const Tensor & self, c10::optional<Scalar> p, IntArrayRef dim, bool keepdim, ScalarType dtype) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::norm_out(/* native_actuals */ out, self, p, dim, keepdim, dtype);
}
Tensor & Type::norm_out(Tensor & out, const Tensor & self, c10::optional<Scalar> p, IntArrayRef dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::norm_out(/* native_actuals */ out, self, p, dim, keepdim);
}
Tensor Type::frobenius_norm(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::frobenius_norm(/* native_actuals */ self);
}
Tensor Type::frobenius_norm(const Tensor & self, IntArrayRef dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::frobenius_norm(/* native_actuals */ self, dim, keepdim);
}
Tensor & Type::frobenius_norm_out(Tensor & out, const Tensor & self, IntArrayRef dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::frobenius_norm_out(/* native_actuals */ out, self, dim, keepdim);
}
Tensor Type::nuclear_norm(const Tensor & self, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::nuclear_norm(/* native_actuals */ self, keepdim);
}
Tensor & Type::nuclear_norm_out(Tensor & out, const Tensor & self, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::nuclear_norm_out(/* native_actuals */ out, self, keepdim);
}
Tensor & Type::sub_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sub_out(/* native_actuals */ out, self, other, alpha);
}
Tensor Type::sub(const Tensor & self, const Tensor & other, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sub(/* native_actuals */ self, other, alpha);
}
Tensor & Type::sub_(Tensor & self, const Tensor & other, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sub_(/* native_actuals */ self, other, alpha);
}
Tensor Type::sub(const Tensor & self, Scalar other, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sub(/* native_actuals */ self, other, alpha);
}
Tensor & Type::sub_(Tensor & self, Scalar other, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sub_(/* native_actuals */ self, other, alpha);
}
Tensor Type::rsub(const Tensor & self, const Tensor & other, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::rsub(/* native_actuals */ self, other, alpha);
}
Tensor Type::rsub(const Tensor & self, Scalar other, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::rsub(/* native_actuals */ self, other, alpha);
}
Tensor Type::_sparse_addmm(const Tensor & self, const Tensor & sparse, const Tensor & dense, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_sparse_addmm(/* native_actuals */ self, sparse, dense, beta, alpha);
}
Tensor & Type::addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addmm_out(/* native_actuals */ out, self, mat1, mat2, beta, alpha);
}
Tensor Type::addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addmm(/* native_actuals */ self, mat1, mat2, beta, alpha);
}
Tensor & Type::addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addmm_(/* native_actuals */ self, mat1, mat2, beta, alpha);
}
Tensor Type::sparse_coo_tensor(IntArrayRef size, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::sparse_coo_tensor(/* native_actuals */ size, options);
}
Tensor Type::sparse_coo_tensor(const Tensor & indices, const Tensor & values, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::sparse_coo_tensor(/* native_actuals */ indices, values, options);
}
Tensor Type::sparse_coo_tensor(const Tensor & indices, const Tensor & values, IntArrayRef size, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::sparse_coo_tensor(/* native_actuals */ indices, values, size, options);
}
Tensor Type::_sparse_coo_tensor_unsafe(const Tensor & indices, const Tensor & values, IntArrayRef size, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    return at::native::_sparse_coo_tensor_unsafe(/* native_actuals */ indices, values, size, options);
}
Tensor Type::to_dense_backward(const Tensor & grad, const Tensor & input) const {
    const OptionalDeviceGuard device_guard(device_of(grad));
    return at::native::to_dense_backward(/* native_actuals */ grad, input);
}
int64_t Type::_dimI(const Tensor & self) const {
    // DeviceGuard omitted
    return at::native::sparse_dim_sparse(/* native_actuals */ self);
}
int64_t Type::_dimV(const Tensor & self) const {
    // DeviceGuard omitted
    return at::native::dense_dim_sparse(/* native_actuals */ self);
}
int64_t Type::numel(const Tensor & self) const {
    // DeviceGuard omitted
    return at::native::numel(/* native_actuals */ self);
}
std::vector<Tensor> Type::unbind(const Tensor & self, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::unbind(/* native_actuals */ self, dim);
}
Tensor Type::to_mkldnn_backward(const Tensor & grad, const Tensor & input) const {
    const OptionalDeviceGuard device_guard(device_of(grad));
    return at::native::to_mkldnn_backward(/* native_actuals */ grad, input);
}
Tensor Type::to(const Tensor & self, const TensorOptions & options, bool non_blocking, bool copy) const {
    // DeviceGuard omitted
    return at::native::to(/* native_actuals */ self, options, non_blocking, copy);
}
Tensor Type::to(const Tensor & self, Device device, ScalarType dtype, bool non_blocking, bool copy) const {
    // DeviceGuard omitted
    return at::native::to(/* native_actuals */ self, device, dtype, non_blocking, copy);
}
Tensor Type::to(const Tensor & self, ScalarType dtype, bool non_blocking, bool copy) const {
    // DeviceGuard omitted
    return at::native::to(/* native_actuals */ self, dtype, non_blocking, copy);
}
Tensor Type::to(const Tensor & self, const Tensor & other, bool non_blocking, bool copy) const {
    // DeviceGuard omitted
    return at::native::to(/* native_actuals */ self, other, non_blocking, copy);
}
std::vector<Tensor> Type::meshgrid(TensorList tensors) const {
    const OptionalDeviceGuard device_guard(device_of(tensors));
    return at::native::meshgrid(/* native_actuals */ tensors);
}
Tensor Type::cartesian_prod(TensorList tensors) const {
    const OptionalDeviceGuard device_guard(device_of(tensors));
    return at::native::cartesian_prod(/* native_actuals */ tensors);
}
Tensor Type::combinations(const Tensor & self, int64_t r, bool with_replacement) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::combinations(/* native_actuals */ self, r, with_replacement);
}
Scalar Type::item(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::item(/* native_actuals */ self);
}
std::tuple<Tensor,Tensor,Tensor> Type::lstm(const Tensor & input, TensorList hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::lstm(/* native_actuals */ input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}
std::tuple<Tensor,Tensor,Tensor> Type::lstm(const Tensor & data, const Tensor & batch_sizes, TensorList hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) const {
    const OptionalDeviceGuard device_guard(device_of(data));
    return at::native::lstm(/* native_actuals */ data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}
std::tuple<Tensor,Tensor> Type::gru(const Tensor & input, const Tensor & hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::gru(/* native_actuals */ input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}
std::tuple<Tensor,Tensor> Type::gru(const Tensor & data, const Tensor & batch_sizes, const Tensor & hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) const {
    const OptionalDeviceGuard device_guard(device_of(data));
    return at::native::gru(/* native_actuals */ data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}
std::tuple<Tensor,Tensor> Type::rnn_tanh(const Tensor & input, const Tensor & hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::rnn_tanh(/* native_actuals */ input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}
std::tuple<Tensor,Tensor> Type::rnn_tanh(const Tensor & data, const Tensor & batch_sizes, const Tensor & hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) const {
    const OptionalDeviceGuard device_guard(device_of(data));
    return at::native::rnn_tanh(/* native_actuals */ data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}
std::tuple<Tensor,Tensor> Type::rnn_relu(const Tensor & input, const Tensor & hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::rnn_relu(/* native_actuals */ input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}
std::tuple<Tensor,Tensor> Type::rnn_relu(const Tensor & data, const Tensor & batch_sizes, const Tensor & hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional) const {
    const OptionalDeviceGuard device_guard(device_of(data));
    return at::native::rnn_relu(/* native_actuals */ data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional);
}
std::tuple<Tensor,Tensor> Type::lstm_cell(const Tensor & input, TensorList hx, const Tensor & w_ih, const Tensor & w_hh, const Tensor & b_ih, const Tensor & b_hh) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::lstm_cell(/* native_actuals */ input, hx, w_ih, w_hh, b_ih, b_hh);
}
Tensor Type::gru_cell(const Tensor & input, const Tensor & hx, const Tensor & w_ih, const Tensor & w_hh, const Tensor & b_ih, const Tensor & b_hh) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::gru_cell(/* native_actuals */ input, hx, w_ih, w_hh, b_ih, b_hh);
}
Tensor Type::rnn_tanh_cell(const Tensor & input, const Tensor & hx, const Tensor & w_ih, const Tensor & w_hh, const Tensor & b_ih, const Tensor & b_hh) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::rnn_tanh_cell(/* native_actuals */ input, hx, w_ih, w_hh, b_ih, b_hh);
}
Tensor Type::rnn_relu_cell(const Tensor & input, const Tensor & hx, const Tensor & w_ih, const Tensor & w_hh, const Tensor & b_ih, const Tensor & b_hh) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::rnn_relu_cell(/* native_actuals */ input, hx, w_ih, w_hh, b_ih, b_hh);
}
std::tuple<Tensor,Tensor,Tensor> Type::quantized_lstm(const Tensor & input, TensorList hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::quantized_lstm(/* native_actuals */ input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first);
}
std::tuple<Tensor,Tensor> Type::quantized_lstm_cell(const Tensor & input, TensorList hx, const Tensor & w_ih, const Tensor & w_hh, const Tensor & b_ih, const Tensor & b_hh, const Tensor & packed_ih, const Tensor & packed_hh, const Tensor & col_offsets_ih, const Tensor & col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::quantized_lstm_cell(/* native_actuals */ input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}
Tensor Type::quantized_gru_cell(const Tensor & input, const Tensor & hx, const Tensor & w_ih, const Tensor & w_hh, const Tensor & b_ih, const Tensor & b_hh, const Tensor & packed_ih, const Tensor & packed_hh, const Tensor & col_offsets_ih, const Tensor & col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::quantized_gru_cell(/* native_actuals */ input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}
Tensor Type::quantized_rnn_relu_cell(const Tensor & input, const Tensor & hx, const Tensor & w_ih, const Tensor & w_hh, const Tensor & b_ih, const Tensor & b_hh, const Tensor & packed_ih, const Tensor & packed_hh, const Tensor & col_offsets_ih, const Tensor & col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::quantized_rnn_relu_cell(/* native_actuals */ input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}
Tensor Type::quantized_rnn_tanh_cell(const Tensor & input, const Tensor & hx, const Tensor & w_ih, const Tensor & w_hh, const Tensor & b_ih, const Tensor & b_hh, const Tensor & packed_ih, const Tensor & packed_hh, const Tensor & col_offsets_ih, const Tensor & col_offsets_hh, Scalar scale_ih, Scalar scale_hh, Scalar zero_point_ih, Scalar zero_point_hh) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::quantized_rnn_tanh_cell(/* native_actuals */ input, hx, w_ih, w_hh, b_ih, b_hh, packed_ih, packed_hh, col_offsets_ih, col_offsets_hh, scale_ih, scale_hh, zero_point_ih, zero_point_hh);
}
std::tuple<Tensor,Tensor> Type::_pack_padded_sequence(const Tensor & input, const Tensor & lengths, bool batch_first) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::_pack_padded_sequence(/* native_actuals */ input, lengths, batch_first);
}
Tensor Type::_pack_padded_sequence_backward(const Tensor & grad, IntArrayRef input_size, const Tensor & batch_sizes, bool batch_first) const {
    const OptionalDeviceGuard device_guard(device_of(grad));
    return at::native::_pack_padded_sequence_backward(/* native_actuals */ grad, input_size, batch_sizes, batch_first);
}
std::tuple<Tensor,Tensor> Type::_pad_packed_sequence(const Tensor & data, const Tensor & batch_sizes, bool batch_first, Scalar padding_value, int64_t total_length) const {
    const OptionalDeviceGuard device_guard(device_of(data));
    return at::native::_pad_packed_sequence(/* native_actuals */ data, batch_sizes, batch_first, padding_value, total_length);
}
void* Type::data_ptr(const Tensor & self) const {
    // DeviceGuard omitted
    return at::native::data_ptr(/* native_actuals */ self);
}
Tensor & Type::set_(Tensor & self, Storage source) const {
    // DeviceGuard omitted
    return at::native::set_(/* native_actuals */ self, source);
}
Tensor & Type::set_(Tensor & self, Storage source, int64_t storage_offset, IntArrayRef size, IntArrayRef stride) const {
    // DeviceGuard omitted
    return at::native::set_(/* native_actuals */ self, source, storage_offset, size, stride);
}
Tensor & Type::set_(Tensor & self, const Tensor & source) const {
    // DeviceGuard omitted
    return at::native::set_(/* native_actuals */ self, source);
}
Tensor & Type::set_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::set_(/* native_actuals */ self);
}
bool Type::is_set_to(const Tensor & self, const Tensor & tensor) const {
    // DeviceGuard omitted
    return at::native::is_set_to(/* native_actuals */ self, tensor);
}
Tensor & Type::masked_fill_(Tensor & self, const Tensor & mask, Scalar value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::masked_fill_(/* native_actuals */ self, mask, value);
}
Tensor Type::masked_fill(const Tensor & self, const Tensor & mask, Scalar value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::masked_fill(/* native_actuals */ self, mask, value);
}
Tensor & Type::masked_fill_(Tensor & self, const Tensor & mask, const Tensor & value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::masked_fill_(/* native_actuals */ self, mask, value);
}
Tensor Type::masked_fill(const Tensor & self, const Tensor & mask, const Tensor & value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::masked_fill(/* native_actuals */ self, mask, value);
}
Tensor & Type::masked_scatter_(Tensor & self, const Tensor & mask, const Tensor & source) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::masked_scatter_(/* native_actuals */ self, mask, source);
}
Tensor Type::masked_scatter(const Tensor & self, const Tensor & mask, const Tensor & source) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::masked_scatter(/* native_actuals */ self, mask, source);
}
Tensor Type::view(const Tensor & self, IntArrayRef size) const {
    // DeviceGuard omitted
    return at::native::view(/* native_actuals */ self, size);
}
Tensor & Type::put_(Tensor & self, const Tensor & index, const Tensor & source, bool accumulate) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::put_(/* native_actuals */ self, index, source, accumulate);
}
Tensor & Type::index_add_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::index_add_(/* native_actuals */ self, dim, index, source);
}
Tensor Type::index_add(const Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::index_add(/* native_actuals */ self, dim, index, source);
}
Tensor & Type::index_fill_(Tensor & self, int64_t dim, const Tensor & index, Scalar value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::index_fill_(/* native_actuals */ self, dim, index, value);
}
Tensor Type::index_fill(const Tensor & self, int64_t dim, const Tensor & index, Scalar value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::index_fill(/* native_actuals */ self, dim, index, value);
}
Tensor & Type::index_fill_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::index_fill_(/* native_actuals */ self, dim, index, value);
}
Tensor Type::index_fill(const Tensor & self, int64_t dim, const Tensor & index, const Tensor & value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::index_fill(/* native_actuals */ self, dim, index, value);
}
Tensor & Type::scatter_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::scatter_(/* native_actuals */ self, dim, index, src);
}
Tensor Type::scatter(const Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::scatter(/* native_actuals */ self, dim, index, src);
}
Tensor & Type::scatter_(Tensor & self, int64_t dim, const Tensor & index, Scalar value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::scatter_(/* native_actuals */ self, dim, index, value);
}
Tensor Type::scatter(const Tensor & self, int64_t dim, const Tensor & index, Scalar value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::scatter(/* native_actuals */ self, dim, index, value);
}
Tensor & Type::scatter_add_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::scatter_add_(/* native_actuals */ self, dim, index, src);
}
Tensor Type::scatter_add(const Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::scatter_add(/* native_actuals */ self, dim, index, src);
}
Tensor & Type::lt_(Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::lt_(/* native_actuals */ self, other);
}
Tensor & Type::lt_(Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::lt_(/* native_actuals */ self, other);
}
Tensor & Type::gt_(Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::gt_(/* native_actuals */ self, other);
}
Tensor & Type::gt_(Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::gt_(/* native_actuals */ self, other);
}
Tensor & Type::le_(Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::le_(/* native_actuals */ self, other);
}
Tensor & Type::le_(Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::le_(/* native_actuals */ self, other);
}
Tensor & Type::ge_(Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::ge_(/* native_actuals */ self, other);
}
Tensor & Type::ge_(Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::ge_(/* native_actuals */ self, other);
}
Tensor & Type::eq_(Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::eq_(/* native_actuals */ self, other);
}
Tensor & Type::eq_(Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::eq_(/* native_actuals */ self, other);
}
Tensor & Type::ne_(Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::ne_(/* native_actuals */ self, other);
}
Tensor & Type::ne_(Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::ne_(/* native_actuals */ self, other);
}
Tensor Type::__and__(const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::__and__(/* native_actuals */ self, other);
}
Tensor Type::__and__(const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::__and__(/* native_actuals */ self, other);
}
Tensor & Type::__iand__(Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::__iand__(/* native_actuals */ self, other);
}
Tensor & Type::__iand__(Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::__iand__(/* native_actuals */ self, other);
}
Tensor Type::__or__(const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::__or__(/* native_actuals */ self, other);
}
Tensor Type::__or__(const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::__or__(/* native_actuals */ self, other);
}
Tensor & Type::__ior__(Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::__ior__(/* native_actuals */ self, other);
}
Tensor & Type::__ior__(Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::__ior__(/* native_actuals */ self, other);
}
Tensor Type::__xor__(const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::__xor__(/* native_actuals */ self, other);
}
Tensor Type::__xor__(const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::__xor__(/* native_actuals */ self, other);
}
Tensor & Type::__ixor__(Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::__ixor__(/* native_actuals */ self, other);
}
Tensor & Type::__ixor__(Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::__ixor__(/* native_actuals */ self, other);
}
Tensor Type::__lshift__(const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::__lshift__(/* native_actuals */ self, other);
}
Tensor Type::__lshift__(const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::__lshift__(/* native_actuals */ self, other);
}
Tensor & Type::__ilshift__(Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::__ilshift__(/* native_actuals */ self, other);
}
Tensor & Type::__ilshift__(Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::__ilshift__(/* native_actuals */ self, other);
}
Tensor Type::__rshift__(const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::__rshift__(/* native_actuals */ self, other);
}
Tensor Type::__rshift__(const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::__rshift__(/* native_actuals */ self, other);
}
Tensor & Type::__irshift__(Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::__irshift__(/* native_actuals */ self, other);
}
Tensor & Type::__irshift__(Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::__irshift__(/* native_actuals */ self, other);
}
Tensor & Type::lgamma_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::lgamma_(/* native_actuals */ self);
}
Tensor & Type::atan2_(Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::atan2_(/* native_actuals */ self, other);
}
Tensor & Type::digamma_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::digamma_(/* native_actuals */ self);
}
Tensor & Type::polygamma_(Tensor & self, int64_t n) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::polygamma_(/* native_actuals */ self, n);
}
Tensor & Type::erfinv_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::erfinv_(/* native_actuals */ self);
}
Tensor & Type::renorm_(Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::renorm_(/* native_actuals */ self, p, dim, maxnorm);
}
Tensor & Type::pow_(Tensor & self, Scalar exponent) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::pow_(/* native_actuals */ self, exponent);
}
Tensor & Type::pow_(Tensor & self, const Tensor & exponent) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::pow_(/* native_actuals */ self, exponent);
}
Tensor & Type::sign_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sign_(/* native_actuals */ self);
}
Tensor & Type::fmod_(Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::fmod_(/* native_actuals */ self, other);
}
Tensor & Type::fmod_(Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::fmod_(/* native_actuals */ self, other);
}
Tensor & Type::remainder_(Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::remainder_(/* native_actuals */ self, other);
}
Tensor & Type::remainder_(Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::remainder_(/* native_actuals */ self, other);
}
Tensor & Type::addbmm_(Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addbmm_(/* native_actuals */ self, batch1, batch2, beta, alpha);
}
Tensor & Type::addbmm_out(Tensor & out, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addbmm_out(/* native_actuals */ out, self, batch1, batch2, beta, alpha);
}
Tensor Type::addbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addbmm(/* native_actuals */ self, batch1, batch2, beta, alpha);
}
Tensor & Type::addcmul_(Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addcmul_(/* native_actuals */ self, tensor1, tensor2, value);
}
Tensor & Type::addcdiv_(Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addcdiv_(/* native_actuals */ self, tensor1, tensor2, value);
}
Tensor & Type::random_(Tensor & self, int64_t from, int64_t to, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::random_(/* native_actuals */ self, from, to, generator);
}
Tensor & Type::random_(Tensor & self, int64_t to, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::random_(/* native_actuals */ self, to, generator);
}
Tensor & Type::random_(Tensor & self, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::random_(/* native_actuals */ self, generator);
}
Tensor & Type::uniform_(Tensor & self, double from, double to, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::uniform_(/* native_actuals */ self, from, to, generator);
}
Tensor & Type::normal_(Tensor & self, double mean, double std, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::normal_(/* native_actuals */ self, mean, std, generator);
}
Tensor & Type::cauchy_(Tensor & self, double median, double sigma, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::cauchy_(/* native_actuals */ self, median, sigma, generator);
}
Tensor & Type::log_normal_(Tensor & self, double mean, double std, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::log_normal_(/* native_actuals */ self, mean, std, generator);
}
Tensor & Type::exponential_(Tensor & self, double lambd, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::exponential_(/* native_actuals */ self, lambd, generator);
}
Tensor & Type::geometric_(Tensor & self, double p, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::geometric_(/* native_actuals */ self, p, generator);
}
Tensor & Type::diag_out(Tensor & out, const Tensor & self, int64_t diagonal) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::diag_out(/* native_actuals */ out, self, diagonal);
}
Tensor Type::diag(const Tensor & self, int64_t diagonal) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::diag(/* native_actuals */ self, diagonal);
}
Tensor & Type::cross_out(Tensor & out, const Tensor & self, const Tensor & other, c10::optional<int64_t> dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::cross_out(/* native_actuals */ out, self, other, dim);
}
Tensor Type::cross(const Tensor & self, const Tensor & other, c10::optional<int64_t> dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::cross(/* native_actuals */ self, other, dim);
}
Tensor Type::triu(const Tensor & self, int64_t diagonal) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::triu(/* native_actuals */ self, diagonal);
}
Tensor Type::tril(const Tensor & self, int64_t diagonal) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::tril(/* native_actuals */ self, diagonal);
}
Tensor Type::trace(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::trace(/* native_actuals */ self);
}
Tensor & Type::ne_out(Tensor & out, const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::ne_out(/* native_actuals */ out, self, other);
}
Tensor Type::ne(const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::ne(/* native_actuals */ self, other);
}
Tensor & Type::ne_out(Tensor & out, const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::ne_out(/* native_actuals */ out, self, other);
}
Tensor Type::ne(const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::ne(/* native_actuals */ self, other);
}
Tensor & Type::eq_out(Tensor & out, const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::eq_out(/* native_actuals */ out, self, other);
}
Tensor Type::eq(const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::eq(/* native_actuals */ self, other);
}
Tensor & Type::eq_out(Tensor & out, const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::eq_out(/* native_actuals */ out, self, other);
}
Tensor Type::eq(const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::eq(/* native_actuals */ self, other);
}
Tensor & Type::ge_out(Tensor & out, const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::ge_out(/* native_actuals */ out, self, other);
}
Tensor Type::ge(const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::ge(/* native_actuals */ self, other);
}
Tensor & Type::ge_out(Tensor & out, const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::ge_out(/* native_actuals */ out, self, other);
}
Tensor Type::ge(const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::ge(/* native_actuals */ self, other);
}
Tensor & Type::le_out(Tensor & out, const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::le_out(/* native_actuals */ out, self, other);
}
Tensor Type::le(const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::le(/* native_actuals */ self, other);
}
Tensor & Type::le_out(Tensor & out, const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::le_out(/* native_actuals */ out, self, other);
}
Tensor Type::le(const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::le(/* native_actuals */ self, other);
}
Tensor & Type::gt_out(Tensor & out, const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::gt_out(/* native_actuals */ out, self, other);
}
Tensor Type::gt(const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::gt(/* native_actuals */ self, other);
}
Tensor & Type::gt_out(Tensor & out, const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::gt_out(/* native_actuals */ out, self, other);
}
Tensor Type::gt(const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::gt(/* native_actuals */ self, other);
}
Tensor & Type::lt_out(Tensor & out, const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::lt_out(/* native_actuals */ out, self, other);
}
Tensor Type::lt(const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::lt(/* native_actuals */ self, other);
}
Tensor & Type::lt_out(Tensor & out, const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::lt_out(/* native_actuals */ out, self, other);
}
Tensor Type::lt(const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::lt(/* native_actuals */ self, other);
}
Tensor & Type::take_out(Tensor & out, const Tensor & self, const Tensor & index) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::take_out(/* native_actuals */ out, self, index);
}
Tensor Type::take(const Tensor & self, const Tensor & index) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::take(/* native_actuals */ self, index);
}
Tensor & Type::index_select_out(Tensor & out, const Tensor & self, int64_t dim, const Tensor & index) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::index_select_out(/* native_actuals */ out, self, dim, index);
}
Tensor Type::index_select(const Tensor & self, int64_t dim, const Tensor & index) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::index_select(/* native_actuals */ self, dim, index);
}
Tensor & Type::masked_select_out(Tensor & out, const Tensor & self, const Tensor & mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::masked_select_out(/* native_actuals */ out, self, mask);
}
Tensor Type::masked_select(const Tensor & self, const Tensor & mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::masked_select(/* native_actuals */ self, mask);
}
Tensor & Type::nonzero_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::nonzero_out(/* native_actuals */ out, self);
}
Tensor Type::nonzero(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::nonzero(/* native_actuals */ self);
}
Tensor & Type::gather_out(Tensor & out, const Tensor & self, int64_t dim, const Tensor & index, bool sparse_grad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::gather_out(/* native_actuals */ out, self, dim, index, sparse_grad);
}
Tensor Type::gather(const Tensor & self, int64_t dim, const Tensor & index, bool sparse_grad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::gather(/* native_actuals */ self, dim, index, sparse_grad);
}
Tensor Type::_gather_sparse_backward(const Tensor & self, int64_t dim, const Tensor & index, const Tensor & grad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::_gather_sparse_backward(/* native_actuals */ self, dim, index, grad);
}
Tensor & Type::addcmul_out(Tensor & out, const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addcmul_out(/* native_actuals */ out, self, tensor1, tensor2, value);
}
Tensor Type::addcmul(const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addcmul(/* native_actuals */ self, tensor1, tensor2, value);
}
Tensor & Type::addcdiv_out(Tensor & out, const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addcdiv_out(/* native_actuals */ out, self, tensor1, tensor2, value);
}
Tensor Type::addcdiv(const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::addcdiv(/* native_actuals */ self, tensor1, tensor2, value);
}
std::tuple<Tensor &,Tensor &> Type::gels_out(Tensor & X, Tensor & qr, const Tensor & self, const Tensor & A) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::gels_out(/* native_actuals */ X, qr, self, A);
}
std::tuple<Tensor,Tensor> Type::gels(const Tensor & self, const Tensor & A) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::gels(/* native_actuals */ self, A);
}
std::tuple<Tensor &,Tensor &> Type::triangular_solve_out(Tensor & X, Tensor & M, const Tensor & self, const Tensor & A, bool upper, bool transpose, bool unitriangular) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::triangular_solve_out(/* native_actuals */ X, M, self, A, upper, transpose, unitriangular);
}
std::tuple<Tensor,Tensor> Type::triangular_solve(const Tensor & self, const Tensor & A, bool upper, bool transpose, bool unitriangular) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::triangular_solve(/* native_actuals */ self, A, upper, transpose, unitriangular);
}
std::tuple<Tensor &,Tensor &> Type::symeig_out(Tensor & e, Tensor & V, const Tensor & self, bool eigenvectors, bool upper) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::symeig_out(/* native_actuals */ e, V, self, eigenvectors, upper);
}
std::tuple<Tensor,Tensor> Type::symeig(const Tensor & self, bool eigenvectors, bool upper) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::symeig(/* native_actuals */ self, eigenvectors, upper);
}
std::tuple<Tensor &,Tensor &> Type::eig_out(Tensor & e, Tensor & v, const Tensor & self, bool eigenvectors) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::eig_out(/* native_actuals */ e, v, self, eigenvectors);
}
std::tuple<Tensor,Tensor> Type::eig(const Tensor & self, bool eigenvectors) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::eig(/* native_actuals */ self, eigenvectors);
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::svd_out(Tensor & U, Tensor & S, Tensor & V, const Tensor & self, bool some, bool compute_uv) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::svd_out(/* native_actuals */ U, S, V, self, some, compute_uv);
}
std::tuple<Tensor,Tensor,Tensor> Type::svd(const Tensor & self, bool some, bool compute_uv) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::svd(/* native_actuals */ self, some, compute_uv);
}
Tensor & Type::cholesky_out(Tensor & out, const Tensor & self, bool upper) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::cholesky_out(/* native_actuals */ out, self, upper);
}
Tensor Type::cholesky(const Tensor & self, bool upper) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::cholesky(/* native_actuals */ self, upper);
}
Tensor & Type::cholesky_solve_out(Tensor & out, const Tensor & self, const Tensor & input2, bool upper) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::cholesky_solve_out(/* native_actuals */ out, self, input2, upper);
}
Tensor Type::cholesky_solve(const Tensor & self, const Tensor & input2, bool upper) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::cholesky_solve(/* native_actuals */ self, input2, upper);
}
std::tuple<Tensor,Tensor> Type::solve(const Tensor & self, const Tensor & A) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::solve(/* native_actuals */ self, A);
}
std::tuple<Tensor &,Tensor &> Type::solve_out(Tensor & solution, Tensor & lu, const Tensor & self, const Tensor & A) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::solve_out(/* native_actuals */ solution, lu, self, A);
}
Tensor & Type::cholesky_inverse_out(Tensor & out, const Tensor & self, bool upper) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::cholesky_inverse_out(/* native_actuals */ out, self, upper);
}
Tensor Type::cholesky_inverse(const Tensor & self, bool upper) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::cholesky_inverse(/* native_actuals */ self, upper);
}
std::tuple<Tensor &,Tensor &> Type::pstrf_out(Tensor & u, Tensor & pivot, const Tensor & self, bool upper, Scalar tol) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::pstrf_out(/* native_actuals */ u, pivot, self, upper, tol);
}
std::tuple<Tensor,Tensor> Type::pstrf(const Tensor & self, bool upper, Scalar tol) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::pstrf(/* native_actuals */ self, upper, tol);
}
std::tuple<Tensor &,Tensor &> Type::qr_out(Tensor & Q, Tensor & R, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::qr_out(/* native_actuals */ Q, R, self);
}
std::tuple<Tensor,Tensor> Type::qr(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::qr(/* native_actuals */ self);
}
std::tuple<Tensor &,Tensor &> Type::geqrf_out(Tensor & a, Tensor & tau, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::geqrf_out(/* native_actuals */ a, tau, self);
}
std::tuple<Tensor,Tensor> Type::geqrf(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::geqrf(/* native_actuals */ self);
}
Tensor & Type::orgqr_out(Tensor & out, const Tensor & self, const Tensor & input2) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::orgqr_out(/* native_actuals */ out, self, input2);
}
Tensor Type::orgqr(const Tensor & self, const Tensor & input2) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::orgqr(/* native_actuals */ self, input2);
}
Tensor & Type::ormqr_out(Tensor & out, const Tensor & self, const Tensor & input2, const Tensor & input3, bool left, bool transpose) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::ormqr_out(/* native_actuals */ out, self, input2, input3, left, transpose);
}
Tensor Type::ormqr(const Tensor & self, const Tensor & input2, const Tensor & input3, bool left, bool transpose) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::ormqr(/* native_actuals */ self, input2, input3, left, transpose);
}
Tensor & Type::lu_solve_out(Tensor & out, const Tensor & self, const Tensor & LU_data, const Tensor & LU_pivots) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::lu_solve_out(/* native_actuals */ out, self, LU_data, LU_pivots);
}
Tensor Type::lu_solve(const Tensor & self, const Tensor & LU_data, const Tensor & LU_pivots) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::lu_solve(/* native_actuals */ self, LU_data, LU_pivots);
}
Tensor & Type::multinomial_out(Tensor & out, const Tensor & self, int64_t num_samples, bool replacement, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::multinomial_out(/* native_actuals */ out, self, num_samples, replacement, generator);
}
Tensor Type::multinomial(const Tensor & self, int64_t num_samples, bool replacement, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::multinomial(/* native_actuals */ self, num_samples, replacement, generator);
}
std::tuple<Tensor,Tensor> Type::_multinomial_alias_setup(const Tensor & probs) const {
    const OptionalDeviceGuard device_guard(device_of(probs));
    return at::native::_multinomial_alias_setup(/* native_actuals */ probs);
}
Tensor Type::_multinomial_alias_draw(const Tensor & J, const Tensor & q, int64_t num_samples, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(J));
    return at::native::_multinomial_alias_draw(/* native_actuals */ J, q, num_samples, generator);
}
Tensor & Type::lgamma_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::lgamma_out(/* native_actuals */ out, self);
}
Tensor Type::lgamma(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::lgamma(/* native_actuals */ self);
}
Tensor & Type::digamma_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::digamma_out(/* native_actuals */ out, self);
}
Tensor Type::digamma(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::digamma(/* native_actuals */ self);
}
Tensor & Type::polygamma_out(Tensor & out, int64_t n, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::polygamma_out(/* native_actuals */ out, n, self);
}
Tensor Type::polygamma(int64_t n, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::polygamma(/* native_actuals */ n, self);
}
Tensor & Type::erfinv_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::erfinv_out(/* native_actuals */ out, self);
}
Tensor Type::erfinv(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::erfinv(/* native_actuals */ self);
}
Tensor Type::dist(const Tensor & self, const Tensor & other, Scalar p) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::dist(/* native_actuals */ self, other, p);
}
Tensor & Type::atan2_out(Tensor & out, const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::atan2_out(/* native_actuals */ out, self, other);
}
Tensor Type::atan2(const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::atan2(/* native_actuals */ self, other);
}
Tensor & Type::sign_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sign_out(/* native_actuals */ out, self);
}
Tensor Type::sign(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sign(/* native_actuals */ self);
}
Tensor & Type::fmod_out(Tensor & out, const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::fmod_out(/* native_actuals */ out, self, other);
}
Tensor Type::fmod(const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::fmod(/* native_actuals */ self, other);
}
Tensor & Type::fmod_out(Tensor & out, const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::fmod_out(/* native_actuals */ out, self, other);
}
Tensor Type::fmod(const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::fmod(/* native_actuals */ self, other);
}
Tensor & Type::remainder_out(Tensor & out, const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::remainder_out(/* native_actuals */ out, self, other);
}
Tensor Type::remainder(const Tensor & self, Scalar other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::remainder(/* native_actuals */ self, other);
}
Tensor & Type::remainder_out(Tensor & out, const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::remainder_out(/* native_actuals */ out, self, other);
}
Tensor Type::remainder(const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::remainder(/* native_actuals */ self, other);
}
Tensor & Type::min_out(Tensor & out, const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::min_out(/* native_actuals */ out, self, other);
}
Tensor Type::min(const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::min(/* native_actuals */ self, other);
}
Tensor Type::min(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::min(/* native_actuals */ self);
}
Tensor & Type::max_out(Tensor & out, const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_out(/* native_actuals */ out, self, other);
}
Tensor Type::max(const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max(/* native_actuals */ self, other);
}
Tensor Type::max(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max(/* native_actuals */ self);
}
std::tuple<Tensor &,Tensor &> Type::sort_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool descending) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sort_out(/* native_actuals */ values, indices, self, dim, descending);
}
std::tuple<Tensor,Tensor> Type::sort(const Tensor & self, int64_t dim, bool descending) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::sort(/* native_actuals */ self, dim, descending);
}
Tensor Type::argsort(const Tensor & self, int64_t dim, bool descending) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::argsort(/* native_actuals */ self, dim, descending);
}
std::tuple<Tensor &,Tensor &> Type::topk_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::topk_out(/* native_actuals */ values, indices, self, k, dim, largest, sorted);
}
std::tuple<Tensor,Tensor> Type::topk(const Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::topk(/* native_actuals */ self, k, dim, largest, sorted);
}
Tensor Type::all(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::all(/* native_actuals */ self);
}
Tensor Type::any(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::any(/* native_actuals */ self);
}
Tensor & Type::renorm_out(Tensor & out, const Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::renorm_out(/* native_actuals */ out, self, p, dim, maxnorm);
}
Tensor Type::renorm(const Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::renorm(/* native_actuals */ self, p, dim, maxnorm);
}
Tensor Type::unfold(const Tensor & self, int64_t dimension, int64_t size, int64_t step) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::unfold(/* native_actuals */ self, dimension, size, step);
}
bool Type::equal(const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::equal(/* native_actuals */ self, other);
}
Tensor & Type::pow_out(Tensor & out, const Tensor & self, const Tensor & exponent) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::pow_out(/* native_actuals */ out, self, exponent);
}
Tensor Type::pow(const Tensor & self, const Tensor & exponent) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::pow(/* native_actuals */ self, exponent);
}
Tensor & Type::pow_out(Tensor & out, Scalar self, const Tensor & exponent) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::pow_out(/* native_actuals */ out, self, exponent);
}
Tensor Type::pow(Scalar self, const Tensor & exponent) const {
    const OptionalDeviceGuard device_guard(device_of(exponent));
    return at::native::pow(/* native_actuals */ self, exponent);
}
Tensor & Type::normal_out(Tensor & out, const Tensor & mean, double std, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::normal_out(/* native_actuals */ out, mean, std, generator);
}
Tensor Type::normal(const Tensor & mean, double std, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(mean));
    return at::native::normal(/* native_actuals */ mean, std, generator);
}
Tensor & Type::normal_out(Tensor & out, double mean, const Tensor & std, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::normal_out(/* native_actuals */ out, mean, std, generator);
}
Tensor Type::normal(double mean, const Tensor & std, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(std));
    return at::native::normal(/* native_actuals */ mean, std, generator);
}
Tensor & Type::normal_out(Tensor & out, const Tensor & mean, const Tensor & std, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::normal_out(/* native_actuals */ out, mean, std, generator);
}
Tensor Type::normal(const Tensor & mean, const Tensor & std, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(mean));
    return at::native::normal(/* native_actuals */ mean, std, generator);
}
Tensor Type::alias(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::alias(/* native_actuals */ self);
}
Tensor & Type::_dirichlet_grad_out(Tensor & out, const Tensor & x, const Tensor & alpha, const Tensor & total) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    return at::native::_dirichlet_grad_out(/* native_actuals */ out, x, alpha, total);
}
Tensor Type::_dirichlet_grad(const Tensor & x, const Tensor & alpha, const Tensor & total) const {
    const OptionalDeviceGuard device_guard(device_of(x));
    return at::native::_dirichlet_grad(/* native_actuals */ x, alpha, total);
}
Tensor & Type::binary_cross_entropy_out(Tensor & out, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::binary_cross_entropy_out(/* native_actuals */ out, self, target, weight, reduction);
}
Tensor Type::binary_cross_entropy(const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::binary_cross_entropy(/* native_actuals */ self, target, weight, reduction);
}
Tensor & Type::binary_cross_entropy_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::binary_cross_entropy_backward_out(/* native_actuals */ grad_input, grad_output, self, target, weight, reduction);
}
Tensor Type::binary_cross_entropy_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::binary_cross_entropy_backward(/* native_actuals */ grad_output, self, target, weight, reduction);
}
Tensor & Type::mse_loss_out(Tensor & out, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mse_loss_out(/* native_actuals */ out, self, target, reduction);
}
Tensor Type::mse_loss(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mse_loss(/* native_actuals */ self, target, reduction);
}
Tensor & Type::mse_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mse_loss_backward_out(/* native_actuals */ grad_input, grad_output, self, target, reduction);
}
Tensor Type::mse_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mse_loss_backward(/* native_actuals */ grad_output, self, target, reduction);
}
Tensor & Type::l1_loss_out(Tensor & out, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::l1_loss_out(/* native_actuals */ out, self, target, reduction);
}
Tensor Type::l1_loss(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::l1_loss(/* native_actuals */ self, target, reduction);
}
Tensor & Type::l1_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::l1_loss_backward_out(/* native_actuals */ grad_input, grad_output, self, target, reduction);
}
Tensor Type::l1_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::l1_loss_backward(/* native_actuals */ grad_output, self, target, reduction);
}
Tensor & Type::multi_margin_loss_out(Tensor & out, const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::multi_margin_loss_out(/* native_actuals */ out, self, target, p, margin, weight, reduction);
}
Tensor Type::multi_margin_loss(const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::multi_margin_loss(/* native_actuals */ self, target, p, margin, weight, reduction);
}
Tensor & Type::multi_margin_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::multi_margin_loss_backward_out(/* native_actuals */ grad_input, grad_output, self, target, p, margin, weight, reduction);
}
Tensor Type::multi_margin_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::multi_margin_loss_backward(/* native_actuals */ grad_output, self, target, p, margin, weight, reduction);
}
Tensor & Type::multilabel_margin_loss_out(Tensor & out, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::multilabel_margin_loss_out(/* native_actuals */ out, self, target, reduction);
}
Tensor Type::multilabel_margin_loss(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::multilabel_margin_loss(/* native_actuals */ self, target, reduction);
}
std::tuple<Tensor &,Tensor &> Type::multilabel_margin_loss_forward_out(Tensor & output, Tensor & is_target, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::multilabel_margin_loss_forward_out(/* native_actuals */ output, is_target, self, target, reduction);
}
std::tuple<Tensor,Tensor> Type::multilabel_margin_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::multilabel_margin_loss_forward(/* native_actuals */ self, target, reduction);
}
Tensor & Type::multilabel_margin_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction, const Tensor & is_target) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::multilabel_margin_loss_backward_out(/* native_actuals */ grad_input, grad_output, self, target, reduction, is_target);
}
Tensor Type::multilabel_margin_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction, const Tensor & is_target) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::multilabel_margin_loss_backward(/* native_actuals */ grad_output, self, target, reduction, is_target);
}
Tensor & Type::nll_loss_out(Tensor & out, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::nll_loss_out(/* native_actuals */ out, self, target, weight, reduction, ignore_index);
}
Tensor Type::nll_loss(const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::nll_loss(/* native_actuals */ self, target, weight, reduction, ignore_index);
}
std::tuple<Tensor &,Tensor &> Type::nll_loss_forward_out(Tensor & output, Tensor & total_weight, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::nll_loss_forward_out(/* native_actuals */ output, total_weight, self, target, weight, reduction, ignore_index);
}
std::tuple<Tensor,Tensor> Type::nll_loss_forward(const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::nll_loss_forward(/* native_actuals */ self, target, weight, reduction, ignore_index);
}
Tensor & Type::nll_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::nll_loss_backward_out(/* native_actuals */ grad_input, grad_output, self, target, weight, reduction, ignore_index, total_weight);
}
Tensor Type::nll_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::nll_loss_backward(/* native_actuals */ grad_output, self, target, weight, reduction, ignore_index, total_weight);
}
Tensor & Type::nll_loss2d_out(Tensor & out, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::nll_loss2d_out(/* native_actuals */ out, self, target, weight, reduction, ignore_index);
}
Tensor Type::nll_loss2d(const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::nll_loss2d(/* native_actuals */ self, target, weight, reduction, ignore_index);
}
std::tuple<Tensor &,Tensor &> Type::nll_loss2d_forward_out(Tensor & output, Tensor & total_weight, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::nll_loss2d_forward_out(/* native_actuals */ output, total_weight, self, target, weight, reduction, ignore_index);
}
std::tuple<Tensor,Tensor> Type::nll_loss2d_forward(const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::nll_loss2d_forward(/* native_actuals */ self, target, weight, reduction, ignore_index);
}
Tensor & Type::nll_loss2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::nll_loss2d_backward_out(/* native_actuals */ grad_input, grad_output, self, target, weight, reduction, ignore_index, total_weight);
}
Tensor Type::nll_loss2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::nll_loss2d_backward(/* native_actuals */ grad_output, self, target, weight, reduction, ignore_index, total_weight);
}
Tensor & Type::smooth_l1_loss_out(Tensor & out, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::smooth_l1_loss_out(/* native_actuals */ out, self, target, reduction);
}
Tensor Type::smooth_l1_loss(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::smooth_l1_loss(/* native_actuals */ self, target, reduction);
}
Tensor & Type::smooth_l1_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::smooth_l1_loss_backward_out(/* native_actuals */ grad_input, grad_output, self, target, reduction);
}
Tensor Type::smooth_l1_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::smooth_l1_loss_backward(/* native_actuals */ grad_output, self, target, reduction);
}
Tensor & Type::soft_margin_loss_out(Tensor & out, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::soft_margin_loss_out(/* native_actuals */ out, self, target, reduction);
}
Tensor Type::soft_margin_loss(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::soft_margin_loss(/* native_actuals */ self, target, reduction);
}
Tensor & Type::soft_margin_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::soft_margin_loss_backward_out(/* native_actuals */ grad_input, grad_output, self, target, reduction);
}
Tensor Type::soft_margin_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::soft_margin_loss_backward(/* native_actuals */ grad_output, self, target, reduction);
}
Tensor & Type::elu_out(Tensor & out, const Tensor & self, Scalar alpha, Scalar scale, Scalar input_scale) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::elu_out(/* native_actuals */ out, self, alpha, scale, input_scale);
}
Tensor Type::elu(const Tensor & self, Scalar alpha, Scalar scale, Scalar input_scale) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::elu(/* native_actuals */ self, alpha, scale, input_scale);
}
Tensor & Type::elu_backward_out(Tensor & grad_input, const Tensor & grad_output, Scalar alpha, Scalar scale, Scalar input_scale, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    return at::native::elu_backward_out(/* native_actuals */ grad_input, grad_output, alpha, scale, input_scale, output);
}
Tensor Type::elu_backward(const Tensor & grad_output, Scalar alpha, Scalar scale, Scalar input_scale, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    return at::native::elu_backward(/* native_actuals */ grad_output, alpha, scale, input_scale, output);
}
Tensor & Type::elu_(Tensor & self, Scalar alpha, Scalar scale, Scalar input_scale) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::elu_(/* native_actuals */ self, alpha, scale, input_scale);
}
Tensor & Type::glu_out(Tensor & out, const Tensor & self, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::glu_out(/* native_actuals */ out, self, dim);
}
Tensor Type::glu(const Tensor & self, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::glu(/* native_actuals */ self, dim);
}
Tensor & Type::glu_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::glu_backward_out(/* native_actuals */ grad_input, grad_output, self, dim);
}
Tensor Type::glu_backward(const Tensor & grad_output, const Tensor & self, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::glu_backward(/* native_actuals */ grad_output, self, dim);
}
Tensor & Type::hardtanh_out(Tensor & out, const Tensor & self, Scalar min_val, Scalar max_val) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::hardtanh_out(/* native_actuals */ out, self, min_val, max_val);
}
Tensor Type::hardtanh(const Tensor & self, Scalar min_val, Scalar max_val) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::hardtanh(/* native_actuals */ self, min_val, max_val);
}
Tensor & Type::hardtanh_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar min_val, Scalar max_val) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::hardtanh_backward_out(/* native_actuals */ grad_input, grad_output, self, min_val, max_val);
}
Tensor Type::hardtanh_backward(const Tensor & grad_output, const Tensor & self, Scalar min_val, Scalar max_val) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::hardtanh_backward(/* native_actuals */ grad_output, self, min_val, max_val);
}
Tensor & Type::hardtanh_(Tensor & self, Scalar min_val, Scalar max_val) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::hardtanh_(/* native_actuals */ self, min_val, max_val);
}
Tensor & Type::leaky_relu_out(Tensor & out, const Tensor & self, Scalar negative_slope) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::leaky_relu_out(/* native_actuals */ out, self, negative_slope);
}
Tensor Type::leaky_relu(const Tensor & self, Scalar negative_slope) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::leaky_relu(/* native_actuals */ self, negative_slope);
}
Tensor & Type::leaky_relu_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar negative_slope) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::leaky_relu_backward_out(/* native_actuals */ grad_input, grad_output, self, negative_slope);
}
Tensor Type::leaky_relu_backward(const Tensor & grad_output, const Tensor & self, Scalar negative_slope) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::leaky_relu_backward(/* native_actuals */ grad_output, self, negative_slope);
}
Tensor & Type::leaky_relu_(Tensor & self, Scalar negative_slope) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::leaky_relu_(/* native_actuals */ self, negative_slope);
}
Tensor & Type::log_sigmoid_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::log_sigmoid_out(/* native_actuals */ out, self);
}
Tensor Type::log_sigmoid(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::log_sigmoid(/* native_actuals */ self);
}
std::tuple<Tensor &,Tensor &> Type::log_sigmoid_forward_out(Tensor & output, Tensor & buffer, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::log_sigmoid_forward_out(/* native_actuals */ output, buffer, self);
}
std::tuple<Tensor,Tensor> Type::log_sigmoid_forward(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::log_sigmoid_forward(/* native_actuals */ self);
}
Tensor & Type::log_sigmoid_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & buffer) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::log_sigmoid_backward_out(/* native_actuals */ grad_input, grad_output, self, buffer);
}
Tensor Type::log_sigmoid_backward(const Tensor & grad_output, const Tensor & self, const Tensor & buffer) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::log_sigmoid_backward(/* native_actuals */ grad_output, self, buffer);
}
Tensor & Type::rrelu_with_noise_out(Tensor & out, const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::rrelu_with_noise_out(/* native_actuals */ out, self, noise, lower, upper, training, generator);
}
Tensor Type::rrelu_with_noise(const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::rrelu_with_noise(/* native_actuals */ self, noise, lower, upper, training, generator);
}
Tensor & Type::rrelu_with_noise_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::rrelu_with_noise_backward_out(/* native_actuals */ grad_input, grad_output, self, noise, lower, upper, training);
}
Tensor Type::rrelu_with_noise_backward(const Tensor & grad_output, const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::rrelu_with_noise_backward(/* native_actuals */ grad_output, self, noise, lower, upper, training);
}
Tensor & Type::rrelu_with_noise_(Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::rrelu_with_noise_(/* native_actuals */ self, noise, lower, upper, training, generator);
}
Tensor & Type::softplus_out(Tensor & out, const Tensor & self, Scalar beta, Scalar threshold) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::softplus_out(/* native_actuals */ out, self, beta, threshold);
}
Tensor Type::softplus(const Tensor & self, Scalar beta, Scalar threshold) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::softplus(/* native_actuals */ self, beta, threshold);
}
Tensor & Type::softplus_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar beta, Scalar threshold, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::softplus_backward_out(/* native_actuals */ grad_input, grad_output, self, beta, threshold, output);
}
Tensor Type::softplus_backward(const Tensor & grad_output, const Tensor & self, Scalar beta, Scalar threshold, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::softplus_backward(/* native_actuals */ grad_output, self, beta, threshold, output);
}
Tensor & Type::softshrink_out(Tensor & out, const Tensor & self, Scalar lambd) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::softshrink_out(/* native_actuals */ out, self, lambd);
}
Tensor Type::softshrink(const Tensor & self, Scalar lambd) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::softshrink(/* native_actuals */ self, lambd);
}
Tensor & Type::softshrink_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar lambd) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::softshrink_backward_out(/* native_actuals */ grad_input, grad_output, self, lambd);
}
Tensor Type::softshrink_backward(const Tensor & grad_output, const Tensor & self, Scalar lambd) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::softshrink_backward(/* native_actuals */ grad_output, self, lambd);
}
Tensor Type::adaptive_avg_pool2d(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::adaptive_avg_pool2d(/* native_actuals */ self, output_size);
}
Tensor & Type::adaptive_avg_pool3d_out(Tensor & out, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::adaptive_avg_pool3d_out(/* native_actuals */ out, self, output_size);
}
Tensor Type::adaptive_avg_pool3d(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::adaptive_avg_pool3d(/* native_actuals */ self, output_size);
}
Tensor & Type::adaptive_avg_pool3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::adaptive_avg_pool3d_backward_out(/* native_actuals */ grad_input, grad_output, self);
}
Tensor Type::adaptive_avg_pool3d_backward(const Tensor & grad_output, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::adaptive_avg_pool3d_backward(/* native_actuals */ grad_output, self);
}
Tensor & Type::avg_pool2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::avg_pool2d_backward_out(/* native_actuals */ grad_input, grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad);
}
Tensor Type::avg_pool2d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::avg_pool2d_backward(/* native_actuals */ grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad);
}
Tensor & Type::avg_pool3d_out(Tensor & out, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::avg_pool3d_out(/* native_actuals */ out, self, kernel_size, stride, padding, ceil_mode, count_include_pad);
}
Tensor Type::avg_pool3d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::avg_pool3d(/* native_actuals */ self, kernel_size, stride, padding, ceil_mode, count_include_pad);
}
Tensor & Type::avg_pool3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::avg_pool3d_backward_out(/* native_actuals */ grad_input, grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad);
}
Tensor Type::avg_pool3d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::avg_pool3d_backward(/* native_actuals */ grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad);
}
std::tuple<Tensor &,Tensor &> Type::max_pool2d_with_indices_out(Tensor & output, Tensor & indices, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_pool2d_with_indices_out(/* native_actuals */ output, indices, self, kernel_size, stride, padding, dilation, ceil_mode);
}
std::tuple<Tensor,Tensor> Type::max_pool2d_with_indices(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_pool2d_with_indices(/* native_actuals */ self, kernel_size, stride, padding, dilation, ceil_mode);
}
Tensor & Type::max_pool2d_with_indices_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_pool2d_with_indices_backward_out(/* native_actuals */ grad_input, grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
}
Tensor Type::max_pool2d_with_indices_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_pool2d_with_indices_backward(/* native_actuals */ grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
}
std::tuple<Tensor &,Tensor &> Type::max_pool3d_with_indices_out(Tensor & output, Tensor & indices, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_pool3d_with_indices_out(/* native_actuals */ output, indices, self, kernel_size, stride, padding, dilation, ceil_mode);
}
std::tuple<Tensor,Tensor> Type::max_pool3d_with_indices(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_pool3d_with_indices(/* native_actuals */ self, kernel_size, stride, padding, dilation, ceil_mode);
}
Tensor & Type::max_pool3d_with_indices_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_pool3d_with_indices_backward_out(/* native_actuals */ grad_input, grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
}
Tensor Type::max_pool3d_with_indices_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_pool3d_with_indices_backward(/* native_actuals */ grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
}
Tensor & Type::max_unpool2d_out(Tensor & out, const Tensor & self, const Tensor & indices, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_unpool2d_out(/* native_actuals */ out, self, indices, output_size);
}
Tensor Type::max_unpool2d(const Tensor & self, const Tensor & indices, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_unpool2d(/* native_actuals */ self, indices, output_size);
}
Tensor & Type::max_unpool2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_unpool2d_backward_out(/* native_actuals */ grad_input, grad_output, self, indices, output_size);
}
Tensor Type::max_unpool2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_unpool2d_backward(/* native_actuals */ grad_output, self, indices, output_size);
}
Tensor & Type::max_unpool3d_out(Tensor & out, const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_unpool3d_out(/* native_actuals */ out, self, indices, output_size, stride, padding);
}
Tensor Type::max_unpool3d(const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_unpool3d(/* native_actuals */ self, indices, output_size, stride, padding);
}
Tensor & Type::max_unpool3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_unpool3d_backward_out(/* native_actuals */ grad_input, grad_output, self, indices, output_size, stride, padding);
}
Tensor Type::max_unpool3d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::max_unpool3d_backward(/* native_actuals */ grad_output, self, indices, output_size, stride, padding);
}
Tensor & Type::sigmoid_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    return at::native::sigmoid_backward_out(/* native_actuals */ grad_input, grad_output, output);
}
Tensor Type::sigmoid_backward(const Tensor & grad_output, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    return at::native::sigmoid_backward(/* native_actuals */ grad_output, output);
}
Tensor & Type::tanh_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    return at::native::tanh_backward_out(/* native_actuals */ grad_input, grad_output, output);
}
Tensor Type::tanh_backward(const Tensor & grad_output, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    return at::native::tanh_backward(/* native_actuals */ grad_output, output);
}
Tensor & Type::thnn_conv_transpose2d_out(Tensor & out, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_transpose2d_out(/* native_actuals */ out, self, weight, kernel_size, bias, stride, padding, output_padding, dilation);
}
Tensor Type::thnn_conv_transpose2d(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_transpose2d(/* native_actuals */ self, weight, kernel_size, bias, stride, padding, output_padding, dilation);
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::thnn_conv_transpose2d_forward_out(Tensor & output, Tensor & columns, Tensor & ones, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_transpose2d_forward_out(/* native_actuals */ output, columns, ones, self, weight, kernel_size, bias, stride, padding, output_padding, dilation);
}
std::tuple<Tensor,Tensor,Tensor> Type::thnn_conv_transpose2d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_transpose2d_forward(/* native_actuals */ self, weight, kernel_size, bias, stride, padding, output_padding, dilation);
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::thnn_conv_transpose2d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation, const Tensor & columns, const Tensor & ones) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_transpose2d_backward_out(/* native_actuals */ grad_input, grad_weight, grad_bias, grad_output, self, weight, kernel_size, stride, padding, output_padding, dilation, columns, ones);
}
std::tuple<Tensor,Tensor,Tensor> Type::thnn_conv_transpose2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation, const Tensor & columns, const Tensor & ones, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_transpose2d_backward(/* native_actuals */ grad_output, self, weight, kernel_size, stride, padding, output_padding, dilation, columns, ones, output_mask);
}
Tensor & Type::thnn_conv_transpose3d_out(Tensor & out, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_transpose3d_out(/* native_actuals */ out, self, weight, kernel_size, bias, stride, padding, output_padding, dilation);
}
Tensor Type::thnn_conv_transpose3d(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_transpose3d(/* native_actuals */ self, weight, kernel_size, bias, stride, padding, output_padding, dilation);
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::thnn_conv_transpose3d_forward_out(Tensor & output, Tensor & finput, Tensor & fgrad_input, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_transpose3d_forward_out(/* native_actuals */ output, finput, fgrad_input, self, weight, kernel_size, bias, stride, padding, output_padding, dilation);
}
std::tuple<Tensor,Tensor,Tensor> Type::thnn_conv_transpose3d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_transpose3d_forward(/* native_actuals */ self, weight, kernel_size, bias, stride, padding, output_padding, dilation);
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::thnn_conv_transpose3d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation, const Tensor & finput, const Tensor & fgrad_input) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_transpose3d_backward_out(/* native_actuals */ grad_input, grad_weight, grad_bias, grad_output, self, weight, kernel_size, stride, padding, output_padding, dilation, finput, fgrad_input);
}
std::tuple<Tensor,Tensor,Tensor> Type::thnn_conv_transpose3d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation, const Tensor & finput, const Tensor & fgrad_input, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_transpose3d_backward(/* native_actuals */ grad_output, self, weight, kernel_size, stride, padding, output_padding, dilation, finput, fgrad_input, output_mask);
}
Tensor & Type::thnn_conv2d_out(Tensor & out, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv2d_out(/* native_actuals */ out, self, weight, kernel_size, bias, stride, padding);
}
Tensor Type::thnn_conv2d(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv2d(/* native_actuals */ self, weight, kernel_size, bias, stride, padding);
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::thnn_conv2d_forward_out(Tensor & output, Tensor & finput, Tensor & fgrad_input, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv2d_forward_out(/* native_actuals */ output, finput, fgrad_input, self, weight, kernel_size, bias, stride, padding);
}
std::tuple<Tensor,Tensor,Tensor> Type::thnn_conv2d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv2d_forward(/* native_actuals */ self, weight, kernel_size, bias, stride, padding);
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::thnn_conv2d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, const Tensor & finput, const Tensor & fgrad_input) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv2d_backward_out(/* native_actuals */ grad_input, grad_weight, grad_bias, grad_output, self, weight, kernel_size, stride, padding, finput, fgrad_input);
}
std::tuple<Tensor,Tensor,Tensor> Type::thnn_conv2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, const Tensor & finput, const Tensor & fgrad_input, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv2d_backward(/* native_actuals */ grad_output, self, weight, kernel_size, stride, padding, finput, fgrad_input, output_mask);
}
Tensor & Type::thnn_conv_depthwise2d_out(Tensor & out, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_depthwise2d_out(/* native_actuals */ out, self, weight, kernel_size, bias, stride, padding, dilation);
}
Tensor Type::thnn_conv_depthwise2d(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_depthwise2d(/* native_actuals */ self, weight, kernel_size, bias, stride, padding, dilation);
}
Tensor & Type::thnn_conv_depthwise2d_forward_out(Tensor & out, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_depthwise2d_forward_out(/* native_actuals */ out, self, weight, kernel_size, bias, stride, padding, dilation);
}
Tensor Type::thnn_conv_depthwise2d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_depthwise2d_forward(/* native_actuals */ self, weight, kernel_size, bias, stride, padding, dilation);
}
std::tuple<Tensor &,Tensor &> Type::thnn_conv_depthwise2d_backward_out(Tensor & grad_input, Tensor & grad_weight, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_depthwise2d_backward_out(/* native_actuals */ grad_input, grad_weight, grad_output, self, weight, kernel_size, stride, padding, dilation);
}
std::tuple<Tensor,Tensor> Type::thnn_conv_depthwise2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, std::array<bool,2> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_depthwise2d_backward(/* native_actuals */ grad_output, self, weight, kernel_size, stride, padding, dilation, output_mask);
}
Tensor & Type::thnn_conv3d_out(Tensor & out, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv3d_out(/* native_actuals */ out, self, weight, kernel_size, bias, stride, padding);
}
Tensor Type::thnn_conv3d(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv3d(/* native_actuals */ self, weight, kernel_size, bias, stride, padding);
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::thnn_conv3d_forward_out(Tensor & output, Tensor & finput, Tensor & fgrad_input, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv3d_forward_out(/* native_actuals */ output, finput, fgrad_input, self, weight, kernel_size, bias, stride, padding);
}
std::tuple<Tensor,Tensor,Tensor> Type::thnn_conv3d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv3d_forward(/* native_actuals */ self, weight, kernel_size, bias, stride, padding);
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::thnn_conv3d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, const Tensor & finput, const Tensor & fgrad_input) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv3d_backward_out(/* native_actuals */ grad_input, grad_weight, grad_bias, grad_output, self, weight, kernel_size, stride, padding, finput, fgrad_input);
}
std::tuple<Tensor,Tensor,Tensor> Type::thnn_conv3d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, const Tensor & finput, const Tensor & fgrad_input, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv3d_backward(/* native_actuals */ grad_output, self, weight, kernel_size, stride, padding, finput, fgrad_input, output_mask);
}
Tensor & Type::thnn_conv_dilated2d_out(Tensor & out, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_dilated2d_out(/* native_actuals */ out, self, weight, kernel_size, bias, stride, padding, dilation);
}
Tensor Type::thnn_conv_dilated2d(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_dilated2d(/* native_actuals */ self, weight, kernel_size, bias, stride, padding, dilation);
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::thnn_conv_dilated2d_forward_out(Tensor & output, Tensor & columns, Tensor & ones, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_dilated2d_forward_out(/* native_actuals */ output, columns, ones, self, weight, kernel_size, bias, stride, padding, dilation);
}
std::tuple<Tensor,Tensor,Tensor> Type::thnn_conv_dilated2d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_dilated2d_forward(/* native_actuals */ self, weight, kernel_size, bias, stride, padding, dilation);
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::thnn_conv_dilated2d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, const Tensor & columns, const Tensor & ones) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_dilated2d_backward_out(/* native_actuals */ grad_input, grad_weight, grad_bias, grad_output, self, weight, kernel_size, stride, padding, dilation, columns, ones);
}
std::tuple<Tensor,Tensor,Tensor> Type::thnn_conv_dilated2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, const Tensor & columns, const Tensor & ones, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_dilated2d_backward(/* native_actuals */ grad_output, self, weight, kernel_size, stride, padding, dilation, columns, ones, output_mask);
}
Tensor & Type::thnn_conv_dilated3d_out(Tensor & out, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_dilated3d_out(/* native_actuals */ out, self, weight, kernel_size, bias, stride, padding, dilation);
}
Tensor Type::thnn_conv_dilated3d(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_dilated3d(/* native_actuals */ self, weight, kernel_size, bias, stride, padding, dilation);
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::thnn_conv_dilated3d_forward_out(Tensor & output, Tensor & columns, Tensor & ones, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_dilated3d_forward_out(/* native_actuals */ output, columns, ones, self, weight, kernel_size, bias, stride, padding, dilation);
}
std::tuple<Tensor,Tensor,Tensor> Type::thnn_conv_dilated3d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_dilated3d_forward(/* native_actuals */ self, weight, kernel_size, bias, stride, padding, dilation);
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::thnn_conv_dilated3d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, const Tensor & columns, const Tensor & ones) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_dilated3d_backward_out(/* native_actuals */ grad_input, grad_weight, grad_bias, grad_output, self, weight, kernel_size, stride, padding, dilation, columns, ones);
}
std::tuple<Tensor,Tensor,Tensor> Type::thnn_conv_dilated3d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, const Tensor & columns, const Tensor & ones, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_conv_dilated3d_backward(/* native_actuals */ grad_output, self, weight, kernel_size, stride, padding, dilation, columns, ones, output_mask);
}
Tensor Type::thnn_col2im(const Tensor & self, IntArrayRef output_size, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_col2im(/* native_actuals */ self, output_size, kernel_size, dilation, padding, stride);
}
Tensor Type::thnn_col2im_backward(const Tensor & grad_output, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    return at::native::thnn_col2im_backward(/* native_actuals */ grad_output, kernel_size, dilation, padding, stride);
}
Tensor Type::thnn_im2col(const Tensor & self, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::thnn_im2col(/* native_actuals */ self, kernel_size, dilation, padding, stride);
}
Tensor Type::thnn_im2col_backward(const Tensor & grad_output, IntArrayRef input_size, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    return at::native::thnn_im2col_backward(/* native_actuals */ grad_output, input_size, kernel_size, dilation, padding, stride);
}
Tensor & Type::_th_set_(Tensor & self, Storage source) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto source_ = checked_storage(source,"source",2, DeviceType::CPU, at::scalarTypeToTypeMeta(ScalarType::Float));
            THFloatTensor_setStorage(self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto source_ = checked_storage(source,"source",2, DeviceType::CPU, at::scalarTypeToTypeMeta(ScalarType::Long));
            THLongTensor_setStorage(self_, source_.unsafeGetStorageImpl(), 0, {static_cast<int64_t>(source.size())}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_set_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_set_(Tensor & self, Storage source, int64_t storage_offset, IntArrayRef size, IntArrayRef stride) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto source_ = checked_storage(source,"source",2, DeviceType::CPU, at::scalarTypeToTypeMeta(ScalarType::Float));
            THFloatTensor_setStorage(self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            self_->maybe_zero_dim(size.size() == 0);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto source_ = checked_storage(source,"source",2, DeviceType::CPU, at::scalarTypeToTypeMeta(ScalarType::Long));
            THLongTensor_setStorage(self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            self_->maybe_zero_dim(size.size() == 0);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_set_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_set_(Tensor & self, const Tensor & source) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto source_ = checked_tensor_unwrap(source,"source",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_set(self_, source_);
            self_->maybe_zero_dim(source_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_set(self_, source_);
            self_->maybe_zero_dim(source_->dim() == 0);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_set_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_set_(Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_setStorage(self_, NULL, 0, {0}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            THLongTensor_setStorage(self_, NULL, 0, {0}, {});
            self_->maybe_zero_dim(false);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_set_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_fill_(Tensor & self, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto value_ = value.toFloat();
            THFloatTensor_fill(self_, value_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto value_ = value.toLong();
            THLongTensor_fill(self_, value_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_fill_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_fill_(Tensor & self, const Tensor & value) const {
    // DeviceGuard omitted
    if (value.dim() == 0) {
        return static_cast<const TypeExtendedInterface*>(this)->_th_fill_(self, value.item());
    }
    AT_ERROR("_th_fill_ only supports a 0-dimensional value tensor, but got tensor "
        "with ", value.dim(), " dimension(s).");
}
bool Type::_th_is_set_to(const Tensor & self, const Tensor & tensor) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto tensor_ = checked_tensor_unwrap(tensor,"tensor",2, false, Backend::CPU, ScalarType::Float);
            return THFloatTensor_isSetTo(self_, tensor_);
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto tensor_ = checked_tensor_unwrap(tensor,"tensor",2, false, Backend::CPU, ScalarType::Long);
            return THLongTensor_isSetTo(self_, tensor_);
            break;
        }
        default:
            AT_ERROR("_th_is_set_to not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_masked_fill_(Tensor & self, const Tensor & mask, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CPU, ScalarType::Byte);
            auto value_ = value.toFloat();
            THFloatTensor_maskedFill(self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CPU, ScalarType::Byte);
            auto value_ = value.toLong();
            THLongTensor_maskedFill(self_, mask_, value_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_masked_fill_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_masked_fill_(Tensor & self, const Tensor & mask, const Tensor & value) const {
    // DeviceGuard omitted
    if (value.dim() == 0) {
        return static_cast<const TypeExtendedInterface*>(this)->_th_masked_fill_(self, mask, value.item());
    }
    AT_ERROR("_th_masked_fill_ only supports a 0-dimensional value tensor, but got tensor "
        "with ", value.dim(), " dimension(s).");
}
Tensor & Type::_th_masked_fill_bool_(Tensor & self, const Tensor & mask, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CPU, ScalarType::Bool);
            auto value_ = value.toFloat();
            THFloatTensor_maskedFillBool(self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CPU, ScalarType::Bool);
            auto value_ = value.toLong();
            THLongTensor_maskedFillBool(self_, mask_, value_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_masked_fill_bool_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_masked_fill_bool_(Tensor & self, const Tensor & mask, const Tensor & value) const {
    // DeviceGuard omitted
    if (value.dim() == 0) {
        return static_cast<const TypeExtendedInterface*>(this)->_th_masked_fill_bool_(self, mask, value.item());
    }
    AT_ERROR("_th_masked_fill_bool_ only supports a 0-dimensional value tensor, but got tensor "
        "with ", value.dim(), " dimension(s).");
}
Tensor & Type::s__th_masked_scatter_(Tensor & self, const Tensor & mask, const Tensor & source) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CPU, ScalarType::Byte);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_maskedCopy(self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CPU, ScalarType::Byte);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CPU, ScalarType::Long);
            THLongTensor_maskedCopy(self_, mask_, source_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_masked_scatter_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_masked_scatter_bool_(Tensor & self, const Tensor & mask, const Tensor & source) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CPU, ScalarType::Bool);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_maskedCopyBool(self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CPU, ScalarType::Bool);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CPU, ScalarType::Long);
            THLongTensor_maskedCopyBool(self_, mask_, source_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_masked_scatter_bool_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_masked_select_out(Tensor & result, const Tensor & self, const Tensor & mask) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CPU, ScalarType::Byte);
            THFloatTensor_maskedSelect(result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CPU, ScalarType::Byte);
            THLongTensor_maskedSelect(result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_masked_select_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_masked_select(const Tensor & self, const Tensor & mask) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CPU, ScalarType::Byte);
            THFloatTensor_maskedSelect(result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CPU, ScalarType::Byte);
            THLongTensor_maskedSelect(result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_masked_select not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_masked_select_bool_out(Tensor & result, const Tensor & self, const Tensor & mask) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CPU, ScalarType::Bool);
            THFloatTensor_maskedSelectBool(result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CPU, ScalarType::Bool);
            THLongTensor_maskedSelectBool(result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_masked_select_bool_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_masked_select_bool(const Tensor & self, const Tensor & mask) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CPU, ScalarType::Bool);
            THFloatTensor_maskedSelectBool(result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto mask_ = checked_tensor_unwrap(mask,"mask",2, false, Backend::CPU, ScalarType::Bool);
            THLongTensor_maskedSelectBool(result_, self_, mask_);
            result_->maybe_zero_dim(self_->dim() == 0 && mask_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_masked_select_bool not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_nonzero_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_nonzero(result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            THLongTensor_nonzero(result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_nonzero_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_nonzero(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_nonzero(result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            THLongTensor_nonzero(result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_nonzero not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_clone(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THFloatTensor_newClone(self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THLongTensor_newClone(self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        default:
            AT_ERROR("_th_clone not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_view(const Tensor & self, IntArrayRef size) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THFloatTensor_newView(self_, size))->maybe_zero_dim(size.size() == 0)));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THLongTensor_newView(self_, size))->maybe_zero_dim(size.size() == 0)));
            break;
        }
        default:
            AT_ERROR("_th_view not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_resize_as_(Tensor & self, const Tensor & the_template) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto the_template_ = checked_tensor_unwrap(the_template,"the_template",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_resizeAs(self_, the_template_);
            self_->maybe_zero_dim(the_template_->dim() == 0);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto the_template_ = checked_tensor_unwrap(the_template,"the_template",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_resizeAs(self_, the_template_);
            self_->maybe_zero_dim(the_template_->dim() == 0);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_resize_as_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_index_select_out(Tensor & result, const Tensor & self, int64_t dim, const Tensor & index) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
            THFloatTensor_indexSelect(result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
            THLongTensor_indexSelect(result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_index_select_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_index_select(const Tensor & self, int64_t dim, const Tensor & index) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
            THFloatTensor_indexSelect(result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
            THLongTensor_indexSelect(result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_index_select not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_index_copy_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_indexCopy(self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CPU, ScalarType::Long);
            THLongTensor_indexCopy(self_, dim, index_, source_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_index_copy_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_take_out(Tensor & result, const Tensor & self, const Tensor & index) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CPU, ScalarType::Long);
            THFloatTensor_take(result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_take(result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_take_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_take(const Tensor & self, const Tensor & index) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CPU, ScalarType::Long);
            THFloatTensor_take(result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_take(result_, self_, index_);
            result_->maybe_zero_dim(index_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_take not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_put_(Tensor & self, const Tensor & index, const Tensor & source, bool accumulate) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CPU, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_put(self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto index_ = checked_tensor_unwrap(index,"index",2, false, Backend::CPU, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",3, false, Backend::CPU, ScalarType::Long);
            THLongTensor_put(self_, index_, source_, accumulate);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_put_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_index_add_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_indexAdd(self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
            auto source_ = checked_tensor_unwrap(source,"source",4, false, Backend::CPU, ScalarType::Long);
            THLongTensor_indexAdd(self_, dim, index_, source_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_index_add_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_index_fill_(Tensor & self, int64_t dim, const Tensor & index, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
            auto value_ = value.toFloat();
            THFloatTensor_indexFill(self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
            auto value_ = value.toLong();
            THLongTensor_indexFill(self_, dim, index_, value_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_index_fill_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_index_fill_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & value) const {
    // DeviceGuard omitted
    if (value.dim() == 0) {
        return static_cast<const TypeExtendedInterface*>(this)->_th_index_fill_(self, dim, index, value.item());
    }
    AT_ERROR("_th_index_fill_ only supports a 0-dimensional value tensor, but got tensor "
        "with ", value.dim(), " dimension(s).");
}
Tensor & Type::_th_unfold_out(Tensor & result, const Tensor & self, int64_t dimension, int64_t size, int64_t step) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dimension = maybe_wrap_dim(dimension, self_);
            THFloatTensor_unfold(result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dimension = maybe_wrap_dim(dimension, self_);
            THLongTensor_unfold(result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_unfold_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_unfold(const Tensor & self, int64_t dimension, int64_t size, int64_t step) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dimension = maybe_wrap_dim(dimension, self_);
            THFloatTensor_unfold(result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dimension = maybe_wrap_dim(dimension, self_);
            THLongTensor_unfold(result_, self_, dimension, size, step);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_unfold not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_scatter_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) const {
    // DeviceGuard omitted
    if (src.dim() == 0) {
        return static_cast<const TypeExtendedInterface*>(this)->_th_scatter_(self, dim, index, src.item());
    }
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_scatter(self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CPU, ScalarType::Long);
            THLongTensor_scatter(self_, dim, index_, src_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_scatter_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_scatter_(Tensor & self, int64_t dim, const Tensor & index, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
            auto value_ = value.toFloat();
            THFloatTensor_scatterFill(self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
            auto value_ = value.toLong();
            THLongTensor_scatterFill(self_, dim, index_, value_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_scatter_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_scatter_add_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_scatterAdd(self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
            auto src_ = checked_tensor_unwrap(src,"src",4, false, Backend::CPU, ScalarType::Long);
            THLongTensor_scatterAdd(self_, dim, index_, src_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_scatter_add_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_gather_out(Tensor & result, const Tensor & self, int64_t dim, const Tensor & index) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
            THFloatTensor_gather(result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
            THLongTensor_gather(result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gather_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_gather(const Tensor & self, int64_t dim, const Tensor & index) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
            THFloatTensor_gather(result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_tensor_unwrap(index,"index",3, false, Backend::CPU, ScalarType::Long);
            THLongTensor_gather(result_, self_, dim, index_);
            result_->maybe_zero_dim(self_->dim() == 0 && index_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gather not supported on CPUType for ", dispatch_scalar_type);
    }
}
bool Type::_th_equal(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            return THFloatTensor_equal(self_, other_);
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            return THLongTensor_equal(self_, other_);
            break;
        }
        default:
            AT_ERROR("_th_equal not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_and_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_bitand(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_bitand(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_and_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_and(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_bitand(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_bitand(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_and not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_and_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cbitand(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cbitand(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_and_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_and(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cbitand(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cbitand(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_and not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_iand_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_bitand(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_bitand(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_iand_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_iand_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cbitand(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cbitand(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_iand_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_or_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_bitor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_bitor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_or_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_or(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_bitor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_bitor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_or not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_or_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cbitor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cbitor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_or_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_or(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cbitor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cbitor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_or not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_ior_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_bitor(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_bitor(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ior_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_ior_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cbitor(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cbitor(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ior_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_xor_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_bitxor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_bitxor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_xor_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_xor(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_bitxor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_bitxor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_xor not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_xor_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cbitxor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cbitxor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_xor_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_xor(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cbitxor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cbitxor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_xor not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_ixor_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_bitxor(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_bitxor(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ixor_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_ixor_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cbitxor(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cbitxor(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ixor_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_lshift_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_lshift(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_lshift(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lshift_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_lshift(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_lshift(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_lshift(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lshift not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_lshift_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_clshift(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_clshift(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lshift_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_lshift(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_clshift(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_clshift(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lshift not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_ilshift_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_lshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_lshift(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ilshift_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_ilshift_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_clshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
            THLongTensor_clshift(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ilshift_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_rshift_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_rshift(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_rshift(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_rshift_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_rshift(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_rshift(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_rshift(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_rshift not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_rshift_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_crshift(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_crshift(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_rshift_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_rshift(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_crshift(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_crshift(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_rshift not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_irshift_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_rshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_rshift(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_irshift_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_irshift_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_crshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
            THLongTensor_crshift(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_irshift_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_lt_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_ltValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_ltValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lt_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_lt(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_ltValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_ltValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lt not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_lt_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_ltTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_ltTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lt_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_lt(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_ltTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_ltTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lt not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_lt_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_ltValueT(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_ltValueT(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_lt_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_lt_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_ltTensorT(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
            THLongTensor_ltTensorT(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_lt_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_gt_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_gtValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_gtValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gt_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_gt(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_gtValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_gtValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gt not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_gt_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_gtTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_gtTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gt_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_gt(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_gtTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_gtTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gt not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_gt_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_gtValueT(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_gtValueT(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_gt_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_gt_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_gtTensorT(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
            THLongTensor_gtTensorT(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_gt_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_le_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_leValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_leValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_le_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_le(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_leValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_leValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_le not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_le_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_leTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_leTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_le_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_le(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_leTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_leTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_le not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_le_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_leValueT(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_leValueT(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_le_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_le_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_leTensorT(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
            THLongTensor_leTensorT(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_le_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_ge_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_geValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_geValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ge_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_ge(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_geValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_geValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ge not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_ge_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_geTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_geTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ge_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_ge(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_geTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_geTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ge not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_ge_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_geValueT(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_geValueT(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ge_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_ge_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_geTensorT(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
            THLongTensor_geTensorT(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ge_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_eq_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_eqValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_eqValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_eq_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_eq(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_eqValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_eqValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_eq not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_eq_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_eqTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_eqTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_eq_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_eq(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_eqTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_eqTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_eq not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_eq_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_eqValueT(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_eqValueT(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_eq_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_eq_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_eqTensorT(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
            THLongTensor_eqTensorT(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_eq_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_ne_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_neValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_neValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ne_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_ne(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_neValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_neValue(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ne not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_ne_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_neTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Byte);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_neTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ne_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_ne(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_neTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Byte), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_neTensor(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ne not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_ne_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_neValueT(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_neValueT(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ne_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_ne_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_neTensorT(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
            THLongTensor_neTensorT(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ne_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_min_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cmin(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cmin(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_min_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_min(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cmin(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cmin(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_min not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_min(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THFloatTensor_minall(self_)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            return at::scalar_tensor(convert<int64_t>(THLongTensor_minall(self_)), options(ScalarType::Long));
            break;
        }
        default:
            AT_ERROR("_th_min not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::_th_min_out(Tensor & min, Tensor & min_indices, const Tensor & self, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto min_ = checked_tensor_unwrap(min,"min",0, false, Backend::CPU, ScalarType::Float);
            auto min_indices_ = checked_tensor_unwrap(min_indices,"min_indices",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_min(min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Long: {
            auto min_ = checked_tensor_unwrap(min,"min",0, false, Backend::CPU, ScalarType::Long);
            auto min_indices_ = checked_tensor_unwrap(min_indices,"min_indices",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_min(min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        default:
            AT_ERROR("_th_min_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_th_min(const Tensor & self, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CPUTensorId()).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_min(min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Long: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CPUTensorId()).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_min(min_, min_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            min_->maybe_zero_dim(maybe_scalar);
            min_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        default:
            AT_ERROR("_th_min not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_max_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cmax(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cmax(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_max_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_max(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cmax(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cmax(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_max not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_max(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THFloatTensor_maxall(self_)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            return at::scalar_tensor(convert<int64_t>(THLongTensor_maxall(self_)), options(ScalarType::Long));
            break;
        }
        default:
            AT_ERROR("_th_max not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::_th_max_out(Tensor & max, Tensor & max_indices, const Tensor & self, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto max_ = checked_tensor_unwrap(max,"max",0, false, Backend::CPU, ScalarType::Float);
            auto max_indices_ = checked_tensor_unwrap(max_indices,"max_indices",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_max(max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Long: {
            auto max_ = checked_tensor_unwrap(max,"max",0, false, Backend::CPU, ScalarType::Long);
            auto max_indices_ = checked_tensor_unwrap(max_indices,"max_indices",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_max(max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        default:
            AT_ERROR("_th_max_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_th_max(const Tensor & self, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CPUTensorId()).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_max(max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Long: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CPUTensorId()).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_max(max_, max_indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0 || (keepdim == false && self_->dim() == 1);
            max_->maybe_zero_dim(maybe_scalar);
            max_indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        default:
            AT_ERROR("_th_max not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::_th_mode_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CPU, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_mode(values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CPU, ScalarType::Long);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_mode(values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_mode_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_th_mode(const Tensor & self, int64_t dim, bool keepdim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CPUTensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_mode(values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CPUTensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_mode(values_, indices_, self_, dim, keepdim);
            bool maybe_scalar = self_->dim() == 0|| (keepdim == false && self_->dim() == 1);
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_mode not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::_th_sort_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool descending) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CPU, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_sort(values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CPU, ScalarType::Long);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_sort(values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_sort_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_th_sort(const Tensor & self, int64_t dim, bool descending) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CPUTensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_sort(values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CPUTensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_sort(values_, indices_, self_, dim, descending);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_sort not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::_th_topk_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CPU, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_topk(values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = checked_tensor_unwrap(values,"values",0, false, Backend::CPU, ScalarType::Long);
            auto indices_ = checked_tensor_unwrap(indices,"indices",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_topk(values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_topk_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_th_topk(const Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CPUTensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_topk(values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CPUTensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_topk(values_, indices_, self_, k, dim, largest, sorted);
            bool maybe_scalar = self_->dim() == 0;
            values_->maybe_zero_dim(maybe_scalar);
            indices_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_topk not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_abs_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_abs_out is not implemented for type ", toString());
}
Tensor Type::_th_abs(const Tensor & self) const {
    AT_ERROR("_th_abs is not implemented for type ", toString());
}
Tensor & Type::_th_sigmoid_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_sigmoid_out is not implemented for type ", toString());
}
Tensor Type::_th_sigmoid(const Tensor & self) const {
    AT_ERROR("_th_sigmoid is not implemented for type ", toString());
}
Tensor & Type::_th_log_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_log_out is not implemented for type ", toString());
}
Tensor Type::_th_log(const Tensor & self) const {
    AT_ERROR("_th_log is not implemented for type ", toString());
}
Tensor & Type::_th_log10_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_log10_out is not implemented for type ", toString());
}
Tensor Type::_th_log10(const Tensor & self) const {
    AT_ERROR("_th_log10 is not implemented for type ", toString());
}
Tensor & Type::_th_log1p_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_log1p_out is not implemented for type ", toString());
}
Tensor Type::_th_log1p(const Tensor & self) const {
    AT_ERROR("_th_log1p is not implemented for type ", toString());
}
Tensor & Type::_th_log2_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_log2_out is not implemented for type ", toString());
}
Tensor Type::_th_log2(const Tensor & self) const {
    AT_ERROR("_th_log2 is not implemented for type ", toString());
}
Tensor & Type::_th_lgamma_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_lgamma(result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lgamma_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_lgamma(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_lgamma(result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lgamma not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_lgamma_(Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_lgamma(self_, self_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_lgamma_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_digamma_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_digamma(result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_digamma_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_digamma(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_digamma(result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_digamma not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_digamma_(Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_digamma(self_, self_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_digamma_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_polygamma_out(Tensor & result, int64_t n, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_polygamma(result_, n, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_polygamma_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_polygamma(int64_t n, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_polygamma(result_, n, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_polygamma not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_polygamma_(Tensor & self, int64_t n) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_polygamma(self_, n, self_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_polygamma_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_exp_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_exp_out is not implemented for type ", toString());
}
Tensor Type::_th_exp(const Tensor & self) const {
    AT_ERROR("_th_exp is not implemented for type ", toString());
}
Tensor & Type::_th_expm1_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_expm1_out is not implemented for type ", toString());
}
Tensor Type::_th_expm1(const Tensor & self) const {
    AT_ERROR("_th_expm1 is not implemented for type ", toString());
}
Tensor & Type::_th_cos_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_cos_out is not implemented for type ", toString());
}
Tensor Type::_th_cos(const Tensor & self) const {
    AT_ERROR("_th_cos is not implemented for type ", toString());
}
Tensor & Type::_th_acos_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_acos_out is not implemented for type ", toString());
}
Tensor Type::_th_acos(const Tensor & self) const {
    AT_ERROR("_th_acos is not implemented for type ", toString());
}
Tensor & Type::_th_cosh_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cosh(result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cosh_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_cosh(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cosh(result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cosh not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_sin_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_sin_out is not implemented for type ", toString());
}
Tensor Type::_th_sin(const Tensor & self) const {
    AT_ERROR("_th_sin is not implemented for type ", toString());
}
Tensor & Type::_th_asin_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_asin_out is not implemented for type ", toString());
}
Tensor Type::_th_asin(const Tensor & self) const {
    AT_ERROR("_th_asin is not implemented for type ", toString());
}
Tensor & Type::_th_sinh_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_sinh(result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sinh_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_sinh(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_sinh(result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sinh not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_tan_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_tan_out is not implemented for type ", toString());
}
Tensor Type::_th_tan(const Tensor & self) const {
    AT_ERROR("_th_tan is not implemented for type ", toString());
}
Tensor & Type::_th_atan_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_atan_out is not implemented for type ", toString());
}
Tensor Type::_th_atan(const Tensor & self) const {
    AT_ERROR("_th_atan is not implemented for type ", toString());
}
Tensor & Type::_th_tanh_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_tanh_out is not implemented for type ", toString());
}
Tensor Type::_th_tanh(const Tensor & self) const {
    AT_ERROR("_th_tanh is not implemented for type ", toString());
}
Tensor & Type::_th_erf_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_erf_out is not implemented for type ", toString());
}
Tensor Type::_th_erf(const Tensor & self) const {
    AT_ERROR("_th_erf is not implemented for type ", toString());
}
Tensor & Type::_th_erfc_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_erfc_out is not implemented for type ", toString());
}
Tensor Type::_th_erfc(const Tensor & self) const {
    AT_ERROR("_th_erfc is not implemented for type ", toString());
}
Tensor & Type::_th_erfinv_(Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_erfinv(self_, self_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_erfinv_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_erfinv_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_erfinv(result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_erfinv_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_erfinv(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_erfinv(result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_erfinv not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_sqrt_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_sqrt_out is not implemented for type ", toString());
}
Tensor Type::_th_sqrt(const Tensor & self) const {
    AT_ERROR("_th_sqrt is not implemented for type ", toString());
}
Tensor & Type::_th_rsqrt_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_rsqrt_out is not implemented for type ", toString());
}
Tensor Type::_th_rsqrt(const Tensor & self) const {
    AT_ERROR("_th_rsqrt is not implemented for type ", toString());
}
Tensor & Type::_th_ceil_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_ceil_out is not implemented for type ", toString());
}
Tensor Type::_th_ceil(const Tensor & self) const {
    AT_ERROR("_th_ceil is not implemented for type ", toString());
}
Tensor & Type::_th_floor_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_floor_out is not implemented for type ", toString());
}
Tensor Type::_th_floor(const Tensor & self) const {
    AT_ERROR("_th_floor is not implemented for type ", toString());
}
Tensor & Type::_th_round_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_round_out is not implemented for type ", toString());
}
Tensor Type::_th_round(const Tensor & self) const {
    AT_ERROR("_th_round is not implemented for type ", toString());
}
Tensor & Type::_th_trunc_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_trunc_out is not implemented for type ", toString());
}
Tensor Type::_th_trunc(const Tensor & self) const {
    AT_ERROR("_th_trunc is not implemented for type ", toString());
}
Tensor & Type::_th_frac_(Tensor & self) const {
    AT_ERROR("_th_frac_ is not implemented for type ", toString());
}
Tensor & Type::_th_frac_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_frac_out is not implemented for type ", toString());
}
Tensor Type::_th_frac(const Tensor & self) const {
    AT_ERROR("_th_frac is not implemented for type ", toString());
}
Tensor & Type::_th_var_out(Tensor & result, const Tensor & self, int64_t dim, bool unbiased, bool keepdim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_var(result_, self_, dim, (unbiased) ? 0 : 1, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        default:
            AT_ERROR("_th_var_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_var(const Tensor & self, int64_t dim, bool unbiased, bool keepdim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_var(result_, self_, dim, (unbiased) ? 0 : 1, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        default:
            AT_ERROR("_th_var not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_var(const Tensor & self, bool unbiased) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THFloatTensor_varall(self_, (unbiased) ? 0 : 1)), options(ScalarType::Float));
            break;
        }
        default:
            AT_ERROR("_th_var not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_std_out(Tensor & result, const Tensor & self, int64_t dim, bool unbiased, bool keepdim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_std(result_, self_, dim, (unbiased) ? 0 : 1, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        default:
            AT_ERROR("_th_std_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_std(const Tensor & self, int64_t dim, bool unbiased, bool keepdim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_std(result_, self_, dim, (unbiased) ? 0 : 1, keepdim);
            result_->maybe_zero_dim(self_->dim() == 0 || (keepdim == false && self_->dim() == 1));
            return result;
            break;
        }
        default:
            AT_ERROR("_th_std not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_std(const Tensor & self, bool unbiased) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THFloatTensor_stdall(self_, (unbiased) ? 0 : 1)), options(ScalarType::Float));
            break;
        }
        default:
            AT_ERROR("_th_std not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_renorm_out(Tensor & result, const Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto p_ = p.toFloat();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toFloat();
            THFloatTensor_renorm(result_, self_, p_, dim, maxnorm_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_renorm_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_renorm(const Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto p_ = p.toFloat();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toFloat();
            THFloatTensor_renorm(result_, self_, p_, dim, maxnorm_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_renorm not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_renorm_(Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto p_ = p.toFloat();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toFloat();
            THFloatTensor_renorm(self_, self_, p_, dim, maxnorm_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_renorm_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_dist(const Tensor & self, const Tensor & other, Scalar p) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            auto p_ = p.toFloat();
            return at::scalar_tensor(convert<float>(THFloatTensor_dist(self_, other_, p_)), options(ScalarType::Float));
            break;
        }
        default:
            AT_ERROR("_th_dist not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_reciprocal_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_reciprocal_out is not implemented for type ", toString());
}
Tensor Type::_th_reciprocal(const Tensor & self) const {
    AT_ERROR("_th_reciprocal is not implemented for type ", toString());
}
Tensor & Type::_th_reciprocal_(Tensor & self) const {
    AT_ERROR("_th_reciprocal_ is not implemented for type ", toString());
}
Tensor & Type::_th_neg_out(Tensor & result, const Tensor & self) const {
    AT_ERROR("_th_neg_out is not implemented for type ", toString());
}
Tensor Type::_th_neg(const Tensor & self) const {
    AT_ERROR("_th_neg is not implemented for type ", toString());
}
Tensor & Type::_th_neg_(Tensor & self) const {
    AT_ERROR("_th_neg_ is not implemented for type ", toString());
}
Tensor & Type::s__th_atan2_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_atan2(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_atan2_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_atan2(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_atan2(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_atan2 not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_atan2_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_atan2(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_atan2_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_pow_out(Tensor & result, const Tensor & self, Scalar exponent) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto exponent_ = exponent.toFloat();
            THFloatTensor_pow(result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto exponent_ = exponent.toLong();
            THLongTensor_pow(result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_pow_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_pow(const Tensor & self, Scalar exponent) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto exponent_ = exponent.toFloat();
            THFloatTensor_pow(result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto exponent_ = exponent.toLong();
            THLongTensor_pow(result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_pow not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_pow_out(Tensor & result, const Tensor & self, const Tensor & exponent) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cpow(result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cpow(result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_pow_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_pow(const Tensor & self, const Tensor & exponent) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cpow(result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cpow(result_, self_, exponent_);
            result_->maybe_zero_dim(self_->dim() == 0 && exponent_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_pow not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_pow_out(Tensor & result, Scalar self, const Tensor & exponent) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(result);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = self.toFloat();
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_tpow(result_, self_, exponent_);
            result_->maybe_zero_dim(exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = self.toLong();
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_tpow(result_, self_, exponent_);
            result_->maybe_zero_dim(exponent_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_pow_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_pow(Scalar self, const Tensor & exponent) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(exponent);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = self.toFloat();
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_tpow(result_, self_, exponent_);
            result_->maybe_zero_dim(exponent_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = self.toLong();
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_tpow(result_, self_, exponent_);
            result_->maybe_zero_dim(exponent_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_pow not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_pow_(Tensor & self, Scalar exponent) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto exponent_ = exponent.toFloat();
            THFloatTensor_pow(self_, self_, exponent_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto exponent_ = exponent.toLong();
            THLongTensor_pow(self_, self_, exponent_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_pow_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_pow_(Tensor & self, const Tensor & exponent) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cpow(self_, self_, exponent_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto exponent_ = checked_tensor_unwrap(exponent,"exponent",3, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cpow(self_, self_, exponent_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_pow_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_histc_out(Tensor & result, const Tensor & self, int64_t bins, Scalar min, Scalar max) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto min_ = min.toFloat();
            auto max_ = max.toFloat();
            THFloatTensor_histc(result_, self_, bins, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_histc_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_histc(const Tensor & self, int64_t bins, Scalar min, Scalar max) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto min_ = min.toFloat();
            auto max_ = max.toFloat();
            THFloatTensor_histc(result_, self_, bins, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_histc not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_zero_(Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_zero(self_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            THLongTensor_zero(self_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_zero_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_cumsum_out(Tensor & result, const Tensor & self, int64_t dim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_cumsum(result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_cumsum(result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cumsum_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_cumsum(const Tensor & self, int64_t dim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_cumsum(result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_cumsum(result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cumsum not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_cumprod_out(Tensor & result, const Tensor & self, int64_t dim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_cumprod(result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_cumprod(result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cumprod_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_cumprod(const Tensor & self, int64_t dim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_cumprod(result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_cumprod(result_, self_, dim);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cumprod not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_sign_out(Tensor & result, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_sign(result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            THLongTensor_sign(result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sign_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_sign(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_sign(result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            THLongTensor_sign(result_, self_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_sign not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_sign_(Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_sign(self_, self_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            THLongTensor_sign(self_, self_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_sign_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_trace(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THFloatTensor_trace(self_)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            return at::scalar_tensor(convert<int64_t>(THLongTensor_trace(self_)), options(ScalarType::Long));
            break;
        }
        default:
            AT_ERROR("_th_trace not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_fmod_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_fmod(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_fmod(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_fmod_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_fmod(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_fmod(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_fmod(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_fmod not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_fmod_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cfmod(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cfmod(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_fmod_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_fmod(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cfmod(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cfmod(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_fmod not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_fmod_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_fmod(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_fmod(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_fmod_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_fmod_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cfmod(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cfmod(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_fmod_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_remainder_out(Tensor & result, const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_remainder(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_remainder(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_remainder_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_remainder(const Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_remainder(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_remainder(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_remainder not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_remainder_out(Tensor & result, const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cremainder(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cremainder(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_remainder_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_remainder(const Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cremainder(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cremainder(result_, self_, other_);
            result_->maybe_zero_dim(self_->dim() == 0 && other_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_remainder not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_remainder_(Tensor & self, Scalar other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_remainder(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_remainder(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_remainder_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_remainder_(Tensor & self, const Tensor & other) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_cremainder(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto other_ = checked_tensor_unwrap(other,"other",3, false, Backend::CPU, ScalarType::Long);
            THLongTensor_cremainder(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_remainder_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_clamp_out(Tensor & result, const Tensor & self, Scalar min, Scalar max) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto min_ = min.toFloat();
            auto max_ = max.toFloat();
            THFloatTensor_clamp(result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto min_ = min.toLong();
            auto max_ = max.toLong();
            THLongTensor_clamp(result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_clamp_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_clamp(const Tensor & self, Scalar min, Scalar max) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto min_ = min.toFloat();
            auto max_ = max.toFloat();
            THFloatTensor_clamp(result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto min_ = min.toLong();
            auto max_ = max.toLong();
            THLongTensor_clamp(result_, self_, min_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_clamp not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_clamp_min_out(Tensor & result, const Tensor & self, Scalar min) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto min_ = min.toFloat();
            THFloatTensor_cmaxValue(result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto min_ = min.toLong();
            THLongTensor_cmaxValue(result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_clamp_min_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_clamp_min(const Tensor & self, Scalar min) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto min_ = min.toFloat();
            THFloatTensor_cmaxValue(result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto min_ = min.toLong();
            THLongTensor_cmaxValue(result_, self_, min_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_clamp_min not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_clamp_max_out(Tensor & result, const Tensor & self, Scalar max) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto max_ = max.toFloat();
            THFloatTensor_cminValue(result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto max_ = max.toLong();
            THLongTensor_cminValue(result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_clamp_max_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_clamp_max(const Tensor & self, Scalar max) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto max_ = max.toFloat();
            THFloatTensor_cminValue(result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto max_ = max.toLong();
            THLongTensor_cminValue(result_, self_, max_);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_clamp_max not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_dot(const Tensor & self, const Tensor & tensor) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto tensor_ = checked_tensor_unwrap(tensor,"tensor",2, false, Backend::CPU, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THFloatTensor_dot(self_, tensor_)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto tensor_ = checked_tensor_unwrap(tensor,"tensor",2, false, Backend::CPU, ScalarType::Long);
            return at::scalar_tensor(convert<int64_t>(THLongTensor_dot(self_, tensor_)), options(ScalarType::Long));
            break;
        }
        default:
            AT_ERROR("_th_dot not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_cross_kernel_out(Tensor & result, const Tensor & self, const Tensor & other, int64_t dim) const {
    AT_ERROR("_th_cross_kernel_out is not implemented for type ", toString());
}
Tensor Type::_th_cross_kernel(const Tensor & self, const Tensor & other, int64_t dim) const {
    AT_ERROR("_th_cross_kernel is not implemented for type ", toString());
}
Tensor & Type::_th_diag_out(Tensor & result, const Tensor & self, int64_t diagonal) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THFloatTensor_diag(result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THLongTensor_diag(result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_diag_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_diag(const Tensor & self, int64_t diagonal) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THFloatTensor_diag(result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            if (self_->dim() == 0) {
              throw std::runtime_error("Input must be 1-d or 2-d");
            }
            THLongTensor_diag(result_, self_, diagonal);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_diag not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_addmm_out(Tensor & result, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto alpha_ = alpha.toFloat();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CPU, ScalarType::Float);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addmm(result_, beta_, self_, alpha_, mat1_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Long);
            auto alpha_ = alpha.toLong();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CPU, ScalarType::Long);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addmm(result_, beta_, self_, alpha_, mat1_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addmm_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toFloat();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto alpha_ = alpha.toFloat();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CPU, ScalarType::Float);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addmm(result_, beta_, self_, alpha_, mat1_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toLong();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Long);
            auto alpha_ = alpha.toLong();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",4, false, Backend::CPU, ScalarType::Long);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",5, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addmm(result_, beta_, self_, alpha_, mat1_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat1_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addmm not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",5, false, Backend::CPU, ScalarType::Float);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",6, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addmm(self_, beta_, self_, alpha_, mat1_, mat2_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            auto mat1_ = checked_tensor_unwrap(mat1,"mat1",5, false, Backend::CPU, ScalarType::Long);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",6, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addmm(self_, beta_, self_, alpha_, mat1_, mat2_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_addmm_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_addmv_out(Tensor & result, const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto alpha_ = alpha.toFloat();
            auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CPU, ScalarType::Float);
            auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addmv(result_, beta_, self_, alpha_, mat_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Long);
            auto alpha_ = alpha.toLong();
            auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CPU, ScalarType::Long);
            auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addmv(result_, beta_, self_, alpha_, mat_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addmv_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_addmv(const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toFloat();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto alpha_ = alpha.toFloat();
            auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CPU, ScalarType::Float);
            auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addmv(result_, beta_, self_, alpha_, mat_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toLong();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Long);
            auto alpha_ = alpha.toLong();
            auto mat_ = checked_tensor_unwrap(mat,"mat",4, false, Backend::CPU, ScalarType::Long);
            auto vec_ = checked_tensor_unwrap(vec,"vec",5, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addmv(result_, beta_, self_, alpha_, mat_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addmv not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_addmv_(Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            auto mat_ = checked_tensor_unwrap(mat,"mat",5, false, Backend::CPU, ScalarType::Float);
            auto vec_ = checked_tensor_unwrap(vec,"vec",6, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addmv(self_, beta_, self_, alpha_, mat_, vec_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            auto mat_ = checked_tensor_unwrap(mat,"mat",5, false, Backend::CPU, ScalarType::Long);
            auto vec_ = checked_tensor_unwrap(vec,"vec",6, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addmv(self_, beta_, self_, alpha_, mat_, vec_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_addmv_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_addr_out(Tensor & result, const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto alpha_ = alpha.toFloat();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CPU, ScalarType::Float);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addr(result_, beta_, self_, alpha_, vec1_, vec2_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Long);
            auto alpha_ = alpha.toLong();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CPU, ScalarType::Long);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addr(result_, beta_, self_, alpha_, vec1_, vec2_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addr_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_addr(const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toFloat();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto alpha_ = alpha.toFloat();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CPU, ScalarType::Float);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addr(result_, beta_, self_, alpha_, vec1_, vec2_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toLong();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Long);
            auto alpha_ = alpha.toLong();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",4, false, Backend::CPU, ScalarType::Long);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",5, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addr(result_, beta_, self_, alpha_, vec1_, vec2_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec1_->dim() == 0 && vec2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addr not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_addr_(Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",5, false, Backend::CPU, ScalarType::Float);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",6, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addr(self_, beta_, self_, alpha_, vec1_, vec2_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            auto vec1_ = checked_tensor_unwrap(vec1,"vec1",5, false, Backend::CPU, ScalarType::Long);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",6, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addr(self_, beta_, self_, alpha_, vec1_, vec2_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_addr_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_ger_out(Tensor & result, const Tensor & self, const Tensor & vec2) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addr(result_, float(0), result_, float(1), self_, vec2_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addr(result_, int64_t(0), result_, int64_t(1), self_, vec2_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ger_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_ger(const Tensor & self, const Tensor & vec2) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addr(result_, float(0), result_, float(1), self_, vec2_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto vec2_ = checked_tensor_unwrap(vec2,"vec2",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addr(result_, int64_t(0), result_, int64_t(1), self_, vec2_);
            result_->maybe_zero_dim(false);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ger not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_mv_out(Tensor & result, const Tensor & self, const Tensor & vec) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            result.resize_({ self.size(0) });
            result.zero_();
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addmv(result_, float(0), result_, float(1), self_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            result.resize_({ self.size(0) });
            result.zero_();
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addmv(result_, int64_t(0), result_, int64_t(1), self_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_mv_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_mv(const Tensor & self, const Tensor & vec) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            result.zero_();
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addmv(result_, float(0), result_, float(1), self_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            result.zero_();
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto vec_ = checked_tensor_unwrap(vec,"vec",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addmv(result_, int64_t(0), result_, int64_t(1), self_, vec_);
            result_->maybe_zero_dim(self_->dim() == 0 && vec_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_mv not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_mm_out(Tensor & result, const Tensor & self, const Tensor & mat2) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            result.resize_({ self.size(0),mat2.size(1) });
            result.zero_();
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addmm(result_, float(0), result_, float(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            result.resize_({ self.size(0),mat2.size(1) });
            result.zero_();
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addmm(result_, int64_t(0), result_, int64_t(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_mm_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_mm(const Tensor & self, const Tensor & mat2) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            result.zero_();
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addmm(result_, float(0), result_, float(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            result.zero_();
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto mat2_ = checked_tensor_unwrap(mat2,"mat2",2, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addmm(result_, int64_t(0), result_, int64_t(1), self_, mat2_);
            result_->maybe_zero_dim(self_->dim() == 0 && mat2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_mm not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_bmm_out(Tensor & result, const Tensor & self, const Tensor & mat2) const {
    AT_ERROR("_th_bmm_out is not implemented for type ", toString());
}
Tensor Type::_th_bmm(const Tensor & self, const Tensor & mat2) const {
    AT_ERROR("_th_bmm is not implemented for type ", toString());
}
Tensor & Type::s__th_addbmm_out(Tensor & result, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto alpha_ = alpha.toFloat();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CPU, ScalarType::Float);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addbmm(result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Long);
            auto alpha_ = alpha.toLong();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CPU, ScalarType::Long);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addbmm(result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addbmm_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_addbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toFloat();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto alpha_ = alpha.toFloat();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CPU, ScalarType::Float);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addbmm(result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto beta_ = beta.toLong();
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Long);
            auto alpha_ = alpha.toLong();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",4, false, Backend::CPU, ScalarType::Long);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",5, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addbmm(result_, beta_, self_, alpha_, batch1_, batch2_);
            result_->maybe_zero_dim(self_->dim() == 0 && batch1_->dim() == 0 && batch2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addbmm not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_addbmm_(Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",5, false, Backend::CPU, ScalarType::Float);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",6, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addbmm(self_, beta_, self_, alpha_, batch1_, batch2_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            auto batch1_ = checked_tensor_unwrap(batch1,"batch1",5, false, Backend::CPU, ScalarType::Long);
            auto batch2_ = checked_tensor_unwrap(batch2,"batch2",6, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addbmm(self_, beta_, self_, alpha_, batch1_, batch2_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_addbmm_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_baddbmm_out(Tensor & result, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    AT_ERROR("s__th_baddbmm_out is not implemented for type ", toString());
}
Tensor Type::s__th_baddbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    AT_ERROR("s__th_baddbmm is not implemented for type ", toString());
}
Tensor & Type::s__th_addcmul_out(Tensor & result, const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto value_ = value.toFloat();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CPU, ScalarType::Float);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addcmul(result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto value_ = value.toLong();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CPU, ScalarType::Long);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addcmul(result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addcmul_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_addcmul(const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto value_ = value.toFloat();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CPU, ScalarType::Float);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addcmul(result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto value_ = value.toLong();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CPU, ScalarType::Long);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addcmul(result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addcmul not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_addcmul_(Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto value_ = value.toFloat();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CPU, ScalarType::Float);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addcmul(self_, self_, value_, tensor1_, tensor2_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto value_ = value.toLong();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CPU, ScalarType::Long);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addcmul(self_, self_, value_, tensor1_, tensor2_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_addcmul_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_addcdiv_out(Tensor & result, const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto value_ = value.toFloat();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CPU, ScalarType::Float);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addcdiv(result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto value_ = value.toLong();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CPU, ScalarType::Long);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addcdiv(result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addcdiv_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s__th_addcdiv(const Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto value_ = value.toFloat();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CPU, ScalarType::Float);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addcdiv(result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto value_ = value.toLong();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",3, false, Backend::CPU, ScalarType::Long);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",4, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addcdiv(result_, self_, value_, tensor1_, tensor2_);
            result_->maybe_zero_dim(self_->dim() == 0 && tensor1_->dim() == 0 && tensor2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addcdiv not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s__th_addcdiv_(Tensor & self, const Tensor & tensor1, const Tensor & tensor2, Scalar value) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto value_ = value.toFloat();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CPU, ScalarType::Float);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_addcdiv(self_, self_, value_, tensor1_, tensor2_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto value_ = value.toLong();
            auto tensor1_ = checked_tensor_unwrap(tensor1,"tensor1",4, false, Backend::CPU, ScalarType::Long);
            auto tensor2_ = checked_tensor_unwrap(tensor2,"tensor2",5, false, Backend::CPU, ScalarType::Long);
            THLongTensor_addcdiv(self_, self_, value_, tensor1_, tensor2_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_addcdiv_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::_th_gels_out(Tensor & res1, Tensor & res2, const Tensor & self, const Tensor & A) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto res1_ = checked_tensor_unwrap(res1,"res1",0, false, Backend::CPU, ScalarType::Float);
            auto res2_ = checked_tensor_unwrap(res2,"res2",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto A_ = checked_tensor_unwrap(A,"A",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_gels(res1_, res2_, self_, A_);
            bool maybe_scalar = self_->dim() == 0 && A_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_gels_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_th_gels(const Tensor & self, const Tensor & A) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto A_ = checked_tensor_unwrap(A,"A",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_gels(res1_, res2_, self_, A_);
            bool maybe_scalar = self_->dim() == 0 && A_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_gels not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::_th_symeig_out(Tensor & res1, Tensor & res2, const Tensor & self, bool eigenvectors, bool upper) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto res1_ = checked_tensor_unwrap(res1,"res1",0, false, Backend::CPU, ScalarType::Float);
            auto res2_ = checked_tensor_unwrap(res2,"res2",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_syev(res1_, res2_, self_, (eigenvectors) ? "V" : "N", (upper) ? "U" : "L");
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_symeig_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_th_symeig(const Tensor & self, bool eigenvectors, bool upper) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_syev(res1_, res2_, self_, (eigenvectors) ? "V" : "N", (upper) ? "U" : "L");
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_symeig not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::_th_eig_out(Tensor & res1, Tensor & res2, const Tensor & self, bool eigenvectors) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto res1_ = checked_tensor_unwrap(res1,"res1",0, false, Backend::CPU, ScalarType::Float);
            auto res2_ = checked_tensor_unwrap(res2,"res2",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_geev(res1_, res2_, self_, (eigenvectors) ? "V" : "N");
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_eig_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_th_eig(const Tensor & self, bool eigenvectors) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_geev(res1_, res2_, self_, (eigenvectors) ? "V" : "N");
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_eig not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::_th_svd_out(Tensor & res1, Tensor & res2, Tensor & res3, const Tensor & self, bool some, bool compute_uv) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto res1_ = checked_tensor_unwrap(res1,"res1",0, false, Backend::CPU, ScalarType::Float);
            auto res2_ = checked_tensor_unwrap(res2,"res2",0, false, Backend::CPU, ScalarType::Float);
            auto res3_ = checked_tensor_unwrap(res3,"res3",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_gesdd(res1_, res2_, res3_, self_, (some) ? "S" : "A", (compute_uv) ? "S" : "N");
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            res3_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(res1, res2, res3);
            break;
        }
        default:
            AT_ERROR("_th_svd_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> Type::_th_svd(const Tensor & self, bool some, bool compute_uv) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto res3_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto res3 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res3_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_gesdd(res1_, res2_, res3_, self_, (some) ? "S" : "A", (compute_uv) ? "S" : "N");
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            res3_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(res1, res2, res3);
            break;
        }
        default:
            AT_ERROR("_th_svd not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_getri_single_out(Tensor & output, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_getri(output_, self_);
            output_->maybe_zero_dim(self_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_getri_single_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_getri_single(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_getri(output_, self_);
            output_->maybe_zero_dim(self_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_getri_single not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_potri_out(Tensor & output, const Tensor & self, bool upper) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_potri(output_, self_, (upper) ? "U" : "L");
            output_->maybe_zero_dim(self_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_potri_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_potri(const Tensor & self, bool upper) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_potri(output_, self_, (upper) ? "U" : "L");
            output_->maybe_zero_dim(self_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_potri not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::_th_pstrf_out(Tensor & res1, Tensor & res2, const Tensor & self, bool upper, Scalar tol) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto res1_ = checked_tensor_unwrap(res1,"res1",0, false, Backend::CPU, ScalarType::Float);
            auto res2_ = checked_tensor_unwrap(res2,"res2",0, false, Backend::CPU, ScalarType::Int);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto tol_ = tol.toFloat();
            THFloatTensor_pstrf(res1_, res2_, self_, (upper) ? "U" : "L", tol_);
            res2 -= 1;  // LAPACK returns 1-indexed pivots
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_pstrf_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_th_pstrf(const Tensor & self, bool upper, Scalar tol) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Int), 0, allocator(), true),CPUTensorId()).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto tol_ = tol.toFloat();
            THFloatTensor_pstrf(res1_, res2_, self_, (upper) ? "U" : "L", tol_);
            res2 -= 1;  // LAPACK returns 1-indexed pivots
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_pstrf not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::_th_qr_out(Tensor & res1, Tensor & res2, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto res1_ = checked_tensor_unwrap(res1,"res1",0, false, Backend::CPU, ScalarType::Float);
            auto res2_ = checked_tensor_unwrap(res2,"res2",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_qr(res1_, res2_, self_);
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_qr_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_th_qr(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_qr(res1_, res2_, self_);
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_qr not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::_th_geqrf_out(Tensor & res1, Tensor & res2, const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto res1_ = checked_tensor_unwrap(res1,"res1",0, false, Backend::CPU, ScalarType::Float);
            auto res2_ = checked_tensor_unwrap(res2,"res2",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_geqrf(res1_, res2_, self_);
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_geqrf_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_th_geqrf(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_geqrf(res1_, res2_, self_);
            bool maybe_scalar = self_->dim() == 0;
            res1_->maybe_zero_dim(maybe_scalar);
            res2_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_geqrf not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_orgqr_out(Tensor & result, const Tensor & self, const Tensor & input2) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto input2_ = checked_tensor_unwrap(input2,"input2",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_orgqr(result_, self_, input2_);
            result_->maybe_zero_dim(self_->dim() == 0 && input2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_orgqr_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_orgqr(const Tensor & self, const Tensor & input2) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto input2_ = checked_tensor_unwrap(input2,"input2",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_orgqr(result_, self_, input2_);
            result_->maybe_zero_dim(self_->dim() == 0 && input2_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_orgqr not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_ormqr_out(Tensor & result, const Tensor & self, const Tensor & input2, const Tensor & input3, bool left, bool transpose) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto input2_ = checked_tensor_unwrap(input2,"input2",2, false, Backend::CPU, ScalarType::Float);
            auto input3_ = checked_tensor_unwrap(input3,"input3",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_ormqr(result_, self_, input2_, input3_, (left) ? "L" : "R", (transpose) ? "T" : "N");
            result_->maybe_zero_dim(self_->dim() == 0 && input2_->dim() == 0 && input3_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ormqr_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_ormqr(const Tensor & self, const Tensor & input2, const Tensor & input3, bool left, bool transpose) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto input2_ = checked_tensor_unwrap(input2,"input2",2, false, Backend::CPU, ScalarType::Float);
            auto input3_ = checked_tensor_unwrap(input3,"input3",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_ormqr(result_, self_, input2_, input3_, (left) ? "L" : "R", (transpose) ? "T" : "N");
            result_->maybe_zero_dim(self_->dim() == 0 && input2_->dim() == 0 && input3_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ormqr not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_btrisolve_out(Tensor & result, const Tensor & self, const Tensor & LU_data, const Tensor & LU_pivots) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto LU_data_ = checked_tensor_unwrap(LU_data,"LU_data",2, false, Backend::CPU, ScalarType::Float);
            auto LU_pivots_ = checked_tensor_unwrap(LU_pivots,"LU_pivots",3, false, Backend::CPU, ScalarType::Int);
            THFloatTensor_btrisolve(result_, self_, LU_data_, LU_pivots_);
            result_->maybe_zero_dim(self_->dim() == 0 && LU_data_->dim() == 0 && LU_pivots_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_btrisolve_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_btrisolve(const Tensor & self, const Tensor & LU_data, const Tensor & LU_pivots) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto LU_data_ = checked_tensor_unwrap(LU_data,"LU_data",2, false, Backend::CPU, ScalarType::Float);
            auto LU_pivots_ = checked_tensor_unwrap(LU_pivots,"LU_pivots",3, false, Backend::CPU, ScalarType::Int);
            THFloatTensor_btrisolve(result_, self_, LU_data_, LU_pivots_);
            result_->maybe_zero_dim(self_->dim() == 0 && LU_data_->dim() == 0 && LU_pivots_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_btrisolve not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_random_(Tensor & self, int64_t from, int64_t to, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            THFloatTensor_clampedRandom(self_, generator_->generator, from, to);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            THLongTensor_clampedRandom(self_, generator_->generator, from, to);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_random_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_random_(Tensor & self, int64_t to, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            THFloatTensor_cappedRandom(self_, generator_->generator, to);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            THLongTensor_cappedRandom(self_, generator_->generator, to);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_random_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_random_(Tensor & self, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            THFloatTensor_random(self_, generator_->generator);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            THLongTensor_random(self_, generator_->generator);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_random_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::_th_multinomial_alias_setup_out(Tensor & J, Tensor & q, const Tensor & probs) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(J);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto probs_ = checked_tensor_unwrap(probs,"probs",1, false, Backend::CPU, ScalarType::Float);
            auto J_ = checked_tensor_unwrap(J,"J",1, false, Backend::CPU, ScalarType::Long);
            auto q_ = checked_tensor_unwrap(q,"q",1, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_multinomialAliasSetup(probs_, J_, q_);
            bool maybe_scalar = probs_->dim() == 0;
            J_->maybe_zero_dim(maybe_scalar);
            q_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &>(J, q);
            break;
        }
        default:
            AT_ERROR("_th_multinomial_alias_setup_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_th_multinomial_alias_setup(const Tensor & probs) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(probs);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto probs_ = checked_tensor_unwrap(probs,"probs",1, false, Backend::CPU, ScalarType::Float);
            auto J_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CPUTensorId()).release();
            auto J = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(J_));
            auto q_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto q = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(q_));
            THFloatTensor_multinomialAliasSetup(probs_, J_, q_);
            bool maybe_scalar = probs_->dim() == 0;
            J_->maybe_zero_dim(maybe_scalar);
            q_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor>(J, q);
            break;
        }
        default:
            AT_ERROR("_th_multinomial_alias_setup not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_multinomial_alias_draw_out(Tensor & result, const Tensor & q, const Tensor & J, int64_t num_samples, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(result);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            auto q_ = checked_tensor_unwrap(q,"q",2, false, Backend::CPU, ScalarType::Float);
            auto J_ = checked_tensor_unwrap(J,"J",3, false, Backend::CPU, ScalarType::Long);
            THFloatTensor_multinomialAliasDraw(result_, generator_->generator, q_, J_, num_samples);
            result_->maybe_zero_dim(q_->dim() == 0 && J_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_multinomial_alias_draw_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_multinomial_alias_draw(const Tensor & q, const Tensor & J, int64_t num_samples, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(q);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            auto q_ = checked_tensor_unwrap(q,"q",2, false, Backend::CPU, ScalarType::Float);
            auto J_ = checked_tensor_unwrap(J,"J",3, false, Backend::CPU, ScalarType::Long);
            THFloatTensor_multinomialAliasDraw(result_, generator_->generator, q_, J_, num_samples);
            result_->maybe_zero_dim(q_->dim() == 0 && J_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_multinomial_alias_draw not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_multinomial_out(Tensor & result, const Tensor & self, int64_t num_samples, bool replacement, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = checked_tensor_unwrap(result,"result",0, false, Backend::CPU, ScalarType::Long);
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_multinomial(result_, generator_->generator, self_, num_samples, replacement);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_multinomial_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_multinomial(const Tensor & self, int64_t num_samples, bool replacement, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CPUTensorId()).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_multinomial(result_, generator_->generator, self_, num_samples, replacement);
            result_->maybe_zero_dim(self_->dim() == 0);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_multinomial not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_uniform_(Tensor & self, double from, double to, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            THFloatTensor_uniform(self_, generator_->generator, from, to);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_uniform_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_normal_out(Tensor & output, const Tensor & mean, double std, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(output);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CPU, ScalarType::Float);
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            auto mean_ = checked_tensor_unwrap(mean,"mean",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_normal_means(output_, generator_->generator, mean_, std);
            output_->maybe_zero_dim(mean_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_normal_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_normal(const Tensor & mean, double std, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(mean);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            auto mean_ = checked_tensor_unwrap(mean,"mean",2, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_normal_means(output_, generator_->generator, mean_, std);
            output_->maybe_zero_dim(mean_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_normal not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_normal_out(Tensor & output, double mean, const Tensor & std, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(output);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CPU, ScalarType::Float);
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            auto std_ = checked_tensor_unwrap(std,"std",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_normal_stddevs(output_, generator_->generator, mean, std_);
            output_->maybe_zero_dim(std_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_normal_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_normal(double mean, const Tensor & std, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(std);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            auto std_ = checked_tensor_unwrap(std,"std",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_normal_stddevs(output_, generator_->generator, mean, std_);
            output_->maybe_zero_dim(std_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_normal not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_normal_out(Tensor & output, const Tensor & mean, const Tensor & std, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(output);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CPU, ScalarType::Float);
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            auto mean_ = checked_tensor_unwrap(mean,"mean",2, false, Backend::CPU, ScalarType::Float);
            auto std_ = checked_tensor_unwrap(std,"std",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_normal_means_stddevs(output_, generator_->generator, mean_, std_);
            output_->maybe_zero_dim(mean_->dim() == 0 && std_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_normal_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_normal(const Tensor & mean, const Tensor & std, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(mean);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            auto mean_ = checked_tensor_unwrap(mean,"mean",2, false, Backend::CPU, ScalarType::Float);
            auto std_ = checked_tensor_unwrap(std,"std",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_normal_means_stddevs(output_, generator_->generator, mean_, std_);
            output_->maybe_zero_dim(mean_->dim() == 0 && std_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_normal not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_normal_(Tensor & self, double mean, double std, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            THFloatTensor_normal(self_, generator_->generator, mean, std);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_normal_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_cauchy_(Tensor & self, double median, double sigma, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            THFloatTensor_cauchy(self_, generator_->generator, median, sigma);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_cauchy_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_log_normal_(Tensor & self, double mean, double std, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            THFloatTensor_logNormal(self_, generator_->generator, mean, std);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_log_normal_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_exponential_(Tensor & self, double lambd, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            THFloatTensor_exponential(self_, generator_->generator, lambd);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_exponential_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_geometric_(Tensor & self, double p, Generator * generator) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            THFloatTensor_geometric(self_, generator_->generator, p);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            THLongTensor_geometric(self_, generator_->generator, p);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_geometric_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_dirichlet_grad_out(Tensor & output, const Tensor & x, const Tensor & alpha, const Tensor & total) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(output);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto output_ = checked_tensor_unwrap(output,"output",0, false, Backend::CPU, ScalarType::Float);
            auto x_ = checked_tensor_unwrap(x,"x",1, false, Backend::CPU, ScalarType::Float);
            auto alpha_ = checked_tensor_unwrap(alpha,"alpha",2, false, Backend::CPU, ScalarType::Float);
            auto total_ = checked_tensor_unwrap(total,"total",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_dirichlet_grad(output_, x_, alpha_, total_);
            output_->maybe_zero_dim(x_->dim() == 0 && alpha_->dim() == 0 && total_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_dirichlet_grad_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_dirichlet_grad(const Tensor & x, const Tensor & alpha, const Tensor & total) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(x);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto x_ = checked_tensor_unwrap(x,"x",1, false, Backend::CPU, ScalarType::Float);
            auto alpha_ = checked_tensor_unwrap(alpha,"alpha",2, false, Backend::CPU, ScalarType::Float);
            auto total_ = checked_tensor_unwrap(total,"total",3, false, Backend::CPU, ScalarType::Float);
            THFloatTensor_dirichlet_grad(output_, x_, alpha_, total_);
            output_->maybe_zero_dim(x_->dim() == 0 && alpha_->dim() == 0 && total_->dim() == 0);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_dirichlet_grad not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_alias(const Tensor & self) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THFloatTensor_newWithTensor(self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Long);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THLongTensor_newWithTensor(self_))->maybe_zero_dim(self_->dim() == 0)));
            break;
        }
        default:
            AT_ERROR("_th_alias not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_th_copy_ignoring_overlaps_(Tensor & self, const Tensor & src) const {
    AT_ERROR("_th_copy_ignoring_overlaps_ is not implemented for type ", toString());
}
Tensor & Type::_th_cat_out(Tensor & self, TensorList tensors, int64_t dim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",0, false, Backend::CPU, ScalarType::Float);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Float);
            THFloatTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_tensor_unwrap(self,"self",0, false, Backend::CPU, ScalarType::Long);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Long);
            THLongTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_cat_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_th_cat(TensorList tensors, int64_t dim) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(tensors);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Float);
            THFloatTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),CPUTensorId()).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Long);
            THLongTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_cat not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_binary_cross_entropy_forward_out(Tensor & output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CPU, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",4, false, Backend::CPU, ScalarType::Float);
            THNN_FloatBCECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_binary_cross_entropy_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_binary_cross_entropy_forward(const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CPU, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatBCECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_binary_cross_entropy_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_binary_cross_entropy_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CPU, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CPU, ScalarType::Float);
            THNN_FloatBCECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_binary_cross_entropy_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_binary_cross_entropy_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CPU, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatBCECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_binary_cross_entropy_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_l1_loss_forward_out(Tensor & output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CPU, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CPU, ScalarType::Float);
            THNN_FloatAbsCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_l1_loss_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_l1_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CPU, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatAbsCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_l1_loss_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_l1_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CPU, ScalarType::Float);
            THNN_FloatAbsCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_l1_loss_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_l1_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatAbsCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_l1_loss_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_mse_loss_forward_out(Tensor & output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CPU, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CPU, ScalarType::Float);
            THNN_FloatMSECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_mse_loss_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_mse_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CPU, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatMSECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_mse_loss_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_mse_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CPU, ScalarType::Float);
            THNN_FloatMSECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_mse_loss_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_mse_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatMSECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_mse_loss_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_multi_margin_loss_forward_out(Tensor & output, const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CPU, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight,"weight",5, true, Backend::CPU, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CPU, ScalarType::Float);
            THNN_FloatMultiMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0 || (reduction == Reduction::None && self_->dim() == 1));
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_multi_margin_loss_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_multi_margin_loss_forward(const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CPU, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight,"weight",5, true, Backend::CPU, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatMultiMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0 || (reduction == Reduction::None && self_->dim() == 1));
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_multi_margin_loss_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_multi_margin_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CPU, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight,"weight",6, true, Backend::CPU, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CPU, ScalarType::Float);
            THNN_FloatMultiMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_multi_margin_loss_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_multi_margin_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, Scalar p, Scalar margin, const Tensor & weight, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CPU, ScalarType::Long);
            auto p_ = p.toDouble();
            auto margin_ = margin.toDouble();
            auto weight_ = checked_tensor_unwrap(weight,"weight",6, true, Backend::CPU, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatMultiMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, p_, weight_ ? weight_ : NULL, margin_);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_multi_margin_loss_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::_thnn_multilabel_margin_loss_forward_out(Tensor & output, Tensor & is_target, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CPU, ScalarType::Long);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CPU, ScalarType::Float);
            auto is_target_ = checked_tensor_unwrap(is_target,"is_target",3, false, Backend::CPU, ScalarType::Float);
            THNN_FloatMultiLabelMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, is_target_, reduction);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            is_target_->maybe_zero_dim(target_->dim() == 0);
            return std::tuple<Tensor &, Tensor &>(output, is_target);
            break;
        }
        default:
            AT_ERROR("_thnn_multilabel_margin_loss_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_thnn_multilabel_margin_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CPU, ScalarType::Long);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto is_target_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto is_target = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(is_target_));
            THNN_FloatMultiLabelMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, is_target_, reduction);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            is_target_->maybe_zero_dim(target_->dim() == 0);
            return std::tuple<Tensor, Tensor>(output, is_target);
            break;
        }
        default:
            AT_ERROR("_thnn_multilabel_margin_loss_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_multilabel_margin_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction, const Tensor & is_target) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CPU, ScalarType::Long);
            auto is_target_ = checked_tensor_unwrap(is_target,"is_target",5, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CPU, ScalarType::Float);
            THNN_FloatMultiLabelMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, is_target_, reduction);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_multilabel_margin_loss_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_multilabel_margin_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction, const Tensor & is_target) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CPU, ScalarType::Long);
            auto is_target_ = checked_tensor_unwrap(is_target,"is_target",5, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatMultiLabelMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, is_target_, reduction);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_multilabel_margin_loss_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::_thnn_nll_loss_forward_out(Tensor & output, Tensor & total_weight, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CPU, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CPU, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CPU, ScalarType::Float);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",5, false, Backend::CPU, ScalarType::Float);
            THNN_FloatClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor &, Tensor &>(output, total_weight);
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_thnn_nll_loss_forward(const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CPU, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CPU, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto total_weight_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto total_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(total_weight_));
            THNN_FloatClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor, Tensor>(output, total_weight);
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_nll_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CPU, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CPU, ScalarType::Float);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",7, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CPU, ScalarType::Float);
            THNN_FloatClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_nll_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CPU, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CPU, ScalarType::Float);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",7, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::_thnn_nll_loss2d_forward_out(Tensor & output, Tensor & total_weight, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CPU, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CPU, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CPU, ScalarType::Float);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",5, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSpatialClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor &, Tensor &>(output, total_weight);
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss2d_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_thnn_nll_loss2d_forward(const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CPU, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, true, Backend::CPU, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto total_weight_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto total_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(total_weight_));
            THNN_FloatSpatialClassNLLCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            output_->maybe_zero_dim(reduction != Reduction::None || self_->dim() == 0);
            total_weight_->maybe_zero_dim(true);
            return std::tuple<Tensor, Tensor>(output, total_weight);
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss2d_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_nll_loss2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CPU, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CPU, ScalarType::Float);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",7, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSpatialClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss2d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_nll_loss2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction, int64_t ignore_index, const Tensor & total_weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CPU, ScalarType::Long);
            auto weight_ = checked_tensor_unwrap(weight,"weight",4, true, Backend::CPU, ScalarType::Float);
            auto total_weight_ = checked_tensor_unwrap(total_weight,"total_weight",7, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatSpatialClassNLLCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL, total_weight_, ignore_index);
            grad_input_->maybe_zero_dim(self_->dim() == 0);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_nll_loss2d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_smooth_l1_loss_forward_out(Tensor & output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CPU, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSmoothL1Criterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_smooth_l1_loss_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_smooth_l1_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CPU, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatSmoothL1Criterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_smooth_l1_loss_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_smooth_l1_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSmoothL1Criterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_smooth_l1_loss_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_smooth_l1_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatSmoothL1Criterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_smooth_l1_loss_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_soft_margin_loss_forward_out(Tensor & output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CPU, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSoftMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_soft_margin_loss_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_soft_margin_loss_forward(const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",2, false, Backend::CPU, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatSoftMarginCriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_soft_margin_loss_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_soft_margin_loss_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSoftMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_soft_margin_loss_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_soft_margin_loss_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto target_ = checked_tensor_unwrap(target,"target",3, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatSoftMarginCriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_soft_margin_loss_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_elu_forward_out(Tensor & output, const Tensor & self, Scalar alpha, Scalar scale, Scalar input_scale) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",4, false, Backend::CPU, ScalarType::Float);
            THNN_FloatELU_updateOutput(globalContext().getTHCState(), self_, output_, alpha_, scale_, input_scale_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_elu_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_elu_forward(const Tensor & self, Scalar alpha, Scalar scale, Scalar input_scale) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatELU_updateOutput(globalContext().getTHCState(), self_, output_, alpha_, scale_, input_scale_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_elu_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_elu_backward_out(Tensor & grad_input, const Tensor & grad_output, Scalar alpha, Scalar scale, Scalar input_scale, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CPU, ScalarType::Float);
            THNN_FloatELU_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_, alpha_, scale_, input_scale_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_elu_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_elu_backward(const Tensor & grad_output, Scalar alpha, Scalar scale, Scalar input_scale, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatELU_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_, alpha_, scale_, input_scale_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_elu_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_elu_forward_(Tensor & self, Scalar alpha, Scalar scale, Scalar input_scale) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto alpha_ = alpha.toDouble();
            auto scale_ = scale.toDouble();
            auto input_scale_ = input_scale.toDouble();
            THNN_FloatELU_updateOutput(globalContext().getTHCState(), self_, self_, alpha_, scale_, input_scale_, true);
            return self;
            break;
        }
        default:
            AT_ERROR("_thnn_elu_forward_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_glu_forward_out(Tensor & output, const Tensor & self, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CPU, ScalarType::Float);
            THNN_FloatGatedLinear_updateOutput(globalContext().getTHCState(), self_, output_, dim);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_glu_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_glu_forward(const Tensor & self, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatGatedLinear_updateOutput(globalContext().getTHCState(), self_, output_, dim);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_glu_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_glu_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CPU, ScalarType::Float);
            THNN_FloatGatedLinear_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, dim);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_glu_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_glu_backward(const Tensor & grad_output, const Tensor & self, int64_t dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatGatedLinear_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, dim);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_glu_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_hardtanh_forward_out(Tensor & output, const Tensor & self, Scalar min_val, Scalar max_val) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CPU, ScalarType::Float);
            THNN_FloatHardTanh_updateOutput(globalContext().getTHCState(), self_, output_, min_val_, max_val_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_hardtanh_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_hardtanh_forward(const Tensor & self, Scalar min_val, Scalar max_val) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatHardTanh_updateOutput(globalContext().getTHCState(), self_, output_, min_val_, max_val_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_hardtanh_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_hardtanh_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar min_val, Scalar max_val) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CPU, ScalarType::Float);
            THNN_FloatHardTanh_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, min_val_, max_val_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_hardtanh_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_hardtanh_backward(const Tensor & grad_output, const Tensor & self, Scalar min_val, Scalar max_val) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatHardTanh_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, min_val_, max_val_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_hardtanh_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_hardtanh_forward_(Tensor & self, Scalar min_val, Scalar max_val) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto min_val_ = min_val.toDouble();
            auto max_val_ = max_val.toDouble();
            THNN_FloatHardTanh_updateOutput(globalContext().getTHCState(), self_, self_, min_val_, max_val_, true);
            return self;
            break;
        }
        default:
            AT_ERROR("_thnn_hardtanh_forward_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_leaky_relu_forward_out(Tensor & output, const Tensor & self, Scalar negative_slope) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto negative_slope_ = negative_slope.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CPU, ScalarType::Float);
            THNN_FloatLeakyReLU_updateOutput(globalContext().getTHCState(), self_, output_, negative_slope_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_leaky_relu_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_leaky_relu_forward(const Tensor & self, Scalar negative_slope) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto negative_slope_ = negative_slope.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatLeakyReLU_updateOutput(globalContext().getTHCState(), self_, output_, negative_slope_, false);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_leaky_relu_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_leaky_relu_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar negative_slope) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto negative_slope_ = negative_slope.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CPU, ScalarType::Float);
            THNN_FloatLeakyReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, negative_slope_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_leaky_relu_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_leaky_relu_backward(const Tensor & grad_output, const Tensor & self, Scalar negative_slope) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto negative_slope_ = negative_slope.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatLeakyReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, negative_slope_, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_leaky_relu_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_leaky_relu_forward_(Tensor & self, Scalar negative_slope) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto negative_slope_ = negative_slope.toDouble();
            THNN_FloatLeakyReLU_updateOutput(globalContext().getTHCState(), self_, self_, negative_slope_, true);
            return self;
            break;
        }
        default:
            AT_ERROR("_thnn_leaky_relu_forward_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::_thnn_log_sigmoid_forward_out(Tensor & output, Tensor & buffer, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",1, false, Backend::CPU, ScalarType::Float);
            auto buffer_ = checked_tensor_unwrap(buffer,"buffer",1, false, Backend::CPU, ScalarType::Float);
            THNN_FloatLogSigmoid_updateOutput(globalContext().getTHCState(), self_, output_, buffer_);
            output_->maybe_zero_dim(false);
            buffer_->maybe_zero_dim(false);
            return std::tuple<Tensor &, Tensor &>(output, buffer);
            break;
        }
        default:
            AT_ERROR("_thnn_log_sigmoid_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_thnn_log_sigmoid_forward(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto buffer_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto buffer = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(buffer_));
            THNN_FloatLogSigmoid_updateOutput(globalContext().getTHCState(), self_, output_, buffer_);
            output_->maybe_zero_dim(false);
            buffer_->maybe_zero_dim(false);
            return std::tuple<Tensor, Tensor>(output, buffer);
            break;
        }
        default:
            AT_ERROR("_thnn_log_sigmoid_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_log_sigmoid_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & buffer) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto buffer_ = checked_tensor_unwrap(buffer,"buffer",3, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CPU, ScalarType::Float);
            THNN_FloatLogSigmoid_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, buffer_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_log_sigmoid_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_log_sigmoid_backward(const Tensor & grad_output, const Tensor & self, const Tensor & buffer) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto buffer_ = checked_tensor_unwrap(buffer,"buffer",3, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatLogSigmoid_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, buffer_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_log_sigmoid_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_rrelu_with_noise_forward_out(Tensor & output, const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto noise_ = checked_tensor_unwrap(noise,"noise",2, false, Backend::CPU, ScalarType::Float);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CPU, ScalarType::Float);
            THNN_FloatRReLU_updateOutput(globalContext().getTHCState(), self_, output_, noise_, lower_, upper_, training, false, generator_->generator);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_rrelu_with_noise_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_rrelu_with_noise_forward(const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto noise_ = checked_tensor_unwrap(noise,"noise",2, false, Backend::CPU, ScalarType::Float);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatRReLU_updateOutput(globalContext().getTHCState(), self_, output_, noise_, lower_, upper_, training, false, generator_->generator);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_rrelu_with_noise_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_rrelu_with_noise_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto noise_ = checked_tensor_unwrap(noise,"noise",3, false, Backend::CPU, ScalarType::Float);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",6, false, Backend::CPU, ScalarType::Float);
            THNN_FloatRReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, noise_, lower_, upper_, training, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_rrelu_with_noise_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_rrelu_with_noise_backward(const Tensor & grad_output, const Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto noise_ = checked_tensor_unwrap(noise,"noise",3, false, Backend::CPU, ScalarType::Float);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatRReLU_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, noise_, lower_, upper_, training, false);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_rrelu_with_noise_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_rrelu_with_noise_forward_(Tensor & self, const Tensor & noise, Scalar lower, Scalar upper, bool training, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto noise_ = checked_tensor_unwrap(noise,"noise",2, false, Backend::CPU, ScalarType::Float);
            auto lower_ = lower.toDouble();
            auto upper_ = upper.toDouble();
            auto generator_ = check_generator<CPUGenerator>(generator, &globalContext().defaultGenerator(device_type()));
            THNN_FloatRReLU_updateOutput(globalContext().getTHCState(), self_, self_, noise_, lower_, upper_, training, true, generator_->generator);
            return self;
            break;
        }
        default:
            AT_ERROR("_thnn_rrelu_with_noise_forward_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_softplus_forward_out(Tensor & output, const Tensor & self, Scalar beta, Scalar threshold) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSoftPlus_updateOutput(globalContext().getTHCState(), self_, output_, beta_, threshold_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_softplus_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_softplus_forward(const Tensor & self, Scalar beta, Scalar threshold) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatSoftPlus_updateOutput(globalContext().getTHCState(), self_, output_, beta_, threshold_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_softplus_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_softplus_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar beta, Scalar threshold, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSoftPlus_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_, beta_, threshold_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_softplus_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_softplus_backward(const Tensor & grad_output, const Tensor & self, Scalar beta, Scalar threshold, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatSoftPlus_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_, beta_, threshold_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_softplus_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_softshrink_forward_out(Tensor & output, const Tensor & self, Scalar lambd) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto lambd_ = lambd.toDouble();
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSoftShrink_updateOutput(globalContext().getTHCState(), self_, output_, lambd_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_softshrink_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_softshrink_forward(const Tensor & self, Scalar lambd) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto lambd_ = lambd.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatSoftShrink_updateOutput(globalContext().getTHCState(), self_, output_, lambd_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_softshrink_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_softshrink_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar lambd) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto lambd_ = lambd.toDouble();
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSoftShrink_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, lambd_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_softshrink_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_softshrink_backward(const Tensor & grad_output, const Tensor & self, Scalar lambd) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto lambd_ = lambd.toDouble();
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatSoftShrink_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, lambd_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_softshrink_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_adaptive_avg_pool3d_forward_out(Tensor & output, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CPU, ScalarType::Float);
            THNN_FloatVolumetricAdaptiveAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[2], output_size_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_adaptive_avg_pool3d_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_adaptive_avg_pool3d_forward(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatVolumetricAdaptiveAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[2], output_size_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_adaptive_avg_pool3d_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_adaptive_avg_pool3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",2, false, Backend::CPU, ScalarType::Float);
            THNN_FloatVolumetricAdaptiveAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_adaptive_avg_pool3d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_adaptive_avg_pool3d_backward(const Tensor & grad_output, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatVolumetricAdaptiveAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_adaptive_avg_pool3d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_avg_pool2d_forward_out(Tensor & output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<2>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSpatialAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], ceil_mode, count_include_pad);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_avg_pool2d_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_avg_pool2d_forward(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<2>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatSpatialAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], ceil_mode, count_include_pad);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_avg_pool2d_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_avg_pool2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<2>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSpatialAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], ceil_mode, count_include_pad);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_avg_pool2d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_avg_pool2d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<2>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatSpatialAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], ceil_mode, count_include_pad);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_avg_pool2d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_avg_pool3d_forward_out(Tensor & output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<3>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 4);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CPU, ScalarType::Float);
            THNN_FloatVolumetricAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], ceil_mode, count_include_pad);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_avg_pool3d_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_avg_pool3d_forward(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<3>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 4);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatVolumetricAveragePooling_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], ceil_mode, count_include_pad);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_avg_pool3d_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_avg_pool3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",7, false, Backend::CPU, ScalarType::Float);
            THNN_FloatVolumetricAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], ceil_mode, count_include_pad);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_avg_pool3d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_avg_pool3d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatVolumetricAveragePooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], ceil_mode, count_include_pad);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_avg_pool3d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::_thnn_max_pool2d_with_indices_forward_out(Tensor & output, Tensor & indices, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<2>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 5);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CPU, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",6, false, Backend::CPU, ScalarType::Long);
            THNN_FloatSpatialDilatedMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], ceil_mode);
            output_->maybe_zero_dim(false);
            return std::tuple<Tensor &, Tensor &>(output, indices);
            break;
        }
        default:
            AT_ERROR("_thnn_max_pool2d_with_indices_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_thnn_max_pool2d_with_indices_forward(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<2>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 5);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CPUTensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            THNN_FloatSpatialDilatedMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], ceil_mode);
            output_->maybe_zero_dim(false);
            return std::tuple<Tensor, Tensor>(output, indices);
            break;
        }
        default:
            AT_ERROR("_thnn_max_pool2d_with_indices_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_max_pool2d_with_indices_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<2>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 6);
            auto indices_ = checked_tensor_unwrap(indices,"indices",8, false, Backend::CPU, ScalarType::Long);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",8, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSpatialDilatedMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], ceil_mode);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_max_pool2d_with_indices_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_max_pool2d_with_indices_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<2>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 6);
            auto indices_ = checked_tensor_unwrap(indices,"indices",8, false, Backend::CPU, ScalarType::Long);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatSpatialDilatedMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], ceil_mode);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_max_pool2d_with_indices_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::_thnn_max_pool3d_with_indices_forward_out(Tensor & output, Tensor & indices, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<3>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 4);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 5);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CPU, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",6, false, Backend::CPU, ScalarType::Long);
            THNN_FloatVolumetricDilatedMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], ceil_mode);
            output_->maybe_zero_dim(false);
            return std::tuple<Tensor &, Tensor &>(output, indices);
            break;
        }
        default:
            AT_ERROR("_thnn_max_pool3d_with_indices_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_thnn_max_pool3d_with_indices_forward(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 2);
            auto stride_ = check_intlist<3>(stride, "stride", 3, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 4);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 5);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),CPUTensorId()).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            THNN_FloatVolumetricDilatedMaxPooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], ceil_mode);
            output_->maybe_zero_dim(false);
            return std::tuple<Tensor, Tensor>(output, indices);
            break;
        }
        default:
            AT_ERROR("_thnn_max_pool3d_with_indices_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_max_pool3d_with_indices_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 6);
            auto indices_ = checked_tensor_unwrap(indices,"indices",8, false, Backend::CPU, ScalarType::Long);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",8, false, Backend::CPU, ScalarType::Float);
            THNN_FloatVolumetricDilatedMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], ceil_mode);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_max_pool3d_with_indices_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_max_pool3d_with_indices_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4, kernel_size);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 6);
            auto indices_ = checked_tensor_unwrap(indices,"indices",8, false, Backend::CPU, ScalarType::Long);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatVolumetricDilatedMaxPooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], ceil_mode);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_max_pool3d_with_indices_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_max_unpool2d_forward_out(Tensor & output, const Tensor & self, const Tensor & indices, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CPU, ScalarType::Long);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 3);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSpatialMaxUnpooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[1], output_size_[0]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_max_unpool2d_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_max_unpool2d_forward(const Tensor & self, const Tensor & indices, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CPU, ScalarType::Long);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 3);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatSpatialMaxUnpooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[1], output_size_[0]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_max_unpool2d_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_max_unpool2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CPU, ScalarType::Long);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 4);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSpatialMaxUnpooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, output_size_[1], output_size_[0]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_max_unpool2d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_max_unpool2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CPU, ScalarType::Long);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 4);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatSpatialMaxUnpooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, output_size_[1], output_size_[0]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_max_unpool2d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_max_unpool3d_forward_out(Tensor & output, const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CPU, ScalarType::Long);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CPU, ScalarType::Float);
            THNN_FloatVolumetricMaxUnpooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[0], output_size_[2], output_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_max_unpool3d_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_max_unpool3d_forward(const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",2, false, Backend::CPU, ScalarType::Long);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 3);
            auto stride_ = check_intlist<3>(stride, "stride", 4);
            auto padding_ = check_intlist<3>(padding, "padding", 5);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatVolumetricMaxUnpooling_updateOutput(globalContext().getTHCState(), self_, output_, indices_, output_size_[0], output_size_[2], output_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_max_unpool3d_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_max_unpool3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CPU, ScalarType::Long);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",6, false, Backend::CPU, ScalarType::Float);
            THNN_FloatVolumetricMaxUnpooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, output_size_[0], output_size_[2], output_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_max_unpool3d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_max_unpool3d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & indices, IntArrayRef output_size, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto indices_ = checked_tensor_unwrap(indices,"indices",3, false, Backend::CPU, ScalarType::Long);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatVolumetricMaxUnpooling_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, indices_, output_size_[0], output_size_[2], output_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_max_unpool3d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_upsample_linear1d_forward_out(Tensor & output, const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CPU, ScalarType::Float);
            THNN_FloatTemporalUpSamplingLinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], align_corners);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_linear1d_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_upsample_linear1d_forward(const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatTemporalUpSamplingLinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], align_corners);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_linear1d_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_upsample_linear1d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<3>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CPU, ScalarType::Float);
            THNN_FloatTemporalUpSamplingLinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], output_size_[0], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_linear1d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_upsample_linear1d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<3>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatTemporalUpSamplingLinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], output_size_[0], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_linear1d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_upsample_bilinear2d_forward_out(Tensor & output, const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSpatialUpSamplingBilinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], align_corners);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_bilinear2d_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_upsample_bilinear2d_forward(const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatSpatialUpSamplingBilinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], align_corners);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_bilinear2d_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_upsample_bilinear2d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSpatialUpSamplingBilinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_bilinear2d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_upsample_bilinear2d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatSpatialUpSamplingBilinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_bilinear2d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_upsample_bicubic2d_forward_out(Tensor & output, const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSpatialUpSamplingBicubic_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], align_corners);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_bicubic2d_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_upsample_bicubic2d_forward(const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatSpatialUpSamplingBicubic_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], align_corners);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_bicubic2d_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_upsample_bicubic2d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSpatialUpSamplingBicubic_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_bicubic2d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_upsample_bicubic2d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatSpatialUpSamplingBicubic_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_bicubic2d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_upsample_trilinear3d_forward_out(Tensor & output, const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",3, false, Backend::CPU, ScalarType::Float);
            THNN_FloatVolumetricUpSamplingTrilinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], output_size_[2], align_corners);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_trilinear3d_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_upsample_trilinear3d_forward(const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatVolumetricUpSamplingTrilinear_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], output_size_[2], align_corners);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_trilinear3d_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_upsample_trilinear3d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<5>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",4, false, Backend::CPU, ScalarType::Float);
            THNN_FloatVolumetricUpSamplingTrilinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], input_size_[4], output_size_[0], output_size_[1], output_size_[2], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_trilinear3d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_upsample_trilinear3d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<5>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatVolumetricUpSamplingTrilinear_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], input_size_[4], output_size_[0], output_size_[1], output_size_[2], align_corners);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_trilinear3d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_upsample_nearest1d_forward_out(Tensor & output, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CPU, ScalarType::Float);
            THNN_FloatTemporalUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0]);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest1d_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_upsample_nearest1d_forward(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatTemporalUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0]);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest1d_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_upsample_nearest1d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<3>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CPU, ScalarType::Float);
            THNN_FloatTemporalUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], output_size_[0]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest1d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_upsample_nearest1d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<1>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<3>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatTemporalUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], output_size_[0]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest1d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_upsample_nearest2d_forward_out(Tensor & output, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSpatialUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1]);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest2d_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_upsample_nearest2d_forward(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatSpatialUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1]);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest2d_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_upsample_nearest2d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSpatialUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest2d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_upsample_nearest2d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<4>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatSpatialUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], output_size_[0], output_size_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest2d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_upsample_nearest3d_forward_out(Tensor & output, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CPU, ScalarType::Float);
            THNN_FloatVolumetricUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], output_size_[2]);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest3d_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_upsample_nearest3d_forward(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatVolumetricUpSamplingNearest_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], output_size_[2]);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest3d_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_upsample_nearest3d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<5>(input_size, "input_size", 3);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",3, false, Backend::CPU, ScalarType::Float);
            THNN_FloatVolumetricUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], input_size_[4], output_size_[0], output_size_[1], output_size_[2]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest3d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_upsample_nearest3d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<3>(output_size, "output_size", 2);
            auto input_size_ = check_intlist<5>(input_size, "input_size", 3);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatVolumetricUpSamplingNearest_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], input_size_[2], input_size_[3], input_size_[4], output_size_[0], output_size_[1], output_size_[2]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_upsample_nearest3d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_sigmoid_forward_out(Tensor & output, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",1, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSigmoid_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_sigmoid_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_sigmoid_forward(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatSigmoid_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_sigmoid_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_sigmoid_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",2, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSigmoid_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_sigmoid_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_sigmoid_backward(const Tensor & grad_output, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatSigmoid_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_sigmoid_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_tanh_forward_out(Tensor & output, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",1, false, Backend::CPU, ScalarType::Float);
            THNN_FloatTanh_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_tanh_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_tanh_forward(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatTanh_updateOutput(globalContext().getTHCState(), self_, output_);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_tanh_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_tanh_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",2, false, Backend::CPU, ScalarType::Float);
            THNN_FloatTanh_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_tanh_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_tanh_backward(const Tensor & grad_output, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto output_ = checked_tensor_unwrap(output,"output",2, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatTanh_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, output_);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_tanh_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::_thnn_conv_transpose2d_forward_out(Tensor & output, Tensor & columns, Tensor & ones, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CPU, ScalarType::Float);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_padding_ = check_intlist<2>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 8);
            auto output_ = checked_tensor_unwrap(output,"output",8, false, Backend::CPU, ScalarType::Float);
            auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CPU, ScalarType::Float);
            auto ones_ = checked_tensor_unwrap(ones,"ones",8, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSpatialFullDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, columns, ones);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_transpose2d_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> Type::_thnn_conv_transpose2d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CPU, ScalarType::Float);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_padding_ = check_intlist<2>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 8);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto columns_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto columns = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(columns_));
            auto ones_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto ones = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(ones_));
            THNN_FloatSpatialFullDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, columns, ones);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_transpose2d_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::_thnn_conv_transpose2d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation, const Tensor & columns, const Tensor & ones) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_padding_ = check_intlist<2>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 8);
            auto columns_ = checked_tensor_unwrap(columns,"columns",9, false, Backend::CPU, ScalarType::Float);
            auto ones_ = checked_tensor_unwrap(ones,"ones",10, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",10, true, Backend::CPU, ScalarType::Float);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",10, true, Backend::CPU, ScalarType::Float);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",10, true, Backend::CPU, ScalarType::Float);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(1) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_FloatSpatialFullDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_FloatSpatialFullDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_transpose2d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> Type::_thnn_conv_transpose2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation, const Tensor & columns, const Tensor & ones, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_padding_ = check_intlist<2>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 8);
            auto columns_ = checked_tensor_unwrap(columns,"columns",9, false, Backend::CPU, ScalarType::Float);
            auto ones_ = checked_tensor_unwrap(ones,"ones",10, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(1) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_FloatSpatialFullDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_FloatSpatialFullDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], output_padding_[1], output_padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_transpose2d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::_thnn_conv_transpose3d_forward_out(Tensor & output, Tensor & finput, Tensor & fgrad_input, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CPU, ScalarType::Float);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto output_padding_ = check_intlist<3>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 8);
            auto output_ = checked_tensor_unwrap(output,"output",8, false, Backend::CPU, ScalarType::Float);
            auto finput_ = checked_tensor_unwrap(finput,"finput",8, false, Backend::CPU, ScalarType::Float);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",8, false, Backend::CPU, ScalarType::Float);
            THNN_FloatVolumetricFullDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, finput, fgrad_input);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_transpose3d_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> Type::_thnn_conv_transpose3d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CPU, ScalarType::Float);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto output_padding_ = check_intlist<3>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 8);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto finput_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto finput = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(finput_));
            auto fgrad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto fgrad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(fgrad_input_));
            THNN_FloatVolumetricFullDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, finput, fgrad_input);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_transpose3d_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::_thnn_conv_transpose3d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation, const Tensor & finput, const Tensor & fgrad_input) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto output_padding_ = check_intlist<3>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 8);
            auto finput_ = checked_tensor_unwrap(finput,"finput",9, false, Backend::CPU, ScalarType::Float);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",10, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",10, true, Backend::CPU, ScalarType::Float);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",10, true, Backend::CPU, ScalarType::Float);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",10, true, Backend::CPU, ScalarType::Float);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(1) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_FloatVolumetricFullDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1]);
            if (grad_weight_ || grad_bias_) THNN_FloatVolumetricFullDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_transpose3d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> Type::_thnn_conv_transpose3d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef dilation, const Tensor & finput, const Tensor & fgrad_input, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto output_padding_ = check_intlist<3>(output_padding, "output_padding", 7);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 8);
            auto finput_ = checked_tensor_unwrap(finput,"finput",9, false, Backend::CPU, ScalarType::Float);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",10, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(1) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_FloatVolumetricFullDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1]);
            if (grad_weight_ || grad_bias_) THNN_FloatVolumetricFullDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], output_padding_[0], output_padding_[2], output_padding_[1], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_transpose3d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::_thnn_conv2d_forward_out(Tensor & output, Tensor & finput, Tensor & fgrad_input, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CPU, ScalarType::Float);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CPU, ScalarType::Float);
            auto finput_ = checked_tensor_unwrap(finput,"finput",6, false, Backend::CPU, ScalarType::Float);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",6, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSpatialConvolutionMM_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, finput, fgrad_input);
            break;
        }
        default:
            AT_ERROR("_thnn_conv2d_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> Type::_thnn_conv2d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CPU, ScalarType::Float);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto finput_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto finput = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(finput_));
            auto fgrad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto fgrad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(fgrad_input_));
            THNN_FloatSpatialConvolutionMM_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, finput, fgrad_input);
            break;
        }
        default:
            AT_ERROR("_thnn_conv2d_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::_thnn_conv2d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, const Tensor & finput, const Tensor & fgrad_input) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto finput_ = checked_tensor_unwrap(finput,"finput",7, false, Backend::CPU, ScalarType::Float);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",8, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",8, true, Backend::CPU, ScalarType::Float);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",8, true, Backend::CPU, ScalarType::Float);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",8, true, Backend::CPU, ScalarType::Float);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_FloatSpatialConvolutionMM_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_FloatSpatialConvolutionMM_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv2d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> Type::_thnn_conv2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, const Tensor & finput, const Tensor & fgrad_input, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto finput_ = checked_tensor_unwrap(finput,"finput",7, false, Backend::CPU, ScalarType::Float);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",8, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_FloatSpatialConvolutionMM_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0]);
            if (grad_weight_ || grad_bias_) THNN_FloatSpatialConvolutionMM_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv2d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_conv_depthwise2d_forward_out(Tensor & output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    AT_ERROR("_thnn_conv_depthwise2d_forward_out is not implemented for type ", toString());
}
Tensor Type::_thnn_conv_depthwise2d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    AT_ERROR("_thnn_conv_depthwise2d_forward is not implemented for type ", toString());
}
std::tuple<Tensor &,Tensor &> Type::_thnn_conv_depthwise2d_backward_out(Tensor & grad_input, Tensor & grad_weight, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    AT_ERROR("_thnn_conv_depthwise2d_backward_out is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor> Type::_thnn_conv_depthwise2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, std::array<bool,2> output_mask) const {
    AT_ERROR("_thnn_conv_depthwise2d_backward is not implemented for type ", toString());
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::_thnn_conv3d_forward_out(Tensor & output, Tensor & finput, Tensor & fgrad_input, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CPU, ScalarType::Float);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CPU, ScalarType::Float);
            auto finput_ = checked_tensor_unwrap(finput,"finput",6, false, Backend::CPU, ScalarType::Float);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",6, false, Backend::CPU, ScalarType::Float);
            THNN_FloatVolumetricConvolutionMM_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, finput, fgrad_input);
            break;
        }
        default:
            AT_ERROR("_thnn_conv3d_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> Type::_thnn_conv3d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CPU, ScalarType::Float);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto finput_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto finput = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(finput_));
            auto fgrad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto fgrad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(fgrad_input_));
            THNN_FloatVolumetricConvolutionMM_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            finput_->maybe_zero_dim(maybe_scalar);
            fgrad_input_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, finput, fgrad_input);
            break;
        }
        default:
            AT_ERROR("_thnn_conv3d_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::_thnn_conv3d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, const Tensor & finput, const Tensor & fgrad_input) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto finput_ = checked_tensor_unwrap(finput,"finput",7, false, Backend::CPU, ScalarType::Float);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",8, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",8, true, Backend::CPU, ScalarType::Float);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",8, true, Backend::CPU, ScalarType::Float);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",8, true, Backend::CPU, ScalarType::Float);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_FloatVolumetricConvolutionMM_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
            if (grad_weight_ || grad_bias_) THNN_FloatVolumetricConvolutionMM_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv3d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> Type::_thnn_conv3d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, const Tensor & finput, const Tensor & fgrad_input, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto finput_ = checked_tensor_unwrap(finput,"finput",7, false, Backend::CPU, ScalarType::Float);
            auto fgrad_input_ = checked_tensor_unwrap(fgrad_input,"fgrad_input",8, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_FloatVolumetricConvolutionMM_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1]);
            if (grad_weight_ || grad_bias_) THNN_FloatVolumetricConvolutionMM_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, finput_, fgrad_input_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv3d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::_thnn_conv_dilated2d_forward_out(Tensor & output, Tensor & columns, Tensor & ones, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CPU, ScalarType::Float);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto output_ = checked_tensor_unwrap(output,"output",7, false, Backend::CPU, ScalarType::Float);
            auto columns_ = checked_tensor_unwrap(columns,"columns",7, false, Backend::CPU, ScalarType::Float);
            auto ones_ = checked_tensor_unwrap(ones,"ones",7, false, Backend::CPU, ScalarType::Float);
            THNN_FloatSpatialDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, columns, ones);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_dilated2d_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> Type::_thnn_conv_dilated2d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CPU, ScalarType::Float);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto columns_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto columns = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(columns_));
            auto ones_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto ones = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(ones_));
            THNN_FloatSpatialDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, columns, ones);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_dilated2d_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::_thnn_conv_dilated2d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, const Tensor & columns, const Tensor & ones) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CPU, ScalarType::Float);
            auto ones_ = checked_tensor_unwrap(ones,"ones",9, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",9, true, Backend::CPU, ScalarType::Float);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",9, true, Backend::CPU, ScalarType::Float);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",9, true, Backend::CPU, ScalarType::Float);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_FloatSpatialDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_weight_ || grad_bias_) THNN_FloatSpatialDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_dilated2d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> Type::_thnn_conv_dilated2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, const Tensor & columns, const Tensor & ones, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto padding_ = check_intlist<2>(padding, "padding", 6);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 7);
            auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CPU, ScalarType::Float);
            auto ones_ = checked_tensor_unwrap(ones,"ones",9, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_FloatSpatialDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0]);
            if (grad_weight_ || grad_bias_) THNN_FloatSpatialDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[1], kernel_size_[0], stride_[1], stride_[0], padding_[1], padding_[0], dilation_[1], dilation_[0], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_dilated2d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::_thnn_conv_dilated3d_forward_out(Tensor & output, Tensor & columns, Tensor & ones, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CPU, ScalarType::Float);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 7);
            auto output_ = checked_tensor_unwrap(output,"output",7, false, Backend::CPU, ScalarType::Float);
            auto columns_ = checked_tensor_unwrap(columns,"columns",7, false, Backend::CPU, ScalarType::Float);
            auto ones_ = checked_tensor_unwrap(ones,"ones",7, false, Backend::CPU, ScalarType::Float);
            THNN_FloatVolumetricDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor &, Tensor &, Tensor &>(output, columns, ones);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_dilated3d_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> Type::_thnn_conv_dilated3d_forward(const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, const Tensor & bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",2, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 3);
            auto bias_ = checked_tensor_unwrap(bias,"bias",4, true, Backend::CPU, ScalarType::Float);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 7);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto columns_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto columns = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(columns_));
            auto ones_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto ones = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(ones_));
            THNN_FloatVolumetricDilatedConvolution_updateOutput(globalContext().getTHCState(), self_, output_, weight_, bias_ ? bias_ : NULL, columns_, ones_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1]);
            bool maybe_scalar = self_->dim() == 0 && weight_->dim() == 0 && (!bias_ || bias_->dim() == 0);
            output_->maybe_zero_dim(maybe_scalar);
            columns_->maybe_zero_dim(maybe_scalar);
            ones_->maybe_zero_dim(maybe_scalar);
            return std::tuple<Tensor, Tensor, Tensor>(output, columns, ones);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_dilated3d_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &,Tensor &> Type::_thnn_conv_dilated3d_backward_out(Tensor & grad_input, Tensor & grad_weight, Tensor & grad_bias, const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, const Tensor & columns, const Tensor & ones) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 7);
            auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CPU, ScalarType::Float);
            auto ones_ = checked_tensor_unwrap(ones,"ones",9, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",9, true, Backend::CPU, ScalarType::Float);
            auto grad_weight_ = checked_tensor_unwrap(grad_weight,"grad_weight",9, true, Backend::CPU, ScalarType::Float);
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = checked_tensor_unwrap(grad_bias,"grad_bias",9, true, Backend::CPU, ScalarType::Float);
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_FloatVolumetricDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1]);
            if (grad_weight_ || grad_bias_) THNN_FloatVolumetricDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor &, Tensor &, Tensor &>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_dilated3d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> Type::_thnn_conv_dilated3d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, const Tensor & columns, const Tensor & ones, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto self_ = checked_tensor_unwrap(self,"self",2, false, Backend::CPU, ScalarType::Float);
            auto weight_ = checked_tensor_unwrap(weight,"weight",3, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<3>(kernel_size, "kernel_size", 4);
            auto stride_ = check_intlist<3>(stride, "stride", 5);
            auto padding_ = check_intlist<3>(padding, "padding", 6);
            auto dilation_ = check_intlist<3>(dilation, "dilation", 7);
            auto columns_ = checked_tensor_unwrap(columns,"columns",8, false, Backend::CPU, ScalarType::Float);
            auto ones_ = checked_tensor_unwrap(ones,"ones",9, false, Backend::CPU, ScalarType::Float);
            auto grad_input_ = output_mask[0] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release() : nullptr;
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_input_));
            auto grad_weight_ = output_mask[1] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release() : nullptr;
            auto grad_weight = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_weight_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_weight_));
            if (grad_weight.defined()) {
                grad_weight.resize_(weight.sizes());
                grad_weight.zero_();
            }
            auto grad_bias_ = output_mask[2] ? c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release() : nullptr;
            auto grad_bias = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_bias_ == nullptr ? (TensorImpl*)UndefinedTensorImpl::singleton() : (TensorImpl*)grad_bias_));
            if (grad_bias.defined()) {
                grad_bias.resize_({ weight.size(0) });
                grad_bias.zero_();
            }
            if (grad_input_) THNN_FloatVolumetricDilatedConvolution_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_ ? grad_input_ : NULL, weight_, columns_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1]);
            if (grad_weight_ || grad_bias_) THNN_FloatVolumetricDilatedConvolution_accGradParameters(globalContext().getTHCState(), self_, grad_output_, grad_weight_ ? grad_weight_ : NULL, grad_bias_ ? grad_bias_ : NULL, columns_, ones_, kernel_size_[0], kernel_size_[2], kernel_size_[1], stride_[0], stride_[2], stride_[1], padding_[0], padding_[2], padding_[1], dilation_[0], dilation_[2], dilation_[1], 1);
            if (grad_input_) grad_input_->maybe_zero_dim(self_->dim() == 0);
            return std::tuple<Tensor, Tensor, Tensor>(grad_input, grad_weight, grad_bias);
            break;
        }
        default:
            AT_ERROR("_thnn_conv_dilated3d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_col2im_forward_out(Tensor & output, const Tensor & self, IntArrayRef output_size, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 4);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto stride_ = check_intlist<2>(stride, "stride", 6);
            auto output_ = checked_tensor_unwrap(output,"output",6, false, Backend::CPU, ScalarType::Float);
            THNN_FloatCol2Im_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_col2im_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_col2im_forward(const Tensor & self, IntArrayRef output_size, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto output_size_ = check_intlist<2>(output_size, "output_size", 2);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 4);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto stride_ = check_intlist<2>(stride, "stride", 6);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatCol2Im_updateOutput(globalContext().getTHCState(), self_, output_, output_size_[0], output_size_[1], kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_col2im_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_col2im_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 3);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",5, false, Backend::CPU, ScalarType::Float);
            THNN_FloatCol2Im_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_col2im_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_col2im_backward(const Tensor & grad_output, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 3);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatCol2Im_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_col2im_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_im2col_forward_out(Tensor & output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 3);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto output_ = checked_tensor_unwrap(output,"output",5, false, Backend::CPU, ScalarType::Float);
            THNN_FloatIm2Col_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_im2col_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_im2col_forward(const Tensor & self, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto self_ = checked_tensor_unwrap(self,"self",1, false, Backend::CPU, ScalarType::Float);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 2);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 3);
            auto padding_ = check_intlist<2>(padding, "padding", 4);
            auto stride_ = check_intlist<2>(stride, "stride", 5);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatIm2Col_updateOutput(globalContext().getTHCState(), self_, output_, kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            output_->maybe_zero_dim(false);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_im2col_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_thnn_im2col_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef input_size, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto input_size_ = check_intlist<2>(input_size, "input_size", 2);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 4);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto stride_ = check_intlist<2>(stride, "stride", 6);
            auto grad_input_ = checked_tensor_unwrap(grad_input,"grad_input",6, false, Backend::CPU, ScalarType::Float);
            THNN_FloatIm2Col_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_im2col_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_thnn_im2col_backward(const Tensor & grad_output, IntArrayRef input_size, IntArrayRef kernel_size, IntArrayRef dilation, IntArrayRef padding, IntArrayRef stride) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Float: {
            auto grad_output_ = checked_tensor_unwrap(grad_output,"grad_output",1, false, Backend::CPU, ScalarType::Float);
            auto input_size_ = check_intlist<2>(input_size, "input_size", 2);
            auto kernel_size_ = check_intlist<2>(kernel_size, "kernel_size", 3);
            auto dilation_ = check_intlist<2>(dilation, "dilation", 4);
            auto padding_ = check_intlist<2>(padding, "padding", 5);
            auto stride_ = check_intlist<2>(stride, "stride", 6);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),CPUTensorId()).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatIm2Col_updateGradInput(globalContext().getTHCState(), grad_output_, grad_input_, input_size_[0], input_size_[1], kernel_size_[0], kernel_size_[1], dilation_[0], dilation_[1], padding_[0], padding_[1], stride_[0], stride_[1]);
            grad_input_->maybe_zero_dim(false);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_im2col_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_cudnn_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity) const {
    AT_ERROR("_cudnn_ctc_loss is not implemented for type ", toString());
}
Tensor Type::_cudnn_rnn_flatten_weight(TensorList weight_arr, int64_t weight_stride0, int64_t input_size, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, bool bidirectional) const {
    AT_ERROR("_cudnn_rnn_flatten_weight is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> Type::_cudnn_rnn(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state) const {
    AT_ERROR("_cudnn_rnn is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor,Tensor,std::vector<Tensor>> Type::_cudnn_rnn_backward(const Tensor & input, TensorList weight, int64_t weight_stride0, const Tensor & weight_buf, const Tensor & hx, const Tensor & cx, const Tensor & output, const Tensor & grad_output, const Tensor & grad_hy, const Tensor & grad_cy, int64_t mode, int64_t hidden_size, int64_t num_layers, bool batch_first, double dropout, bool train, bool bidirectional, IntArrayRef batch_sizes, const Tensor & dropout_state, const Tensor & reserve, std::array<bool,4> output_mask) const {
    AT_ERROR("_cudnn_rnn_backward is not implemented for type ", toString());
}
Tensor Type::_cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed, const TensorOptions & options) const {
    AT_ERROR("_cudnn_init_dropout_state is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor> Type::_fused_dropout(const Tensor & self, double p, Generator * generator) const {
    AT_ERROR("_fused_dropout is not implemented for type ", toString());
}
Tensor Type::_masked_scale(const Tensor & self, const Tensor & mask, double scale) const {
    AT_ERROR("_masked_scale is not implemented for type ", toString());
}
Tensor & Type::abs_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_abs__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("abs_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::abs_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_abs_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("abs_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::acos_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_acos__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("acos_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::acos_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_acos_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("acos_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::add(const Tensor & self, const Tensor & other, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::add(/* actuals */ self, other, alpha);
        break;
        default:
            AT_ERROR("add not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::add_(Tensor & self, const Tensor & other, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::add_(/* actuals */ self, other, alpha);
        break;
        default:
            AT_ERROR("add_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::add_out(/* actuals */ out, self, other, alpha);
        break;
        default:
            AT_ERROR("add_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::arange_out(Tensor & out, Scalar start, Scalar end, Scalar step) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    auto dispatch_scalar_type = infer_scalar_type(out);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::arange_cpu_out(/* actuals */ out, start, end, step);
        break;
        default:
            AT_ERROR("arange_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::as_strided(const Tensor & self, IntArrayRef size, IntArrayRef stride, c10::optional<int64_t> storage_offset) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool:
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::as_strided_tensorimpl(/* actuals */ self, size, stride, storage_offset);
        break;
        default:
            AT_ERROR("as_strided not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::asin_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_asin__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("asin_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::asin_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_asin_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("asin_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::atan_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_atan__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("atan_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::atan_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_atan_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("atan_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::baddbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::baddbmm_cpu(/* actuals */ self, batch1, batch2, beta, alpha);
        break;
        default:
            AT_ERROR("baddbmm not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::baddbmm_(Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::baddbmm__cpu(/* actuals */ self, batch1, batch2, beta, alpha);
        break;
        default:
            AT_ERROR("baddbmm_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::baddbmm_out(Tensor & out, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::baddbmm_out_cpu(/* actuals */ out, self, batch1, batch2, beta, alpha);
        break;
        default:
            AT_ERROR("baddbmm_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::bernoulli_(Tensor & self, const Tensor & p, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::bernoulli_tensor_cpu_(/* actuals */ self, p, generator);
        break;
        default:
            AT_ERROR("bernoulli_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::bernoulli_(Tensor & self, double p, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::bernoulli_scalar_cpu_(/* actuals */ self, p, generator);
        break;
        default:
            AT_ERROR("bernoulli_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::bincount(const Tensor & self, const Tensor & weights, int64_t minlength) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_bincount_cpu(/* actuals */ self, weights, minlength);
        break;
        default:
            AT_ERROR("bincount not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::bmm(const Tensor & self, const Tensor & mat2) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::bmm_cpu(/* actuals */ self, mat2);
        break;
        default:
            AT_ERROR("bmm not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::bmm_out(Tensor & out, const Tensor & self, const Tensor & mat2) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::bmm_out_cpu(/* actuals */ out, self, mat2);
        break;
        default:
            AT_ERROR("bmm_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::ceil_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_ceil__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("ceil_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::ceil_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_ceil_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("ceil_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::clamp_(Tensor & self, c10::optional<Scalar> min, c10::optional<Scalar> max) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_clamp__cpu(/* actuals */ self, min, max);
        break;
        default:
            AT_ERROR("clamp_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::clamp_out(Tensor & out, const Tensor & self, c10::optional<Scalar> min, c10::optional<Scalar> max) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_clamp_out_cpu(/* actuals */ out, self, min, max);
        break;
        default:
            AT_ERROR("clamp_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::clamp_max_(Tensor & self, Scalar max) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_clamp_max__cpu(/* actuals */ self, max);
        break;
        default:
            AT_ERROR("clamp_max_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::clamp_max_out(Tensor & out, const Tensor & self, Scalar max) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_clamp_max_out_cpu(/* actuals */ out, self, max);
        break;
        default:
            AT_ERROR("clamp_max_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::clamp_min_(Tensor & self, Scalar min) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_clamp_min__cpu(/* actuals */ self, min);
        break;
        default:
            AT_ERROR("clamp_min_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::clamp_min_out(Tensor & out, const Tensor & self, Scalar min) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_clamp_min_out_cpu(/* actuals */ out, self, min);
        break;
        default:
            AT_ERROR("clamp_min_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool:
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_s_copy__cpu(/* actuals */ self, src, non_blocking);
        break;
        default:
            AT_ERROR("s_copy_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_s_copy_from(const Tensor & self, const Tensor & dst, bool non_blocking) const {
    AT_ERROR("_s_copy_from is not implemented for type ", toString());
}
void Type::_copy_same_type_(Tensor & self, const Tensor & src) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool:
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
 at::native::_copy_same_type__cpu(/* actuals */ self, src);
        break;
        default:
            AT_ERROR("_copy_same_type_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::cos_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_cos__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("cos_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::cos_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_cos_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("cos_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::cosh_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_cosh__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("cosh_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::cosh_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_cosh_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("cosh_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::cudnn_affine_grid_generator(const Tensor & theta, int64_t N, int64_t C, int64_t H, int64_t W) const {
    AT_ERROR("cudnn_affine_grid_generator is not implemented for type ", toString());
}
Tensor Type::cudnn_affine_grid_generator_backward(const Tensor & grad, int64_t N, int64_t C, int64_t H, int64_t W) const {
    AT_ERROR("cudnn_affine_grid_generator_backward is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor,Tensor> Type::cudnn_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
    AT_ERROR("cudnn_batch_norm is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor,Tensor> Type::cudnn_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
    AT_ERROR("cudnn_batch_norm_backward is not implemented for type ", toString());
}
Tensor Type::cudnn_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("cudnn_convolution is not implemented for type ", toString());
}
Tensor Type::cudnn_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("cudnn_convolution_backward_input is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor,Tensor> Type::cudnn_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
    AT_ERROR("cudnn_convolution_backward is not implemented for type ", toString());
}
Tensor Type::cudnn_convolution_backward_bias(const Tensor & grad_output) const {
    AT_ERROR("cudnn_convolution_backward_bias is not implemented for type ", toString());
}
Tensor Type::cudnn_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("cudnn_convolution_backward_weight is not implemented for type ", toString());
}
Tensor Type::cudnn_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("cudnn_convolution_transpose is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor,Tensor> Type::cudnn_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
    AT_ERROR("cudnn_convolution_transpose_backward is not implemented for type ", toString());
}
Tensor Type::cudnn_convolution_transpose_backward_bias(const Tensor & grad_output) const {
    AT_ERROR("cudnn_convolution_transpose_backward_bias is not implemented for type ", toString());
}
Tensor Type::cudnn_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("cudnn_convolution_transpose_backward_input is not implemented for type ", toString());
}
Tensor Type::cudnn_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("cudnn_convolution_transpose_backward_weight is not implemented for type ", toString());
}
Tensor Type::cudnn_grid_sampler(const Tensor & self, const Tensor & grid) const {
    AT_ERROR("cudnn_grid_sampler is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor> Type::cudnn_grid_sampler_backward(const Tensor & self, const Tensor & grid, const Tensor & grad_output) const {
    AT_ERROR("cudnn_grid_sampler_backward is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor> Type::_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool zero_infinity) const {
    const OptionalDeviceGuard device_guard(device_of(log_probs));
    auto dispatch_scalar_type = infer_scalar_type(log_probs);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::ctc_loss_cpu(/* actuals */ log_probs, targets, input_lengths, target_lengths, blank, zero_infinity);
        break;
        default:
            AT_ERROR("_ctc_loss not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_ctc_loss_backward(const Tensor & grad, const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, const Tensor & neg_log_likelihood, const Tensor & log_alpha, int64_t blank, bool zero_infinity) const {
    const OptionalDeviceGuard device_guard(device_of(grad));
    auto dispatch_scalar_type = infer_scalar_type(grad);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::ctc_loss_backward_cpu(/* actuals */ grad, log_probs, targets, input_lengths, target_lengths, neg_log_likelihood, log_alpha, blank, zero_infinity);
        break;
        default:
            AT_ERROR("_ctc_loss_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::embedding_dense_backward(const Tensor & grad_output, const Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::embedding_dense_backward_cpu(/* actuals */ grad_output, indices, num_weights, padding_idx, scale_grad_by_freq);
        break;
        default:
            AT_ERROR("embedding_dense_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::embedding_renorm_(Tensor & self, const Tensor & indices, double max_norm, double norm_type) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::embedding_renorm_cpu_(/* actuals */ self, indices, max_norm, norm_type);
        break;
        default:
            AT_ERROR("embedding_renorm_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor,Tensor> Type::_embedding_bag(const Tensor & weight, const Tensor & indices, const Tensor & offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, const Tensor & per_sample_weights) const {
    const OptionalDeviceGuard device_guard(device_of(weight));
    auto dispatch_scalar_type = infer_scalar_type(weight);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_embedding_bag_cpu(/* actuals */ weight, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights);
        break;
        default:
            AT_ERROR("_embedding_bag not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_embedding_bag_dense_backward(const Tensor & grad, const Tensor & indices, const Tensor & offsets, const Tensor & offset2bag, const Tensor & bag_size, const Tensor & maximum_indices, int64_t num_weights, bool scale_grad_by_freq, int64_t mode, const Tensor & per_sample_weights) const {
    const OptionalDeviceGuard device_guard(device_of(grad));
    auto dispatch_scalar_type = infer_scalar_type(grad);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_embedding_bag_dense_backward_cpu(/* actuals */ grad, indices, offsets, offset2bag, bag_size, maximum_indices, num_weights, scale_grad_by_freq, mode, per_sample_weights);
        break;
        default:
            AT_ERROR("_embedding_bag_dense_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_embedding_bag_per_sample_weights_backward(const Tensor & grad, const Tensor & weight, const Tensor & indices, const Tensor & offsets, const Tensor & offset2bag, int64_t mode) const {
    const OptionalDeviceGuard device_guard(device_of(grad));
    auto dispatch_scalar_type = infer_scalar_type(grad);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_embedding_bag_per_sample_weights_backward_cpu(/* actuals */ grad, weight, indices, offsets, offset2bag, mode);
        break;
        default:
            AT_ERROR("_embedding_bag_per_sample_weights_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::empty(IntArrayRef size, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    auto dispatch_scalar_type = typeMetaToScalarType(options.dtype());
    switch (dispatch_scalar_type) {
        case ScalarType::Bool:
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::empty_cpu(/* actuals */ size, options);
        break;
        default:
            AT_ERROR("empty not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_empty_affine_quantized(IntArrayRef size, const TensorOptions & options, double scale, int64_t zero_point) const {
    AT_ERROR("_empty_affine_quantized is not implemented for type ", toString());
}
Tensor & Type::resize_(Tensor & self, IntArrayRef size) const {
    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool:
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::resize_cpu_(/* actuals */ self, size);
        break;
        default:
            AT_ERROR("resize_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::empty_strided(IntArrayRef size, IntArrayRef stride, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    auto dispatch_scalar_type = typeMetaToScalarType(options.dtype());
    switch (dispatch_scalar_type) {
        case ScalarType::Bool:
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::empty_strided_cpu(/* actuals */ size, stride, options);
        break;
        default:
            AT_ERROR("empty_strided not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::erf_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_erf__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("erf_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::erf_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_erf_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("erf_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::erfc_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_erfc__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("erfc_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::erfc_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_erfc_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("erfc_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::exp_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_exp__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("exp_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::exp_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_exp_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("exp_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::expm1_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_expm1__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("expm1_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::expm1_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_expm1_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("expm1_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::eye_out(Tensor & out, int64_t n) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    auto dispatch_scalar_type = infer_scalar_type(out);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::eye_out_cpu(/* actuals */ out, n);
        break;
        default:
            AT_ERROR("eye_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::eye_out(Tensor & out, int64_t n, int64_t m) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    auto dispatch_scalar_type = infer_scalar_type(out);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::eye_out_cpu(/* actuals */ out, n, m);
        break;
        default:
            AT_ERROR("eye_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::floor_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_floor__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("floor_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::floor_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_floor_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("floor_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::frac_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_frac__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("frac_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::frac_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_frac_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("frac_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::from_file(std::string filename, c10::optional<bool> shared, c10::optional<int64_t> size, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    auto dispatch_scalar_type = typeMetaToScalarType(options.dtype());
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::from_file(/* actuals */ filename, shared, size, options);
        break;
        default:
            AT_ERROR("from_file not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::grid_sampler_2d(const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    auto dispatch_scalar_type = infer_scalar_type(input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::grid_sampler_2d_cpu(/* actuals */ input, grid, interpolation_mode, padding_mode);
        break;
        default:
            AT_ERROR("grid_sampler_2d not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::grid_sampler_2d_backward(const Tensor & grad_output, const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::grid_sampler_2d_backward_cpu(/* actuals */ grad_output, input, grid, interpolation_mode, padding_mode);
        break;
        default:
            AT_ERROR("grid_sampler_2d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::grid_sampler_3d(const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    auto dispatch_scalar_type = infer_scalar_type(input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::grid_sampler_3d_cpu(/* actuals */ input, grid, interpolation_mode, padding_mode);
        break;
        default:
            AT_ERROR("grid_sampler_3d not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::grid_sampler_3d_backward(const Tensor & grad_output, const Tensor & input, const Tensor & grid, int64_t interpolation_mode, int64_t padding_mode) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::grid_sampler_3d_backward_cpu(/* actuals */ grad_output, input, grid, interpolation_mode, padding_mode);
        break;
        default:
            AT_ERROR("grid_sampler_3d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_fft_with_size(const Tensor & self, int64_t signal_ndim, bool complex_input, bool complex_output, bool inverse, IntArrayRef checked_signal_sizes, bool normalized, bool onesided, IntArrayRef output_sizes) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_fft_mkl(/* actuals */ self, signal_ndim, complex_input, complex_output, inverse, checked_signal_sizes, normalized, onesided, output_sizes);
        break;
        default:
            AT_ERROR("_fft_with_size not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_inverse_helper(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_inverse_helper_cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("_inverse_helper not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::kl_div_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, int64_t reduction) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::kl_div_backward_cpu(/* actuals */ grad_output, self, target, reduction);
        break;
        default:
            AT_ERROR("kl_div_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::kthvalue_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t k, int64_t dim, bool keepdim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::kthvalue_out_cpu(/* actuals */ values, indices, self, k, dim, keepdim);
        break;
        default:
            AT_ERROR("kthvalue_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::mkldnn_linear(const Tensor & input, const Tensor & weight, const Tensor & bias) const {
    AT_ERROR("mkldnn_linear is not implemented for type ", toString());
}
Tensor & Type::linspace_out(Tensor & out, Scalar start, Scalar end, int64_t steps) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    auto dispatch_scalar_type = infer_scalar_type(out);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::linspace_cpu_out(/* actuals */ out, start, end, steps);
        break;
        default:
            AT_ERROR("linspace_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::log_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_log__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("log_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::log_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_log_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("log_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::log10_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_log10__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("log10_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::log10_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_log10_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("log10_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::log1p_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_log1p__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("log1p_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::log1p_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_log1p_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("log1p_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::log2_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_log2__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("log2_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::log2_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_log2_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("log2_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::logspace_out(Tensor & out, Scalar start, Scalar end, int64_t steps, double base) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    auto dispatch_scalar_type = infer_scalar_type(out);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::logspace_cpu_out(/* actuals */ out, start, end, steps, base);
        break;
        default:
            AT_ERROR("logspace_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_log_softmax(const Tensor & self, int64_t dim, bool half_to_float) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::log_softmax_cpu(/* actuals */ self, dim, half_to_float);
        break;
        default:
            AT_ERROR("_log_softmax not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_log_softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::log_softmax_backward_cpu(/* actuals */ grad_output, output, dim, self);
        break;
        default:
            AT_ERROR("_log_softmax_backward_data not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::mkldnn_max_pool2d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) const {
    AT_ERROR("mkldnn_max_pool2d is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor,Tensor> Type::miopen_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double exponential_average_factor, double epsilon) const {
    AT_ERROR("miopen_batch_norm is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor,Tensor> Type::miopen_batch_norm_backward(const Tensor & input, const Tensor & grad_output, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_var, double epsilon) const {
    AT_ERROR("miopen_batch_norm_backward is not implemented for type ", toString());
}
Tensor Type::miopen_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("miopen_convolution is not implemented for type ", toString());
}
Tensor Type::miopen_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("miopen_convolution_backward_input is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor,Tensor> Type::miopen_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
    AT_ERROR("miopen_convolution_backward is not implemented for type ", toString());
}
Tensor Type::miopen_convolution_backward_bias(const Tensor & grad_output) const {
    AT_ERROR("miopen_convolution_backward_bias is not implemented for type ", toString());
}
Tensor Type::miopen_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("miopen_convolution_backward_weight is not implemented for type ", toString());
}
Tensor Type::miopen_convolution_transpose(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("miopen_convolution_transpose is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor,Tensor> Type::miopen_convolution_transpose_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef output_padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
    AT_ERROR("miopen_convolution_transpose_backward is not implemented for type ", toString());
}
Tensor Type::miopen_convolution_transpose_backward_input(const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("miopen_convolution_transpose_backward_input is not implemented for type ", toString());
}
Tensor Type::miopen_convolution_transpose_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("miopen_convolution_transpose_backward_weight is not implemented for type ", toString());
}
Tensor Type::miopen_depthwise_convolution(const Tensor & self, const Tensor & weight, const Tensor & bias, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("miopen_depthwise_convolution is not implemented for type ", toString());
}
Tensor Type::miopen_depthwise_convolution_backward_input(IntArrayRef self_size, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("miopen_depthwise_convolution_backward_input is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor,Tensor> Type::miopen_depthwise_convolution_backward(const Tensor & self, const Tensor & grad_output, const Tensor & weight, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic, std::array<bool,3> output_mask) const {
    AT_ERROR("miopen_depthwise_convolution_backward is not implemented for type ", toString());
}
Tensor Type::miopen_depthwise_convolution_backward_weight(IntArrayRef weight_size, const Tensor & grad_output, const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups, bool benchmark, bool deterministic) const {
    AT_ERROR("miopen_depthwise_convolution_backward_weight is not implemented for type ", toString());
}
Tensor Type::narrow_copy(const Tensor & self, int64_t dim, int64_t start, int64_t length) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::narrow_copy_dense(/* actuals */ self, dim, start, length);
        break;
        default:
            AT_ERROR("narrow_copy not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> Type::native_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    auto dispatch_scalar_type = infer_scalar_type(input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::batch_norm_cpu(/* actuals */ input, weight, bias, running_mean, running_var, training, momentum, eps);
        break;
        default:
            AT_ERROR("native_batch_norm not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::batch_norm_stats(const Tensor & input, double eps) const {
    AT_ERROR("batch_norm_stats is not implemented for type ", toString());
}
Tensor Type::batch_norm_elemt(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & mean, const Tensor & invstd, double eps) const {
    AT_ERROR("batch_norm_elemt is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor> Type::batch_norm_gather_stats(const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & running_mean, const Tensor & running_var, double momentum, double eps, int64_t count) const {
    AT_ERROR("batch_norm_gather_stats is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor,Tensor> Type::native_batch_norm_backward(const Tensor & grad_out, const Tensor & input, const Tensor & weight, const Tensor & running_mean, const Tensor & running_var, const Tensor & save_mean, const Tensor & save_invstd, bool train, double eps, std::array<bool,3> output_mask) const {
    const OptionalDeviceGuard device_guard(device_of(grad_out));
    auto dispatch_scalar_type = infer_scalar_type(grad_out);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::batch_norm_backward_cpu(/* actuals */ grad_out, input, weight, running_mean, running_var, save_mean, save_invstd, train, eps, output_mask);
        break;
        default:
            AT_ERROR("native_batch_norm_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor,Tensor> Type::batch_norm_backward_reduce(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, bool input_g, bool weight_g, bool bias_g) const {
    AT_ERROR("batch_norm_backward_reduce is not implemented for type ", toString());
}
Tensor Type::batch_norm_backward_elemt(const Tensor & grad_out, const Tensor & input, const Tensor & mean, const Tensor & invstd, const Tensor & weight, const Tensor & mean_dy, const Tensor & mean_dy_xmu) const {
    AT_ERROR("batch_norm_backward_elemt is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor> Type::batch_norm_update_stats(const Tensor & input, const Tensor & running_mean, const Tensor & running_var, double momentum) const {
    const OptionalDeviceGuard device_guard(device_of(input));
    auto dispatch_scalar_type = infer_scalar_type(input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::batch_norm_update_stats_cpu(/* actuals */ input, running_mean, running_var, momentum);
        break;
        default:
            AT_ERROR("batch_norm_update_stats not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::randperm_out(Tensor & out, int64_t n, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    auto dispatch_scalar_type = infer_scalar_type(out);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::randperm_out_cpu(/* actuals */ out, n, generator);
        break;
        default:
            AT_ERROR("randperm_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::range_out(Tensor & out, Scalar start, Scalar end, Scalar step) const {
    const OptionalDeviceGuard device_guard(device_of(out));
    auto dispatch_scalar_type = infer_scalar_type(out);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::range_cpu_out(/* actuals */ out, start, end, step);
        break;
        default:
            AT_ERROR("range_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::reciprocal_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_reciprocal__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("reciprocal_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::reciprocal_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_reciprocal_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("reciprocal_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::neg_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_neg__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("neg_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::neg_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_neg_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("neg_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::repeat_interleave(const Tensor & repeats) const {
    const OptionalDeviceGuard device_guard(device_of(repeats));
    auto dispatch_scalar_type = infer_scalar_type(repeats);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::repeat_interleave_cpu(/* actuals */ repeats);
        break;
        default:
            AT_ERROR("repeat_interleave not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::mkldnn_reshape(const Tensor & self, IntArrayRef shape) const {
    AT_ERROR("mkldnn_reshape is not implemented for type ", toString());
}
Tensor & Type::round_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_round__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("round_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::round_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_round_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("round_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::relu(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::relu(/* actuals */ self);
        break;
        default:
            AT_ERROR("relu not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::relu_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::relu_(/* actuals */ self);
        break;
        default:
            AT_ERROR("relu_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::prelu(const Tensor & self, const Tensor & weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::prelu_cpu(/* actuals */ self, weight);
        break;
        default:
            AT_ERROR("prelu not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::prelu_backward(const Tensor & grad_output, const Tensor & self, const Tensor & weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::prelu_backward_cpu(/* actuals */ grad_output, self, weight);
        break;
        default:
            AT_ERROR("prelu_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::hardshrink(const Tensor & self, Scalar lambd) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::hardshrink_cpu(/* actuals */ self, lambd);
        break;
        default:
            AT_ERROR("hardshrink not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::hardshrink_backward(const Tensor & grad_out, const Tensor & self, Scalar lambd) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::hardshrink_backward_cpu(/* actuals */ grad_out, self, lambd);
        break;
        default:
            AT_ERROR("hardshrink_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::rsqrt_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_rsqrt__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("rsqrt_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::rsqrt_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_rsqrt_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("rsqrt_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::sigmoid_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_sigmoid__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("sigmoid_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::sigmoid_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_sigmoid_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("sigmoid_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::sin_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_sin__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("sin_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::sin_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_sin_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("sin_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::sinh_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_sinh__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("sinh_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::sinh_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_sinh_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("sinh_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_softmax(const Tensor & self, int64_t dim, bool half_to_float) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::softmax_cpu(/* actuals */ self, dim, half_to_float);
        break;
        default:
            AT_ERROR("_softmax not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_softmax_backward_data(const Tensor & grad_output, const Tensor & output, int64_t dim, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::softmax_backward_cpu(/* actuals */ grad_output, output, dim, self);
        break;
        default:
            AT_ERROR("_softmax_backward_data not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_sparse_add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) const {
    AT_ERROR("_sparse_add_out is not implemented for type ", toString());
}
Tensor & Type::_sparse_dense_add_out(Tensor & out, const Tensor & self, SparseTensorRef other, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::add_out_dense_sparse_cpu(/* actuals */ out, self, other, alpha);
        break;
        default:
            AT_ERROR("_sparse_dense_add_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::_sparse_div_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
    AT_ERROR("_sparse_div_zerodim_out is not implemented for type ", toString());
}
Tensor & Type::_sparse_div_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
    AT_ERROR("_sparse_div_scalar_out is not implemented for type ", toString());
}
Tensor & Type::_sparse_mul_out(Tensor & out, const Tensor & self, const Tensor & other) const {
    AT_ERROR("_sparse_mul_out is not implemented for type ", toString());
}
Tensor & Type::_sparse_mul_zerodim_out(Tensor & out, const Tensor & self, const Tensor & other) const {
    AT_ERROR("_sparse_mul_zerodim_out is not implemented for type ", toString());
}
Tensor & Type::_sparse_mul_scalar_out(Tensor & out, const Tensor & self, Scalar other) const {
    AT_ERROR("_sparse_mul_scalar_out is not implemented for type ", toString());
}
Tensor & Type::sspaddmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_sspaddmm_out_only_sparse(/* actuals */ out, self, mat1, mat2, beta, alpha);
        break;
        default:
            AT_ERROR("sspaddmm_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::sqrt_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_sqrt__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("sqrt_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::sqrt_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_sqrt_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("sqrt_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::tan_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_tan__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("tan_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::tan_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_tan_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("tan_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::tanh_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_tanh__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("tanh_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::tanh_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_tanh_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("tanh_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::flip(const Tensor & self, IntArrayRef dims) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::flip_cpu(/* actuals */ self, dims);
        break;
        default:
            AT_ERROR("flip not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::roll(const Tensor & self, IntArrayRef shifts, IntArrayRef dims) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::roll_cpu(/* actuals */ self, shifts, dims);
        break;
        default:
            AT_ERROR("roll not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::trunc_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_trunc__cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("trunc_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::trunc_out(Tensor & out, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_trunc_out_cpu(/* actuals */ out, self);
        break;
        default:
            AT_ERROR("trunc_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_unique(const Tensor & self, bool sorted, bool return_inverse) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_unique_cpu(/* actuals */ self, sorted, return_inverse);
        break;
        default:
            AT_ERROR("_unique not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> Type::unique_dim(const Tensor & self, int64_t dim, bool sorted, bool return_inverse, bool return_counts) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::unique_dim_cpu(/* actuals */ self, dim, sorted, return_inverse, return_counts);
        break;
        default:
            AT_ERROR("unique_dim not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> Type::unique_consecutive(const Tensor & self, bool return_inverse, bool return_counts, c10::optional<int64_t> dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::unique_consecutive_cpu(/* actuals */ self, return_inverse, return_counts, dim);
        break;
        default:
            AT_ERROR("unique_consecutive not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> Type::unique_dim_consecutive(const Tensor & self, int64_t dim, bool return_inverse, bool return_counts) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::unique_dim_consecutive_cpu(/* actuals */ self, dim, return_inverse, return_counts);
        break;
        default:
            AT_ERROR("unique_dim_consecutive not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> Type::_unique2(const Tensor & self, bool sorted, bool return_inverse, bool return_counts) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_unique2_cpu(/* actuals */ self, sorted, return_inverse, return_counts);
        break;
        default:
            AT_ERROR("_unique2 not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_s_where(const Tensor & condition, const Tensor & self, const Tensor & other) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_s_where_cpu(/* actuals */ condition, self, other);
        break;
        default:
            AT_ERROR("_s_where not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_weight_norm_cuda_interface(const Tensor & v, const Tensor & g, int64_t dim) const {
    AT_ERROR("_weight_norm_cuda_interface is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor> Type::_weight_norm_cuda_interface_backward(const Tensor & grad_w, const Tensor & saved_v, const Tensor & saved_g, const Tensor & saved_norms, int64_t dim) const {
    AT_ERROR("_weight_norm_cuda_interface_backward is not implemented for type ", toString());
}
Tensor Type::_standard_gamma_grad(const Tensor & self, const Tensor & output) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_standard_gamma_grad_cpu(/* actuals */ self, output);
        break;
        default:
            AT_ERROR("_standard_gamma_grad not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_standard_gamma(const Tensor & self, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_s_gamma_cpu(/* actuals */ self, generator);
        break;
        default:
            AT_ERROR("_standard_gamma not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_sample_dirichlet(const Tensor & self, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_s_dirichlet_cpu(/* actuals */ self, generator);
        break;
        default:
            AT_ERROR("_sample_dirichlet not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::poisson(const Tensor & self, Generator * generator) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_s_poisson_cpu(/* actuals */ self, generator);
        break;
        default:
            AT_ERROR("poisson not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::native_norm(const Tensor & self, Scalar p) const {
    AT_ERROR("native_norm is not implemented for type ", toString());
}
Tensor Type::_sparse_sum_backward(const Tensor & grad, const Tensor & self, IntArrayRef dim) const {
    AT_ERROR("_sparse_sum_backward is not implemented for type ", toString());
}
Tensor Type::clone(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool:
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::clone(/* actuals */ self);
        break;
        default:
            AT_ERROR("clone not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::resize_as_(Tensor & self, const Tensor & the_template) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool:
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::resize_as_(/* actuals */ self, the_template);
        break;
        default:
            AT_ERROR("resize_as_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::pow_out(Tensor & out, const Tensor & self, Scalar exponent) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::pow_out(/* actuals */ out, self, exponent);
        break;
        default:
            AT_ERROR("pow_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::pow(const Tensor & self, Scalar exponent) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::pow(/* actuals */ self, exponent);
        break;
        default:
            AT_ERROR("pow not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::zero_(Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool:
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::zero_(/* actuals */ self);
        break;
        default:
            AT_ERROR("zero_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s_native_addmm_out(Tensor & out, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::s_addmm_out_sparse_dense_cpu(/* actuals */ out, self, mat1, mat2, beta, alpha);
        break;
        default:
            AT_ERROR("s_native_addmm_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::s_native_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::s_addmm_sparse_dense_cpu(/* actuals */ self, mat1, mat2, beta, alpha);
        break;
        default:
            AT_ERROR("s_native_addmm not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::s_native_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::s_addmm_sparse_dense_cpu_(/* actuals */ self, mat1, mat2, beta, alpha);
        break;
        default:
            AT_ERROR("s_native_addmm_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_sparse_coo_tensor_with_dims(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const TensorOptions & options) const {
    AT_ERROR("_sparse_coo_tensor_with_dims is not implemented for type ", toString());
}
Tensor Type::_sparse_coo_tensor_with_dims_and_tensors(int64_t sparse_dim, int64_t dense_dim, IntArrayRef size, const Tensor & indices, const Tensor & values, const TensorOptions & options) const {
    AT_ERROR("_sparse_coo_tensor_with_dims_and_tensors is not implemented for type ", toString());
}
Tensor & Type::sparse_resize_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
    AT_ERROR("sparse_resize_ is not implemented for type ", toString());
}
Tensor & Type::sparse_resize_and_clear_(Tensor & self, IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const {
    AT_ERROR("sparse_resize_and_clear_ is not implemented for type ", toString());
}
Tensor Type::sparse_mask(const Tensor & self, SparseTensorRef mask) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::sparse_mask_cpu(/* actuals */ self, mask);
        break;
        default:
            AT_ERROR("sparse_mask not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::to_dense(const Tensor & self) const {
    AT_ERROR("to_dense is not implemented for type ", toString());
}
int64_t Type::sparse_dim(const Tensor & self) const {
    AT_ERROR("sparse_dim is not implemented for type ", toString());
}
int64_t Type::dense_dim(const Tensor & self) const {
    AT_ERROR("dense_dim is not implemented for type ", toString());
}
int64_t Type::_nnz(const Tensor & self) const {
    AT_ERROR("_nnz is not implemented for type ", toString());
}
Tensor Type::coalesce(const Tensor & self) const {
    AT_ERROR("coalesce is not implemented for type ", toString());
}
bool Type::is_coalesced(const Tensor & self) const {
    AT_ERROR("is_coalesced is not implemented for type ", toString());
}
Tensor Type::_indices(const Tensor & self) const {
    AT_ERROR("_indices is not implemented for type ", toString());
}
Tensor Type::_values(const Tensor & self) const {
    AT_ERROR("_values is not implemented for type ", toString());
}
Tensor & Type::_coalesced_(Tensor & self, bool coalesced) const {
    AT_ERROR("_coalesced_ is not implemented for type ", toString());
}
Tensor Type::indices(const Tensor & self) const {
    AT_ERROR("indices is not implemented for type ", toString());
}
Tensor Type::values(const Tensor & self) const {
    AT_ERROR("values is not implemented for type ", toString());
}
Tensor & Type::hspmm_out(Tensor & out, const Tensor & mat1, const Tensor & mat2) const {
    AT_ERROR("hspmm_out is not implemented for type ", toString());
}
Tensor Type::hspmm(const Tensor & mat1, const Tensor & mat2) const {
    AT_ERROR("hspmm is not implemented for type ", toString());
}
Tensor & Type::copy_sparse_to_sparse_(Tensor & self, const Tensor & src, bool non_blocking) const {
    AT_ERROR("copy_sparse_to_sparse_ is not implemented for type ", toString());
}
Tensor Type::to_sparse(const Tensor & self, int64_t sparse_dim) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::dense_to_sparse(/* actuals */ self, sparse_dim);
        break;
        default:
            AT_ERROR("to_sparse not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::to_sparse(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::dense_to_sparse(/* actuals */ self);
        break;
        default:
            AT_ERROR("to_sparse not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::to_mkldnn(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::dense_to_mkldnn(/* actuals */ self);
        break;
        default:
            AT_ERROR("to_mkldnn not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::mkldnn_reorder_conv2d_weight(const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups) const {
    AT_ERROR("mkldnn_reorder_conv2d_weight is not implemented for type ", toString());
}
Tensor Type::quantize_linear(const Tensor & self, double scale, int64_t zero_point) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::quantize_linear_cpu(/* actuals */ self, scale, zero_point);
        break;
        default:
            AT_ERROR("quantize_linear not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::dequantize(const Tensor & self) const {
    AT_ERROR("dequantize is not implemented for type ", toString());
}
Scalar Type::q_scale(const Tensor & self) const {
    AT_ERROR("q_scale is not implemented for type ", toString());
}
Scalar Type::q_zero_point(const Tensor & self) const {
    AT_ERROR("q_zero_point is not implemented for type ", toString());
}
Tensor Type::int_repr(const Tensor & self) const {
    AT_ERROR("int_repr is not implemented for type ", toString());
}
Scalar Type::_local_scalar_dense(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool:
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Half:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_local_scalar_dense_cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("_local_scalar_dense not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> Type::_thnn_fused_lstm_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & cx, const Tensor & input_bias, const Tensor & hidden_bias) const {
    AT_ERROR("_thnn_fused_lstm_cell is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> Type::_thnn_fused_lstm_cell_backward(const Tensor & grad_hy, const Tensor & grad_cy, const Tensor & cx, const Tensor & cy, const Tensor & workspace, bool has_bias) const {
    AT_ERROR("_thnn_fused_lstm_cell_backward is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor> Type::_thnn_fused_gru_cell(const Tensor & input_gates, const Tensor & hidden_gates, const Tensor & hx, const Tensor & input_bias, const Tensor & hidden_bias) const {
    AT_ERROR("_thnn_fused_gru_cell is not implemented for type ", toString());
}
std::tuple<Tensor,Tensor,Tensor,Tensor,Tensor> Type::_thnn_fused_gru_cell_backward(const Tensor & grad_hy, const Tensor & workspace, bool has_bias) const {
    AT_ERROR("_thnn_fused_gru_cell_backward is not implemented for type ", toString());
}
Tensor & Type::tril_(Tensor & self, int64_t diagonal) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::tril_cpu_(/* actuals */ self, diagonal);
        break;
        default:
            AT_ERROR("tril_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::triu_(Tensor & self, int64_t diagonal) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::triu_cpu_(/* actuals */ self, diagonal);
        break;
        default:
            AT_ERROR("triu_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::lerp_(Tensor & self, const Tensor & end, Scalar weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::lerp_cpu_scalar_(/* actuals */ self, end, weight);
        break;
        default:
            AT_ERROR("lerp_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::lerp_(Tensor & self, const Tensor & end, const Tensor & weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::lerp_cpu_tensor_(/* actuals */ self, end, weight);
        break;
        default:
            AT_ERROR("lerp_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::triu_out(Tensor & out, const Tensor & self, int64_t diagonal) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::triu_cpu_out(/* actuals */ out, self, diagonal);
        break;
        default:
            AT_ERROR("triu_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::tril_out(Tensor & out, const Tensor & self, int64_t diagonal) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::tril_cpu_out(/* actuals */ out, self, diagonal);
        break;
        default:
            AT_ERROR("tril_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::tril_indices(int64_t row, int64_t col, int64_t offset, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    auto dispatch_scalar_type = typeMetaToScalarType(options.dtype());
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::tril_indices_cpu(/* actuals */ row, col, offset, options);
        break;
        default:
            AT_ERROR("tril_indices not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::triu_indices(int64_t row, int64_t col, int64_t offset, const TensorOptions & options) const {
    const DeviceGuard device_guard(options.device());
    auto dispatch_scalar_type = typeMetaToScalarType(options.dtype());
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::triu_indices_cpu(/* actuals */ row, col, offset, options);
        break;
        default:
            AT_ERROR("triu_indices not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_triangular_solve_helper(const Tensor & self, const Tensor & A, bool upper, bool transpose, bool unitriangular) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_triangular_solve_helper_cpu(/* actuals */ self, A, upper, transpose, unitriangular);
        break;
        default:
            AT_ERROR("_triangular_solve_helper not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_cholesky_helper(const Tensor & self, bool upper) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_cholesky_helper_cpu(/* actuals */ self, upper);
        break;
        default:
            AT_ERROR("_cholesky_helper not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_cholesky_solve_helper(const Tensor & self, const Tensor & A, bool upper) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_cholesky_solve_helper_cpu(/* actuals */ self, A, upper);
        break;
        default:
            AT_ERROR("_cholesky_solve_helper not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::_solve_helper(const Tensor & self, const Tensor & A) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_solve_helper_cpu(/* actuals */ self, A);
        break;
        default:
            AT_ERROR("_solve_helper not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor,Tensor> Type::_lu_with_info(const Tensor & self, bool pivot, bool check_errors) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_lu_with_info_cpu(/* actuals */ self, pivot, check_errors);
        break;
        default:
            AT_ERROR("_lu_with_info not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::lerp_out(Tensor & out, const Tensor & self, const Tensor & end, Scalar weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::lerp_cpu_scalar_out(/* actuals */ out, self, end, weight);
        break;
        default:
            AT_ERROR("lerp_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::lerp_out(Tensor & out, const Tensor & self, const Tensor & end, const Tensor & weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::lerp_cpu_tensor_out(/* actuals */ out, self, end, weight);
        break;
        default:
            AT_ERROR("lerp_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::lerp(const Tensor & self, const Tensor & end, Scalar weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::lerp_cpu_scalar(/* actuals */ self, end, weight);
        break;
        default:
            AT_ERROR("lerp not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::lerp(const Tensor & self, const Tensor & end, const Tensor & weight) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::lerp_cpu_tensor(/* actuals */ self, end, weight);
        break;
        default:
            AT_ERROR("lerp not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::histc_out(Tensor & out, const Tensor & self, int64_t bins, Scalar min, Scalar max) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_histc_out_cpu(/* actuals */ out, self, bins, min, max);
        break;
        default:
            AT_ERROR("histc_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::histc(const Tensor & self, int64_t bins, Scalar min, Scalar max) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::_histc_cpu(/* actuals */ self, bins, min, max);
        break;
        default:
            AT_ERROR("histc not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::median(const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::median_cpu(/* actuals */ self);
        break;
        default:
            AT_ERROR("median not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::adaptive_avg_pool2d_out(Tensor & out, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_avg_pool2d_out_cpu(/* actuals */ out, self, output_size);
        break;
        default:
            AT_ERROR("adaptive_avg_pool2d_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::mkldnn_adaptive_avg_pool2d(const Tensor & self, IntArrayRef output_size) const {
    AT_ERROR("mkldnn_adaptive_avg_pool2d is not implemented for type ", toString());
}
Tensor Type::_adaptive_avg_pool2d(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_avg_pool2d_cpu(/* actuals */ self, output_size);
        break;
        default:
            AT_ERROR("_adaptive_avg_pool2d not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::_adaptive_avg_pool2d_backward(const Tensor & grad_output, const Tensor & self) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_avg_pool2d_backward_cpu(/* actuals */ grad_output, self);
        break;
        default:
            AT_ERROR("_adaptive_avg_pool2d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::adaptive_max_pool2d_out(Tensor & out, Tensor & indices, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_max_pool2d_out_cpu(/* actuals */ out, indices, self, output_size);
        break;
        default:
            AT_ERROR("adaptive_max_pool2d_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::adaptive_max_pool2d(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_max_pool2d_cpu(/* actuals */ self, output_size);
        break;
        default:
            AT_ERROR("adaptive_max_pool2d not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::adaptive_max_pool2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_max_pool2d_backward_out_cpu(/* actuals */ grad_input, grad_output, self, indices);
        break;
        default:
            AT_ERROR("adaptive_max_pool2d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::adaptive_max_pool2d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_max_pool2d_backward_cpu(/* actuals */ grad_output, self, indices);
        break;
        default:
            AT_ERROR("adaptive_max_pool2d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::adaptive_max_pool3d_out(Tensor & out, Tensor & indices, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_max_pool3d_out_cpu(/* actuals */ out, indices, self, output_size);
        break;
        default:
            AT_ERROR("adaptive_max_pool3d_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::adaptive_max_pool3d(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_max_pool3d_cpu(/* actuals */ self, output_size);
        break;
        default:
            AT_ERROR("adaptive_max_pool3d not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::adaptive_max_pool3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_max_pool3d_backward_out_cpu(/* actuals */ grad_input, grad_output, self, indices);
        break;
        default:
            AT_ERROR("adaptive_max_pool3d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::adaptive_max_pool3d_backward(const Tensor & grad_output, const Tensor & self, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::adaptive_max_pool3d_backward_cpu(/* actuals */ grad_output, self, indices);
        break;
        default:
            AT_ERROR("adaptive_max_pool3d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::avg_pool2d_out(Tensor & out, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::avg_pool2d_out(/* actuals */ out, self, kernel_size, stride, padding, ceil_mode, count_include_pad);
        break;
        default:
            AT_ERROR("avg_pool2d_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::avg_pool2d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::avg_pool2d(/* actuals */ self, kernel_size, stride, padding, ceil_mode, count_include_pad);
        break;
        default:
            AT_ERROR("avg_pool2d not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::fractional_max_pool2d_out(Tensor & output, Tensor & indices, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & random_samples) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::fractional_max_pool2d_out_cpu(/* actuals */ output, indices, self, kernel_size, output_size, random_samples);
        break;
        default:
            AT_ERROR("fractional_max_pool2d_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::fractional_max_pool2d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & random_samples) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::fractional_max_pool2d_cpu(/* actuals */ self, kernel_size, output_size, random_samples);
        break;
        default:
            AT_ERROR("fractional_max_pool2d not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::fractional_max_pool2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::fractional_max_pool2d_backward_out_cpu(/* actuals */ grad_input, grad_output, self, kernel_size, output_size, indices);
        break;
        default:
            AT_ERROR("fractional_max_pool2d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::fractional_max_pool2d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::fractional_max_pool2d_backward_cpu(/* actuals */ grad_output, self, kernel_size, output_size, indices);
        break;
        default:
            AT_ERROR("fractional_max_pool2d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> Type::fractional_max_pool3d_out(Tensor & output, Tensor & indices, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & random_samples) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::fractional_max_pool3d_out_cpu(/* actuals */ output, indices, self, kernel_size, output_size, random_samples);
        break;
        default:
            AT_ERROR("fractional_max_pool3d_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> Type::fractional_max_pool3d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & random_samples) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::fractional_max_pool3d_cpu(/* actuals */ self, kernel_size, output_size, random_samples);
        break;
        default:
            AT_ERROR("fractional_max_pool3d not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::fractional_max_pool3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::fractional_max_pool3d_backward_out_cpu(/* actuals */ grad_input, grad_output, self, kernel_size, output_size, indices);
        break;
        default:
            AT_ERROR("fractional_max_pool3d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::fractional_max_pool3d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef kernel_size, IntArrayRef output_size, const Tensor & indices) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::fractional_max_pool3d_backward_cpu(/* actuals */ grad_output, self, kernel_size, output_size, indices);
        break;
        default:
            AT_ERROR("fractional_max_pool3d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::reflection_pad1d_out(Tensor & out, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::reflection_pad1d_out_cpu(/* actuals */ out, self, padding);
        break;
        default:
            AT_ERROR("reflection_pad1d_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::reflection_pad1d(const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::reflection_pad1d_cpu(/* actuals */ self, padding);
        break;
        default:
            AT_ERROR("reflection_pad1d not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::reflection_pad1d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::reflection_pad1d_backward_out_cpu(/* actuals */ grad_input, grad_output, self, padding);
        break;
        default:
            AT_ERROR("reflection_pad1d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::reflection_pad1d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::reflection_pad1d_backward_cpu(/* actuals */ grad_output, self, padding);
        break;
        default:
            AT_ERROR("reflection_pad1d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::reflection_pad2d_out(Tensor & out, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::reflection_pad2d_out_cpu(/* actuals */ out, self, padding);
        break;
        default:
            AT_ERROR("reflection_pad2d_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::reflection_pad2d(const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::reflection_pad2d_cpu(/* actuals */ self, padding);
        break;
        default:
            AT_ERROR("reflection_pad2d not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::reflection_pad2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::reflection_pad2d_backward_out_cpu(/* actuals */ grad_input, grad_output, self, padding);
        break;
        default:
            AT_ERROR("reflection_pad2d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::reflection_pad2d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::reflection_pad2d_backward_cpu(/* actuals */ grad_output, self, padding);
        break;
        default:
            AT_ERROR("reflection_pad2d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::replication_pad1d_out(Tensor & out, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad1d_out_cpu(/* actuals */ out, self, padding);
        break;
        default:
            AT_ERROR("replication_pad1d_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::replication_pad1d(const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad1d_cpu(/* actuals */ self, padding);
        break;
        default:
            AT_ERROR("replication_pad1d not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::replication_pad1d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad1d_backward_out_cpu(/* actuals */ grad_input, grad_output, self, padding);
        break;
        default:
            AT_ERROR("replication_pad1d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::replication_pad1d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad1d_backward_cpu(/* actuals */ grad_output, self, padding);
        break;
        default:
            AT_ERROR("replication_pad1d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::replication_pad2d_out(Tensor & out, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad2d_out_cpu(/* actuals */ out, self, padding);
        break;
        default:
            AT_ERROR("replication_pad2d_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::replication_pad2d(const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad2d_cpu(/* actuals */ self, padding);
        break;
        default:
            AT_ERROR("replication_pad2d not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::replication_pad2d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad2d_backward_out_cpu(/* actuals */ grad_input, grad_output, self, padding);
        break;
        default:
            AT_ERROR("replication_pad2d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::replication_pad2d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad2d_backward_cpu(/* actuals */ grad_output, self, padding);
        break;
        default:
            AT_ERROR("replication_pad2d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::replication_pad3d_out(Tensor & out, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad3d_out_cpu(/* actuals */ out, self, padding);
        break;
        default:
            AT_ERROR("replication_pad3d_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::replication_pad3d(const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad3d_cpu(/* actuals */ self, padding);
        break;
        default:
            AT_ERROR("replication_pad3d not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::replication_pad3d_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad3d_backward_out_cpu(/* actuals */ grad_input, grad_output, self, padding);
        break;
        default:
            AT_ERROR("replication_pad3d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::replication_pad3d_backward(const Tensor & grad_output, const Tensor & self, IntArrayRef padding) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::replication_pad3d_backward_cpu(/* actuals */ grad_output, self, padding);
        break;
        default:
            AT_ERROR("replication_pad3d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::upsample_linear1d_out(Tensor & out, const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_linear1d_out_cpu(/* actuals */ out, self, output_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_linear1d_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::upsample_linear1d(const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_linear1d_cpu(/* actuals */ self, output_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_linear1d not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::upsample_linear1d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_linear1d_backward_out_cpu(/* actuals */ grad_input, grad_output, output_size, input_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_linear1d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::upsample_linear1d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_linear1d_backward_cpu(/* actuals */ grad_output, output_size, input_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_linear1d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::upsample_bilinear2d_out(Tensor & out, const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_bilinear2d_out_cpu(/* actuals */ out, self, output_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_bilinear2d_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::upsample_bilinear2d(const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_bilinear2d_cpu(/* actuals */ self, output_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_bilinear2d not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::upsample_bilinear2d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_bilinear2d_backward_out_cpu(/* actuals */ grad_input, grad_output, output_size, input_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_bilinear2d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::upsample_bilinear2d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_bilinear2d_backward_cpu(/* actuals */ grad_output, output_size, input_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_bilinear2d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::upsample_bicubic2d_out(Tensor & out, const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_bicubic2d_out_cpu(/* actuals */ out, self, output_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_bicubic2d_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::upsample_bicubic2d(const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_bicubic2d_cpu(/* actuals */ self, output_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_bicubic2d not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::upsample_bicubic2d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_bicubic2d_backward_out_cpu(/* actuals */ grad_input, grad_output, output_size, input_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_bicubic2d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::upsample_bicubic2d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_bicubic2d_backward_cpu(/* actuals */ grad_output, output_size, input_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_bicubic2d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::upsample_trilinear3d_out(Tensor & out, const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_trilinear3d_out_cpu(/* actuals */ out, self, output_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_trilinear3d_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::upsample_trilinear3d(const Tensor & self, IntArrayRef output_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_trilinear3d_cpu(/* actuals */ self, output_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_trilinear3d not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::upsample_trilinear3d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_trilinear3d_backward_out_cpu(/* actuals */ grad_input, grad_output, output_size, input_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_trilinear3d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::upsample_trilinear3d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size, bool align_corners) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_trilinear3d_backward_cpu(/* actuals */ grad_output, output_size, input_size, align_corners);
        break;
        default:
            AT_ERROR("upsample_trilinear3d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::upsample_nearest1d_out(Tensor & out, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest1d_out_cpu(/* actuals */ out, self, output_size);
        break;
        default:
            AT_ERROR("upsample_nearest1d_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::upsample_nearest1d(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest1d_cpu(/* actuals */ self, output_size);
        break;
        default:
            AT_ERROR("upsample_nearest1d not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::upsample_nearest1d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest1d_backward_out_cpu(/* actuals */ grad_input, grad_output, output_size, input_size);
        break;
        default:
            AT_ERROR("upsample_nearest1d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::upsample_nearest1d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest1d_backward_cpu(/* actuals */ grad_output, output_size, input_size);
        break;
        default:
            AT_ERROR("upsample_nearest1d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::upsample_nearest2d_out(Tensor & out, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest2d_out_cpu(/* actuals */ out, self, output_size);
        break;
        default:
            AT_ERROR("upsample_nearest2d_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::upsample_nearest2d(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest2d_cpu(/* actuals */ self, output_size);
        break;
        default:
            AT_ERROR("upsample_nearest2d not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::upsample_nearest2d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest2d_backward_out_cpu(/* actuals */ grad_input, grad_output, output_size, input_size);
        break;
        default:
            AT_ERROR("upsample_nearest2d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::upsample_nearest2d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest2d_backward_cpu(/* actuals */ grad_output, output_size, input_size);
        break;
        default:
            AT_ERROR("upsample_nearest2d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::upsample_nearest3d_out(Tensor & out, const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest3d_out_cpu(/* actuals */ out, self, output_size);
        break;
        default:
            AT_ERROR("upsample_nearest3d_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::upsample_nearest3d(const Tensor & self, IntArrayRef output_size) const {
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest3d_cpu(/* actuals */ self, output_size);
        break;
        default:
            AT_ERROR("upsample_nearest3d not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & Type::upsample_nearest3d_backward_out(Tensor & grad_input, const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_input));
    auto dispatch_scalar_type = infer_scalar_type(grad_input);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest3d_backward_out_cpu(/* actuals */ grad_input, grad_output, output_size, input_size);
        break;
        default:
            AT_ERROR("upsample_nearest3d_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor Type::upsample_nearest3d_backward(const Tensor & grad_output, IntArrayRef output_size, IntArrayRef input_size) const {
    const OptionalDeviceGuard device_guard(device_of(grad_output));
    auto dispatch_scalar_type = infer_scalar_type(grad_output);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte:
        case ScalarType::Char:
        case ScalarType::Double:
        case ScalarType::Float:
        case ScalarType::Int:
        case ScalarType::Long:
        case ScalarType::QInt8:
        case ScalarType::Short:
            return at::native::upsample_nearest3d_backward_cpu(/* actuals */ grad_output, output_size, input_size);
        break;
        default:
            AT_ERROR("upsample_nearest3d_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}

}
